<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta content="Minimalist Data Wrangling with Python" name="citation_title" />
<meta content="Marek Gagolewski" name="citation_author" />
<meta content="2023" name="citation_date" />
<meta content="2023" name="citation_publication_date" />
<meta content="https://datawranglingpy.gagolewski.com/datawranglingpy.pdf" name="citation_pdf_url" />
<meta content="https://datawranglingpy.gagolewski.com/" name="citation_public_url" />
<meta content="10.5281/zenodo.6451068" name="citation_doi" />
<meta content="Minimalist Data Wrangling with Python is envisaged as a student's first introduction to data science, providing a high-level overview as well as discussing key concepts in detail. We explore methods for cleaning data gathered from different sources, transforming, selecting, and extracting features, performing exploratory data analysis and dimensionality reduction, identifying naturally occurring data clusters, modelling patterns in data, comparing data between groups, and reporting the results. This textbook is a non-profit project. Its online and PDF versions are freely available at https://datawranglingpy.gagolewski.com/." name="citation_abstract" />
<meta content="summary" name="twitter:card" />
<meta content="Minimalist Data Wrangling with Python" name="twitter:title" />
<meta content="Minimalist Data Wrangling with Python" name="og:title" />
<meta content="Minimalist Data Wrangling with Python is envisaged as a student's first introduction to data science, providing a high-level overview as well as discussing key concepts in detail. We explore methods for cleaning data gathered from different sources, transforming, selecting, and extracting features, performing exploratory data analysis and dimensionality reduction, identifying naturally occurring data clusters, modelling patterns in data, comparing data between groups, and reporting the results. This textbook is a non-profit project. Its online and PDF versions are freely available at https://datawranglingpy.gagolewski.com/." name="twitter:description" />
<meta content="Minimalist Data Wrangling with Python is envisaged as a student's first introduction to data science, providing a high-level overview as well as discussing key concepts in detail. We explore methods for cleaning data gathered from different sources, transforming, selecting, and extracting features, performing exploratory data analysis and dimensionality reduction, identifying naturally occurring data clusters, modelling patterns in data, comparing data between groups, and reporting the results. This textbook is a non-profit project. Its online and PDF versions are freely available at https://datawranglingpy.gagolewski.com/." name="og:description" />
<meta content="gagolews/datawranglingpy" name="og:site_name" />
<meta content="https://datawranglingpy.gagolewski.com/" name="og:url" />
<meta content="https://datawranglingpy.gagolewski.com/_images/cover.png" name="twitter:image" />
<meta content="https://datawranglingpy.gagolewski.com/_images/cover.png" name="og:image" />
<meta content="https://datawranglingpy.gagolewski.com/" name="DC.identifier" />
<meta content="Marek Gagolewski" name="DC.publisher" />
<meta content="INDEX,FOLLOW" name="robots" />
<meta content="book" name="og:type" />
<meta content="9780645571912" name="og:book:isbn" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="7. Multidimensional numeric data at a glance" href="310-matrix.html" /><link rel="prev" title="5. Processing unidimensional data" href="220-transform-vector.html" />
        <link rel="canonical" href="https://datawranglingpy.gagolewski.com/chapter/230-distribution.html" />

    <link rel="shortcut icon" href="https://www.gagolewski.com/_static/img/datawranglingpy.png"/><!-- Generated with Sphinx 7.2.5 and Furo 2023.08.19 -->
        <title>6. Continuous probability distributions - Minimalist Data Wrangling with Python</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=135e06be" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=36a5483c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=cc2833c3" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  --admonition-font-size: 95%;
  --admonition-title-font-size: 95%;
  --color-brand-primary: red;
  --color-brand-content: #CC3333;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --admonition-font-size: 95%;
  --admonition-title-font-size: 95%;
  --color-brand-primary: #ff2b53;
  --color-brand-content: #dd3333;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --admonition-font-size: 95%;
  --admonition-title-font-size: 95%;
  --color-brand-primary: #ff2b53;
  --color-brand-content: #dd3333;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">Minimalist Data Wrangling with Python</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto colour theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky">
<div class="sidebar-logo-container">
  <a class="sidebar-brand" href="../index.html"><img class="sidebar-logo" src="https://www.gagolewski.com/_static/img/datawranglingpy.png" alt="Logo"/></a>
</div>

<span class="sidebar-brand-text">
<a class="sidebar-brand" href="../index.html">Minimalist Data Wrangling with Python</a>
</span>
<div class="sidebar-brand">
An open-access textbook<br />
by <a href='https://www.gagolewski.com/' style="display: contents">Marek Gagolewski</a><br />
v1.0.3.9011
</div>
<form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">About</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.gagolewski.com/">Author</a></li>
<li class="toctree-l1"><a class="reference external" href="https://datawranglingpy.gagolewski.com/datawranglingpy.pdf">This book in PDF</a></li>
<li class="toctree-l1"><a class="reference internal" href="../order-paper-copy.html">Order a paper copy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/gagolews/datawranglingpy">Report bugs or typos (GitHub)</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/gagolews/teaching-data">Datasets</a></li>
<li class="toctree-l1"><a class="reference external" href="https://deepr.gagolewski.com/">Deep R programming</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Start here</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="000-preface.html">Preface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introducing Python</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="110-setup.html">1. Getting started with Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="120-scalar.html">2. Scalar types and control structures in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="130-sequential.html">3. Sequential and other types in Python</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Unidimensional data</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="210-vector.html">4. Unidimensional numeric data and their empirical distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="220-transform-vector.html">5. Processing unidimensional data</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">6. Continuous probability distributions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multidimensional data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="310-matrix.html">7. Multidimensional numeric data at a glance</a></li>
<li class="toctree-l1"><a class="reference internal" href="320-transform-matrix.html">8. Processing multidimensional data</a></li>
<li class="toctree-l1"><a class="reference internal" href="330-relationship.html">9. Exploring relationships between variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Heterogeneous data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="410-data-frame.html">10. Introducing data frames</a></li>
<li class="toctree-l1"><a class="reference internal" href="420-categorical.html">11. Handling categorical data</a></li>
<li class="toctree-l1"><a class="reference internal" href="430-group-by.html">12. Processing data in groups</a></li>
<li class="toctree-l1"><a class="reference internal" href="440-sql.html">13. Accessing databases</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other data types</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="510-text.html">14. Text data</a></li>
<li class="toctree-l1"><a class="reference internal" href="520-missingness.html">15. Missing, censored, and questionable data</a></li>
<li class="toctree-l1"><a class="reference internal" href="530-time-series.html">16. Time series</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="998-changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="999-bibliography.html">References</a></li>
</ul>

</div></div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="edit-this-page">
  <a class="muted-link" href="https://github.com/gagolews/datawranglingpy" title="Edit this page">
    <svg aria-hidden="true" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <path d="M4 20h4l10.5 -10.5a1.5 1.5 0 0 0 -4 -4l-10.5 10.5v4" />
      <line x1="13.5" y1="6.5" x2="17.5" y2="10.5" />
    </svg>
    <span class="visually-hidden">Edit this page</span>
  </a>
</div><div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto colour theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section class="tex2jax_ignore mathjax_ignore" id="continuous-probability-distributions">
<span id="chap-distribution"></span><h1><span class="section-number">6. </span>Continuous probability distributions<a class="headerlink" href="#continuous-probability-distributions" title="Link to this heading">#</a></h1>
<blockquote>
<div><p><em>This open-access textbook
is, and will remain, freely available for everyone’s enjoyment
(also in <a class="reference external" href="https://datawranglingpy.gagolewski.com/datawranglingpy.pdf">PDF</a>;
a paper copy can also be <a class="reference internal" href="../order-paper-copy.html"><span class="doc std std-doc">ordered</span></a>).
It is a non-profit project. Although available online, it is a whole course,
and should be read from the beginning to the end.
Refer to the <a class="reference internal" href="000-preface.html#chap-preface"><span class="std std-ref">Preface</span></a> for general introductory remarks. Any
<a class="reference external" href="https://github.com/gagolews/datawranglingpy">bug/typo reports/fixes</a>
are appreciated. Make sure to check out
<a class="reference external" href="https://deepr.gagolewski.com/"><em>Deep R Programming</em></a>
<span id="id1">[<a class="reference internal" href="999-bibliography.html#id2" title="Gagolewski, M. (2023).  Deep R Programming. Zenodo. URL: https://deepr.gagolewski.com/, DOI: 10.5281/zenodo.7490464.">34</a>]</span> too.</em></p>
</div></blockquote>
<p>Each successful data analyst will deal with hundreds or thousands of
datasets in their lifetime. In the long run, at some level, most of them will
be deemed <em>boring</em>. This is because only a few common patterns
will be occurring over and over again.</p>
<p>In particular, the previously mentioned bell-shapedness and right-skewness
are quite prevalent in the so-called real world.
Surprisingly, however, this is exactly when things
become scientific and interesting, allowing us to study various phenomena
at an appropriate level of generality.</p>
<p>Mathematically, such idealised patterns in the histogram shapes
can be formalised using the notion of a
<em>probability density function</em> (PDF) of a <em>continuous, real-valued random
variable</em>.</p>
<p>Intuitively<a class="footnote-reference brackets" href="#foothistconvergence" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>, a PDF is
a smooth curve that would arise if we drew a histogram
for the entire <em>population</em> (e.g., all women living currently on Earth and
beyond or otherwise an extremely large data sample obtained by independently
querying the same underlying data generating process) in such a way that
the total area of all the bars is equal to 1
and the bin sizes are very small.</p>
<p>As stated at the beginning, we do not intend this to be a course in
probability theory and mathematical statistics.
Rather, it precedes and motivates them
(e.g., <span id="id3">[<a class="reference internal" href="999-bibliography.html#id47" title="Dekking, F.M., Kraaikamp, C., Lopuhaä, H.P., and Meester, L.E. (2005).  A Modern Introduction to Probability and Statistics: Understanding Why and How. Springer.">21</a>, <a class="reference internal" href="999-bibliography.html#id50" title="Gentle, J.E. (2009).  Computational Statistics. Springer-Verlag.">38</a>, <a class="reference internal" href="999-bibliography.html#id45" title="Gentle, J.E. (2020).  Theory of Statistics. book draft. URL: https://mason.gmu.edu/~jgentle/books/MathStat.pdf.">40</a>, <a class="reference internal" href="999-bibliography.html#id155" title="Ross, S.M. (2020).  Introduction to Probability and Statistics for Engineers and Scientists. Academic Press.">79</a>]</span>). Therefore,
our definitions are out of necessity simplified so that they are digestible.
For the purpose of our illustrations, we will consider
the following characterisation.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>(*) We call an integrable function <span class="math">\(f:\mathbb{R}\to\mathbb{R}\)</span>
a <em>probability density function</em>
if <span class="math">\(f(x)\ge 0\)</span> for all <span class="math">\(x\)</span> and <span class="math">\(\int_{-\infty}^\infty f(x)\,dx=1\)</span>,
i.e., it is nonnegative and normalised in such a way that
the total area under the whole curve is 1.</p>
<p>For any <span class="math">\(a &lt; b\)</span>, we treat the area under the fragment of
the <span class="math">\(f(x)\)</span> curve for <span class="math">\(x\)</span> between <span class="math">\(a\)</span> and <span class="math">\(b\)</span>, i.e., <span class="math">\(\int_a^b f(x)\,dx\)</span>,
as the probability of the underlying real-valued
random variable’s (theoretical data generating process’)
falling into the <span class="math">\([a, b]\)</span> interval.</p>
</div>
<p>Some distributions appear more frequently than others
and appear to fit empirical data or parts thereof particularly well;
compare <span id="id4">[<a class="reference internal" href="999-bibliography.html#id83" title="Forbes, C., Evans, M., Hastings, N., and Peacock, B. (2010).  Statistical Distributions. Wiley.">27</a>]</span>.
In this chapter, we review a few noteworthy probability distributions:
the normal, log-normal, Pareto, and uniform families
(we will also mention the chi-squared,
Kolmogorov, and exponential ones in this course).</p>
<section id="normal-distribution">
<h2><span class="section-number">6.1. </span>Normal distribution<a class="headerlink" href="#normal-distribution" title="Link to this heading">#</a></h2>
<p>A <em>normal (Gaussian) distribution</em> has a prototypical,
nicely symmetric, bell-shaped density.
It is described by two parameters:
<span class="math">\(\mu\in\mathbb{R}\)</span> (the expected value, at which the PDF is centred)
and <span class="math">\(\sigma&gt;0\)</span> (the standard deviation, saying how
much the distribution is dispersed around <span class="math">\(\mu\)</span>);
compare <a class="reference internal" href="#fig-normalpdf"><span class="std std-numref">Figure 6.1</span></a>.</p>
<p>The probability density function of <span class="math">\(\mathrm{N}(\mu, \sigma)\)</span> is given by:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
f(x) = \frac{1}{\sqrt{2\pi \sigma^2}}
\exp\left( -\frac{(x-\mu)^2}{2\sigma^2} \right).
\]</div>
</div>
<figure class="align-default" id="id28">
<span id="fig-normalpdf"></span><img alt="../_images/normalpdf-1.png" src="../_images/normalpdf-1.png" />
<figcaption>
<p><span class="caption-number">Figure 6.1 </span><span class="caption-text">The probability density functions of some normal distributions <span class="math">\(\mathrm{N}(\mu, \sigma)\)</span>. Note that <span class="math">\(\mu\)</span> is responsible for shifting and <span class="math">\(\sigma\)</span> affects scaling/stretching of the probability mass.</span><a class="headerlink" href="#id28" title="Link to this image">#</a></p>
</figcaption>
</figure>
<section id="estimating-parameters">
<h3><span class="section-number">6.1.1. </span>Estimating parameters<a class="headerlink" href="#estimating-parameters" title="Link to this heading">#</a></h3>
<p>A course in statistics (which, again, this one is not,
we are merely making an illustration here), may tell us
that the sample arithmetic mean <span class="math">\(\bar{x}\)</span> and standard deviation <span class="math">\(s\)</span>
are natural, statistically well-behaving <em>estimators</em> of the said parameters:
if all observations would really be drawn independently from
<span class="math">\(\mathrm{N}(\mu, \sigma)\)</span> each, then we <em>expect</em> <span class="math">\(\bar{x}\)</span> and <span class="math">\(s\)</span>
to be equal to, more or less, <span class="math">\(\mu\)</span> and <span class="math">\(\sigma\)</span>
(the larger the sample size, the smaller the error).</p>
<p>Recall the <code class="docutils literal notranslate"><span class="pre">heights</span></code> (females from the NHANES study)
dataset and its bell-shaped histogram
in <a class="reference internal" href="210-vector.html#fig-heights-histogram-bins11"><span class="std std-numref">Figure 4.2</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">heights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching-data/master/marek/nhanes_adult_female_height_2020.txt&quot;</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">heights</span><span class="p">)</span>
<span class="n">n</span>
<span class="c1">## 4221</span>
</pre></div>
</div>
<p>Let us estimate the said parameters for this sample:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">heights</span><span class="p">)</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span>
<span class="c1">## (160.13679222932953, 7.062858532891359)</span>
</pre></div>
</div>
<p>Mathematically, we will denote these two by
<span class="math">\(\hat{\mu}\)</span> and <span class="math">\(\hat{\sigma}\)</span> (mu and sigma with a hat) to emphasise that
they are merely guesstimates<a class="footnote-reference brackets" href="#footmle" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> of the unknown respective parameters
<span class="math">\(\mu\)</span> and <span class="math">\(\sigma\)</span>. On a side note, in this context,
the requested <code class="docutils literal notranslate"><span class="pre">ddof=1</span></code> estimator has slightly better statistical properties.</p>
<p>Let us draw the fitted density function
(i.e., the PDF of <span class="math">\(\mathrm{N}(160.1, 7.06)\)</span> which we can compute using
<strong class="command">scipy.stats.norm.pdf</strong>),
on top of the histogram; see <a class="reference internal" href="#fig-heights-normal"><span class="std std-numref">Figure 6.2</span></a>.
We pass <code class="docutils literal notranslate"><span class="pre">density=True</span></code> to <strong class="command">matplotlib.pyplot.hist</strong>
to normalise the bars’ heights so that
their total area sums to 1.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">heights</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">heights</span><span class="p">),</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">),</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;PDF of N(</span><span class="si">{</span><span class="n">mu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Density&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id29">
<span id="fig-heights-normal"></span><img alt="../_images/heights-normal-3.png" src="../_images/heights-normal-3.png" />
<figcaption>
<p><span class="caption-number">Figure 6.2 </span><span class="caption-text">A histogram and the probability density function of the fitted normal distribution for the <code class="docutils literal notranslate"><span class="pre">heights</span></code> dataset.</span><a class="headerlink" href="#id29" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>At first glance, this is a genuinely nice match.
Before proceeding with an overview of the ways to assess
the goodness-of-fit more rigorously, we should praise the potential
benefits of having an idealised <em>model</em> of our dataset at our disposal.</p>
</section>
<section id="data-models-are-useful">
<span id="sec-three-sigma"></span><h3><span class="section-number">6.1.2. </span>Data models are useful<a class="headerlink" href="#data-models-are-useful" title="Link to this heading">#</a></h3>
<p><em>If</em> (provided that, assuming that, on condition that)
our sample is a realisation of independent random variables
following a given distribution, or a data analyst judges that such an
approximation might be justified or beneficial, then we have a set of many
numbers <em>reduced</em> to merely a few parameters.</p>
<p>In the above case, we might want to risk the statement
that data follow the normal distribution (assumption 1)
with parameters <span class="math">\(\mu=160.1\)</span> and <span class="math">\(\sigma=7.06\)</span> (assumption 2).
Still, the choice of the distribution family is one thing,
and the way we estimate the underlying parameters
(in our case, we use <span class="math">\(\hat{\mu}\)</span> and <span class="math">\(\hat{\sigma}\)</span>)
is another.</p>
<p>This not only saves storage space and computational time, but also – based
on what we can learn from a course in probability and statistics
(by appropriately integrating the PDF) –
we can imply facts such as for normally distributed data:</p>
<ul class="simple">
<li><p>c. 68% of (i.e., a <em>majority</em>) women are
<span class="math">\(\mu\pm \sigma\)</span> tall (the <span class="math">\(1\sigma\)</span> rule),</p></li>
<li><p>c. 95% of (i.e., <em>most typical</em>) women are
<span class="math">\(\mu\pm 2\sigma\)</span> tall (the <span class="math">\(2\sigma\)</span> rule),</p></li>
<li><p>c. 99.7% of (i.e., <em>almost all</em>) women are
<span class="math">\(\mu\pm 3\sigma\)</span> tall (the <span class="math">\(3\sigma\)</span> rule).</p></li>
</ul>
<p>Also, if we knew that the distribution of heights of men
is also normal with some other parameters, we could be able to
make some comparisons between the two samples.
For example, we could compute the probability that a woman
randomly selected from the crowd is taller than a male passerby.</p>
<p>Furthermore, there is a range of <em>parametric</em> (assuming some distribution
family) statistical methods that could <em>additionally</em> be used if we assumed
the data normality, e.g., the t-test to compare the expected values.</p>
<div class="proof proof-type-exercise" id="id30">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.1</span>
        
    </div><div class="proof-content">
<p>How different manufacturing industries (e.g., clothing)
can make use of such models?
Are simplifications necessary when dealing with complexity?
What are the alternatives?</p>
</div></div><div class="admonition important">
<p class="admonition-title">Important</p>
<p>We are always expected to verify the assumptions of a model that we wish
to apply in practice.  In particular, we will soon note that incomes
are not normally distributed. Therefore, we must not
refer to the above <span class="math">\(2\sigma\)</span> or <span class="math">\(3\sigma\)</span> rule in their case.
A cow neither barks nor can it serve as a screwdriver. Period.</p>
</div>
</section>
</section>
<section id="assessing-goodness-of-fit">
<h2><span class="section-number">6.2. </span>Assessing goodness-of-fit<a class="headerlink" href="#assessing-goodness-of-fit" title="Link to this heading">#</a></h2>
<section id="comparing-cumulative-distribution-functions">
<h3><span class="section-number">6.2.1. </span>Comparing cumulative distribution functions<a class="headerlink" href="#comparing-cumulative-distribution-functions" title="Link to this heading">#</a></h3>
<p>Bell-shaped histograms are encountered quite frequently in real-world data:
e.g., measurement errors in physical experiments and standardised tests’
results (like IQ and other ability scores) tend to be distributed this way,
at least approximately.</p>
<p>If we yearn for more precision, there is a better way of assessing
the extent to which a sample deviates from a hypothesised distribution.
Namely, we can measure the discrepancy between
some theoretical <em>cumulative distribution function</em> (CDF)
and the empirical one (<span class="math">\(\hat{F}_n\)</span>; see <a class="reference internal" href="210-vector.html#sec-ecdf"><span class="std std-numref">Section 4.3.8</span></a>).</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If <span class="math">\(f\)</span> is a PDF, then the corresponding
theoretical CDF is defined as <span class="math">\(F(x) = \int_{-\infty}^x f(t)\,dt\)</span>,
i.e., the probability of the underlying random variable’s
taking a value less than or equal to <span class="math">\(x\)</span>.</p>
<p>By definition<a class="footnote-reference brackets" href="#footcdf" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>, each CDF takes values in the unit interval
(<span class="math">\([0, 1]\)</span>) and is nondecreasing.</p>
</div>
<p>For the normal distribution family,
the values of the theoretical CDF can be computed by calling
<strong class="command">scipy.stats.norm.cdf</strong>; see <a class="reference internal" href="#fig-heights-cdf"><span class="std std-numref">Figure 6.3</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">heights</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">heights</span><span class="p">),</span> <span class="mi">1001</span><span class="p">)</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>  <span class="c1"># sample the CDF at many points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;CDF of N(</span><span class="si">{</span><span class="n">mu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">heights_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">heights</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">heights_sorted</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">,</span>
    <span class="n">drawstyle</span><span class="o">=</span><span class="s2">&quot;steps-post&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Empirical CDF&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Prob(height $</span><span class="se">\\</span><span class="s2">leq$ x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id31">
<span id="fig-heights-cdf"></span><img alt="../_images/heights-cdf-5.png" src="../_images/heights-cdf-5.png" />
<figcaption>
<p><span class="caption-number">Figure 6.3 </span><span class="caption-text">The empirical CDF and the fitted normal CDF for the <code class="docutils literal notranslate"><span class="pre">heights</span></code> dataset: the fit is superb.</span><a class="headerlink" href="#id31" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>This <em>looks</em> like a superb match.</p>
<div class="proof proof-type-example" id="id32">

    <div class="proof-title">
        <span class="proof-type">Example 6.2</span>
        
    </div><div class="proof-content">
<p><span class="math">\(F(b)-F(a)=\int_{a}^b f(t)\,dt\)</span> is the probability
of generating a value in the interval <span class="math">\([a, b]\)</span>.</p>
<p>Let us empirically verify the <span class="math">\(3\sigma\)</span> rule:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">F</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
<span class="n">F</span><span class="p">(</span><span class="n">mu</span><span class="o">+</span><span class="mi">3</span><span class="o">*</span><span class="n">sigma</span><span class="p">)</span> <span class="o">-</span> <span class="n">F</span><span class="p">(</span><span class="n">mu</span><span class="o">-</span><span class="mi">3</span><span class="o">*</span><span class="n">sigma</span><span class="p">)</span>
<span class="c1">## 0.9973002039367398</span>
</pre></div>
</div>
<p>Indeed, almost all observations are within
<span class="math">\([\mu-3\sigma, \mu+3\sigma]\)</span>, if data are normally distributed.</p>
</div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>A common way to summarise the discrepancy between
the empirical and a given theoretical CDF is by computing
the greatest absolute deviation:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
\hat{D}_n = \sup_{t\in\mathbb{R}} | \hat{F}_n(t) - F(t) |,
\]</div>
</div>
<p>where the supremum is a continuous version of the maximum.</p>
</div>
<p>It holds:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
\hat{D}_n = \max \left\{
    \max_{k=1,\dots,n}\left\{ \left|\tfrac{k-1}{n} - F(x_{(k)})\right| \right\},
    \max_{k=1,\dots,n}\left\{ \left|\tfrac{k}{n} - F(x_{(k)})\right| \right\}
\right\},
\]</div>
</div>
<p>i.e., <span class="math">\(F\)</span> needs to be probed only at the <span class="math">\(n\)</span> points from the sorted
input sample.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_Dn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">F</span><span class="p">):</span>  <span class="c1"># equivalent to scipy.stats.kstest(x, F)[0]</span>
    <span class="n">Fx</span> <span class="o">=</span> <span class="n">F</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 1, 2, ..., n</span>
    <span class="n">Dn1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">n</span> <span class="o">-</span> <span class="n">Fx</span><span class="p">))</span>
    <span class="n">Dn2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">k</span><span class="o">/</span><span class="n">n</span> <span class="o">-</span> <span class="n">Fx</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="n">Dn1</span><span class="p">,</span> <span class="n">Dn2</span><span class="p">)</span>

<span class="n">Dn</span> <span class="o">=</span> <span class="n">compute_Dn</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="n">F</span><span class="p">)</span>
<span class="n">Dn</span>
<span class="c1">## 0.010470976524201148</span>
</pre></div>
</div>
<p>If the difference is <em>sufficiently<a class="footnote-reference brackets" href="#footsuffsmall" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a> small</em>,
then we can assume that a normal model describes data quite well.
This is indeed the case here: we may estimate the probability
of someone being as tall as any given height
with an error less than about 1.05%.</p>
</section>
<section id="comparing-quantiles">
<span id="sec-qqplot"></span><h3><span class="section-number">6.2.2. </span>Comparing quantiles<a class="headerlink" href="#comparing-quantiles" title="Link to this heading">#</a></h3>
<p>A <em>Q-Q plot</em> (quantile-quantile or probability plot)
is another graphical method for comparing two distributions.
This time, instead of working with a cumulative distribution function <span class="math">\(F\)</span>,
we will be dealing with its (generalised) inverse, i.e.,
the quantile function <span class="math">\(Q\)</span>.</p>
<p>Given a CDF <span class="math">\(F\)</span>, the corresponding <em>quantile function</em> is defined
for any <span class="math">\(p\in(0, 1)\)</span> as:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
Q(p) = \inf\{ x: F(x) \ge p \},
\]</div>
</div>
<p>i.e., the smallest <span class="math">\(x\)</span> such that the probability of drawing
a value not greater than <span class="math">\(x\)</span> is at least <span class="math">\(p\)</span>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If a CDF <span class="math">\(F\)</span> is continuous, and this is the assumption in the current
chapter, then <span class="math">\(Q\)</span> is exactly its inverse, i.e., it holds <span class="math">\(Q(p)=F^{-1}(p)\)</span>
for all <span class="math">\(p\in(0, 1)\)</span>; compare <a class="reference internal" href="#fig-normalcdfvsquant"><span class="std std-numref">Figure 6.4</span></a>.</p>
</div>
<figure class="align-default" id="id33">
<span id="fig-normalcdfvsquant"></span><img alt="../_images/normalcdfvsquant-7.png" src="../_images/normalcdfvsquant-7.png" />
<figcaption>
<p><span class="caption-number">Figure 6.4 </span><span class="caption-text">The cumulative distribution functions (left) and the quantile functions (being the inverse of the CDF; right) of some normal distributions.</span><a class="headerlink" href="#id33" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The theoretical quantiles can be generated
by the <strong class="command">scipy.stats.norm.ppf</strong> function.
Here, <em>ppf</em> stands for the percent point function which is another
(yet quite esoteric) name for the above <span class="math">\(Q\)</span>.</p>
<p>For instance, in our <span class="math">\(\mathrm{N}(160.1, 7.06)\)</span>-distributed <code class="docutils literal notranslate"><span class="pre">heights</span></code> dataset,
<span class="math">\(Q(0.9)\)</span> is the height not exceeded by 90% of the female population.
In other words, only 10% of American women are taller than:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
<span class="c1">## 169.18820963937648</span>
</pre></div>
</div>
<div style="margin-top: 1em"></div><p>A Q-Q plot draws a version of sample quantiles
as a function of the corresponding theoretical quantiles.
The sample quantiles, introduced
in <a class="reference internal" href="220-transform-vector.html#sec-quantiles"><span class="std std-numref">Section 5.1.1.3</span></a>, are natural estimators
of the theoretical quantile function.
However, we also mentioned that there are quite a few possible
definitions thereof in the literature; compare <span id="id8">[<a class="reference internal" href="999-bibliography.html#id12" title="Hyndman, R.J. and Fan, Y. (1996).  Sample quantiles in statistical packages. American Statistician, 50(4):361–365. DOI: 10.2307/2684934.">53</a>]</span>.</p>
<p>For simplicity, instead of using <strong class="command">numpy.quantile</strong>,
we will assume that the <span class="math">\(\frac{i}{n+1}\)</span>-quantile<a class="footnote-reference brackets" href="#footprobplot" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a>
is equal to <span class="math">\(x_{(i)}\)</span>, i.e., the <span class="math">\(i\)</span>-th smallest value in
a given sample <span class="math">\((x_1,x_2,\dots,x_n)\)</span>
and consider only <span class="math">\(i=1, 2, \dots, n\)</span>.</p>
<p>Our simplified setting avoids the problem which arises
when the 0- or 1-quantiles of the theoretical distribution,
i.e., <span class="math">\(Q(0)\)</span> or <span class="math">\(Q(1)\)</span>, are infinite (and this is the case for
the normal distribution family).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">qq_plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Q</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Draws a Q-Q plot, given:</span>
<span class="sd">    * x - a data sample (vector)</span>
<span class="sd">    * Q - a theoretical quantile function</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>    <span class="c1"># 1/(n+1), 2/(n+2), ..., n/(n+1)</span>
    <span class="n">x_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>          <span class="c1"># sample quantiles</span>
    <span class="n">quantiles</span> <span class="o">=</span> <span class="n">Q</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>               <span class="c1"># theoretical quantiles</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">quantiles</span><span class="p">,</span> <span class="n">x_sorted</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axline</span><span class="p">((</span><span class="n">x_sorted</span><span class="p">[</span><span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="n">x_sorted</span><span class="p">[</span><span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">]),</span> <span class="n">slope</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>  <span class="c1"># identity line</span>
</pre></div>
</div>
<p><a class="reference internal" href="#fig-qq-heights"><span class="std std-numref">Figure 6.5</span></a> depicts the Q-Q plot for our example dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">qq_plot</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">q</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantiles of N(</span><span class="si">{</span><span class="n">mu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Sample quantiles&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id34">
<span id="fig-qq-heights"></span><img alt="../_images/qq-heights-9.png" src="../_images/qq-heights-9.png" />
<figcaption>
<p><span class="caption-number">Figure 6.5 </span><span class="caption-text">The Q-Q plot for the <code class="docutils literal notranslate"><span class="pre">heights</span></code> dataset. It is a nice fit.</span><a class="headerlink" href="#id34" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Ideally, the points are expected to be arranged on the <span class="math">\(y=x\)</span> line
(which was added for readability). This would happen if
the sample quantiles matched the theoretical ones perfectly.
In our case, there are small discrepancies<a class="footnote-reference brackets" href="#footqqplotpearson" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a> in the tails
(e.g., the smallest observation was slightly smaller than expected,
and the largest one was larger than expected),
although it is quite <em>normal</em> a behaviour for small samples
and certain distribution families.
Still, we can say that we observe a fine fit.</p>
</section>
<section id="kolmogorovsmirnov-test">
<span id="sec-ks-test"></span><h3><span class="section-number">6.2.3. </span>Kolmogorov–Smirnov test (*)<a class="headerlink" href="#kolmogorovsmirnov-test" title="Link to this heading">#</a></h3>
<p>To be scientific, we must yearn for some more formal method that will
enable us to test the null hypothesis stating that a given empirical
distribution <span class="math">\(\hat{F}_n\)</span> does not differ <em>significantly</em> from the
theoretical continuous CDF <span class="math">\(F\)</span>:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
\left\{
\begin{array}{rll}
H_0: & \hat{F}_n = F & \text{(null hypothesis)}\\
H_1: & \hat{F}_n \neq F & \text{(two-sided alternative)} \\
\end{array}
\right.
\]</div>
</div>
<p>The popular goodness-of-fit test by Kolmogorov and Smirnov can give
us a conservative interval of the acceptable values of <span class="math">\(\hat{D}_n\)</span>
(again: the largest deviation between the empirical and theoretical CDF)
as a function of <span class="math">\(n\)</span> (within the framework of frequentist hypothesis testing).</p>
<p>Namely, if the <em>test statistic</em> <span class="math">\(\hat{D}_n\)</span> is smaller than some
<em>critical value</em> <span class="math">\(K_n\)</span>, then we shall deem the difference insignificant.
This is to take into account the fact that reality might deviate
from the ideal. <a class="reference internal" href="#sec-natural-variability"><span class="std std-numref">Section 6.4.4</span></a> mentions
that even for samples that truly come from a hypothesised distribution,
there is some inherent variability. We need to be somewhat tolerant.</p>
<p>Any authoritative textbook in statistics will tell us (and prove) that,
under the assumption that <span class="math">\(\hat{F}_n\)</span> is the ECDF of a sample
of <span class="math">\(n\)</span> independent variables <em>really</em> generated from a continuous CDF <span class="math">\(F\)</span>,
the random variable <span class="math">\(\hat{D}_n = \sup_{t\in\mathbb{R}} | \hat{F}_n(t) - F(t) |\)</span>
follows the Kolmogorov distribution with parameter <span class="math">\(n\)</span>
(available via <strong class="command">scipy.stats.kstwo</strong>).</p>
<p>In other words, if we generate many samples of length <span class="math">\(n\)</span> from <span class="math">\(F\)</span>,
and compute <span class="math">\(\hat{D}_n\)</span>s for each of them,
we expect it to be distributed like in <a class="reference internal" href="#fig-kolmogorovdistr"><span class="std std-numref">Figure 6.6</span></a>.</p>
<figure class="align-default" id="id35">
<span id="fig-kolmogorovdistr"></span><img alt="../_images/kolmogorovdistr-11.png" src="../_images/kolmogorovdistr-11.png" />
<figcaption>
<p><span class="caption-number">Figure 6.6 </span><span class="caption-text">Densities (left) and cumulative distribution functions (right) of some Kolmogorov distributions. The greater the sample size, the smaller the acceptable deviations between the theoretical and empirical CDFs.</span><a class="headerlink" href="#id35" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The choice <span class="math">\(K_n\)</span> involves a trade-off between our desire to:</p>
<ul class="simple">
<li><p>accept the null hypothesis when it is true
(data <em>really</em> come from <span class="math">\(F\)</span>), and</p></li>
<li><p>reject it when it is false (data follow some other distribution,
i.e., the difference is significant enough).</p></li>
</ul>
<p>These two needs are, unfortunately, mutually exclusive.</p>
<p>In practice, we assume some fixed upper bound (<em>significance level</em>)
for making the former kind of mistake, which we call the <em>type-I error</em>.
A nicely conservative (in a good way<a class="footnote-reference brackets" href="#footsmallalpha" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a>) value that we
suggest employing is <span class="math">\(\alpha=0.001=0.1\%\)</span>, i.e., only 1 out of 1000 samples
that really come from <span class="math">\(F\)</span> will be rejected as not coming from <span class="math">\(F\)</span>.</p>
<p>Such a <span class="math">\(K_n\)</span> may be determined by considering the inverse of the
CDF of the Kolmogorov distribution, <span class="math">\(\Xi_n\)</span>.
Namely, <span class="math">\(K_n=\Xi_n^{-1}(1-\alpha)\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span>  <span class="c1"># significance level</span>
<span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">kstwo</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="c1">## 0.029964456376393188</span>
</pre></div>
</div>
<p>In our case <span class="math">\(\hat{D}_n &lt; K_n\)</span> because <span class="math">\(0.01047 &lt; 0.02996\)</span>.
We conclude that our empirical (<code class="docutils literal notranslate"><span class="pre">heights</span></code>) distribution
does not differ significantly (at significance level <span class="math">\(0.1\%\)</span>)
from the assumed one, i.e., <span class="math">\(\mathrm{N}(160.1, 7.06)\)</span>.
In other words, we do not have enough evidence against the statement
that data are normally distributed. It is the presumption of innocence:
they are normal enough.</p>
<p>We will go back to this discussion in <a class="reference internal" href="#sec-natural-variability"><span class="std std-numref">Section 6.4.4</span></a>
and <a class="reference internal" href="430-group-by.html#sec-ks-test2"><span class="std std-numref">Section 12.2.6</span></a>.</p>
</section>
</section>
<section id="other-noteworthy-distributions">
<h2><span class="section-number">6.3. </span>Other noteworthy distributions<a class="headerlink" href="#other-noteworthy-distributions" title="Link to this heading">#</a></h2>
<section id="log-normal-distribution">
<span id="sec-log-normal-distribution"></span><h3><span class="section-number">6.3.1. </span>Log-normal distribution<a class="headerlink" href="#log-normal-distribution" title="Link to this heading">#</a></h3>
<p>We say that a sample is <em>log-normally distributed</em>,
if its logarithm is normally distributed.
Such a behaviour is frequently observed in biology and medicine
(size of living tissue), social sciences (number of sexual partners), or
technology (file sizes).
Also, recall that <a class="reference internal" href="#fig-heights-log"><span class="std std-numref">Figure 6.7</span></a> reveals that this
is the case for the most<a class="footnote-reference brackets" href="#footmost" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a> of the UK taxpayers’ incomes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">income</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching-data/master/marek/uk_income_simulated_2020.txt&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Count&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id36">
<span id="fig-heights-log"></span><img alt="../_images/heights-log-13.png" src="../_images/heights-log-13.png" />
<figcaption>
<p><span class="caption-number">Figure 6.7 </span><span class="caption-text">A histogram of the logarithm of incomes.</span><a class="headerlink" href="#id36" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Let us thus proceed with the fitting of a log-normal model,
L<span class="math">\(\mathrm{N}(\mu, \sigma)\)</span>.
The fitting process is similar to the normal case, but this time
we determine the mean and standard deviation based on the logarithms of data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lmu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">income</span><span class="p">))</span>
<span class="n">lsigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">lmu</span><span class="p">,</span> <span class="n">lsigma</span>
<span class="c1">## (10.314409794364623, 0.5816585197803816)</span>
</pre></div>
</div>
<p>We need to take note of the fact that <strong class="command">scipy.stats.lognorm</strong>
encodes the distribution via the parameter <span class="math">\(s\)</span> equal to <span class="math">\(\sigma\)</span>
and scale equal to <span class="math">\(e^\mu\)</span>. Computing the PDF at different points
must done as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="mi">101</span><span class="p">)</span>
<span class="n">fx</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">lognorm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">lsigma</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">lmu</span><span class="p">))</span>
</pre></div>
</div>
<p><a class="reference internal" href="#fig-heights-lognormal"><span class="std std-numref">Figure 6.8</span></a> depicts the fitted probability density
function together with the histograms on the log- and original scale.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">logbins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">geomspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="mi">31</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">income</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">logbins</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">fx</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>  <span class="c1"># log-scale on the x-axis</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Density&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">income</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">fx</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;PDF of LN(</span><span class="si">{</span><span class="n">lmu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">lsigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Density&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id37">
<span id="fig-heights-lognormal"></span><img alt="../_images/heights-lognormal-15.png" src="../_images/heights-lognormal-15.png" />
<figcaption>
<p><span class="caption-number">Figure 6.8 </span><span class="caption-text">A histogram and the probability density function of the fitted log-normal distribution for the <code class="docutils literal notranslate"><span class="pre">income</span></code> dataset, on log- (left) and original (right) scale.</span><a class="headerlink" href="#id37" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Overall, this fit is not too bad.
Nonetheless, we are only dealing with a sample of 1000 households;
the original UK Office of National Statistics
<a class="reference external" href="https://www.ons.gov.uk/peoplepopulationandcommunity/personalandhouseholdfinances/incomeandwealth/bulletins/householddisposableincomeandinequality/financialyear2020">data</a>
could tell us more about the quality of this model in general,
but it is beyond the scope of our simple exercise.</p>
<p>Furthermore, <a class="reference internal" href="#fig-qq-income"><span class="std std-numref">Figure 6.9</span></a> gives the quantile-quantile plot on a
double logarithmic scale for the above log-normal model.
Additionally, we (empirically) verify the hypothesis
of normality (using a “normal” normal distribution, not its “log”
version).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">qq_plot</span><span class="p">(</span>  <span class="c1"># see above for the definition</span>
    <span class="n">income</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">q</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">lognorm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">lsigma</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">lmu</span><span class="p">))</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantiles of LN(</span><span class="si">{</span><span class="n">lmu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">lsigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Sample quantiles&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">income</span><span class="p">)</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">income</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">qq_plot</span><span class="p">(</span><span class="n">income</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">q</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantiles of N(</span><span class="si">{</span><span class="n">mu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id38">
<span id="fig-qq-income"></span><img alt="../_images/qq-income-17.png" src="../_images/qq-income-17.png" />
<figcaption>
<p><span class="caption-number">Figure 6.9 </span><span class="caption-text">The Q-Q plots for the <code class="docutils literal notranslate"><span class="pre">income</span></code> dataset vs a fitted log-normal (good fit; left) and normal (bad fit; right) distribution.</span><a class="headerlink" href="#id38" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="proof proof-type-exercise" id="id39">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.3</span>
        
    </div><div class="proof-content">
<p>Graphically compare the empirical CDF for <code class="docutils literal notranslate"><span class="pre">income</span></code>
and the theoretical CDF of <span class="math">\(\mathrm{LN}(10.3, 0.58)\)</span>.</p>
</div></div><div class="proof proof-type-exercise" id="id40">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.4</span>
        
    </div><div class="proof-content">
<p>(*) Perform the Kolmogorov–Smirnov goodness-of-fit test
as in <a class="reference internal" href="#sec-ks-test"><span class="std std-numref">Section 6.2.3</span></a>, to verify that the hypothesis
of log-normality is not rejected at the <span class="math">\(\alpha=0.001\)</span> significance level.
At the same time, the income distribution significantly differs
from a normal one.</p>
</div></div><p>The hypothesis that our data follow
a normal distribution is most likely false.
On the other hand, the log-normal model, might be quite adequate.
It again reduced the whole dataset to merely two numbers,
<span class="math">\(\mu\)</span> and , based on which (and probability theory),
we may deduce that:</p>
<ul class="simple">
<li><p>the expected average (mean) income is <span class="math">\(e^{\mu + \sigma^2/2}\)</span>,</p></li>
<li><p>median is <span class="math">\(e^\mu\)</span>,</p></li>
<li><p>most probable one (mode) in <span class="math">\(e^{\mu-\sigma^2}\)</span>,</p></li>
</ul>
<p>etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Recall again that for skewed distributions such as this one,
reporting the mean might be misleading.
This is why <em>most</em> people get angry when they read the news
about the prospering economy (“yeah, we’d like to see that
kind of money in our pockets”). Hence, it is not only <span class="math">\(\mu\)</span> that matters,
it is also <span class="math">\(\sigma\)</span> that quantifies the discrepancy between the rich
and the poor (too much inequality is bad, but also too much uniformity is
to be avoided).</p>
</div>
<p>For a normal distribution, the situation is vastly different.
The mean, the median, and the most probable
outcomes tend to be the same: the distribution is
symmetric around <span class="math">\(\mu\)</span>.</p>
<div class="proof proof-type-exercise" id="id41">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.5</span>
        
    </div><div class="proof-content">
<p>What is the fraction of people with earnings
below the mean in our <span class="math">\(\mathrm{LN}(10.3, 0.58)\)</span> model?
Hint: use <strong class="command">scipy.stats.lognorm.cdf</strong> to get the answer.</p>
</div></div></section>
<section id="pareto-distribution">
<span id="sec-pareto"></span><h3><span class="section-number">6.3.2. </span>Pareto distribution<a class="headerlink" href="#pareto-distribution" title="Link to this heading">#</a></h3>
<p>Consider again the dataset
on the populations of the US cities in the 2000 US Census:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cities</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching-data/master/other/us_cities_2000.txt&quot;</span><span class="p">)</span>
<span class="nb">len</span><span class="p">(</span><span class="n">cities</span><span class="p">),</span> <span class="nb">sum</span><span class="p">(</span><span class="n">cities</span><span class="p">)</span>  <span class="c1"># number of cities, total population</span>
<span class="c1">## (19447, 175062893.0)</span>
</pre></div>
</div>
<p><a class="reference internal" href="#fig-hist-cities"><span class="std std-numref">Figure 6.10</span></a> gives the histogram
of the city sizes with the populations on the log-scale.
It kind of looks like a log-normal distribution again,
which the readers can inspect themselves when they are feeling playful.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logbins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">geomspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">cities</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">cities</span><span class="p">),</span> <span class="mi">21</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">cities</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">logbins</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Count&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id42">
<span id="fig-hist-cities"></span><img alt="../_images/hist-cities-19.png" src="../_images/hist-cities-19.png" />
<figcaption>
<p><span class="caption-number">Figure 6.10 </span><span class="caption-text">A histogram of the unabridged <code class="docutils literal notranslate"><span class="pre">cities</span></code> dataset. Note the log-scale on the x-axis.</span><a class="headerlink" href="#id42" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>This time, however, we will be interested in not what is <em>typical</em>,
but what is in some sense <em>anomalous</em> or <em>extreme</em>.
Let us look again at the <em>truncated</em> version of the city size
distribution by considering the cities with 10 000 or more inhabitants
(i.e., we will only study the right tail of the original data,
just like in <a class="reference internal" href="210-vector.html#sec-cities"><span class="std std-numref">Section 4.3.7</span></a>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="mi">10_000</span>
<span class="n">large_cities</span> <span class="o">=</span> <span class="n">cities</span><span class="p">[</span><span class="n">cities</span> <span class="o">&gt;=</span> <span class="n">s</span><span class="p">]</span>
<span class="nb">len</span><span class="p">(</span><span class="n">large_cities</span><span class="p">),</span> <span class="nb">sum</span><span class="p">(</span><span class="n">large_cities</span><span class="p">)</span>  <span class="c1"># number of cities, total population</span>
<span class="c1">## (2696, 146199374.0)</span>
</pre></div>
</div>
<p>Plotting the above on a double logarithmic scale
can be performed by calling <strong class="command">plt.yscale</strong><code class="code docutils literal notranslate"><span class="pre">(&quot;log&quot;)</span></code>,
which is left as an exercise.
Anyway, doing so will lead to a picture similar to <a class="reference internal" href="#fig-fit-pareto"><span class="std std-numref">Figure 6.11</span></a>
below. This reveals something remarkable. The bar tops on the
double log-scale are arranged more or less in a straight line.</p>
<p>There are many datasets that exhibit this behaviour.
We say that they follow a <em>power law</em> (power in the arithmetic sense, not
social one); see <span id="id13">[<a class="reference internal" href="999-bibliography.html#id43" title="Clauset, A., Shalizi, C.R., and Newman, M.E.J. (2009).  Power-law distributions in empirical data. SIAM Review, 51(4):661–703. DOI: 10.1137/070710111.">14</a>, <a class="reference internal" href="999-bibliography.html#id42" title="Newman, M.E.J. (2005).  Power laws, Pareto distributions and Zipf's law. Contemporary Physics, pages 323–351. DOI: 10.1080/00107510500052444.">68</a>]</span> for discussion.</p>
<p>Let us introduce the <em>Pareto distribution</em> family which has a prototypical
power law-like density. It is identified by two parameters:</p>
<ul class="simple">
<li><p>the (what <strong class="program">scipy</strong> calls it) scale parameter <span class="math">\(s&gt;0\)</span> is equal to
the shift from <span class="math">\(0\)</span>,</p></li>
<li><p>the shape parameter, <span class="math">\(\alpha&gt;0\)</span>, controls the slope of the said line on
the double log-scale.</p></li>
</ul>
<p>The probability density function of <span class="math">\(\mathrm{P}(\alpha, s)\)</span>
is given for <span class="math">\(x\ge s\)</span> by:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
f(x) = \frac{\alpha s^\alpha}{x^{\alpha+1}},
\]</div>
</div>
<p>and <span class="math">\(f(x)=0\)</span> otherwise.</p>
<p><span class="math">\(s\)</span> is usually taken as the sample minimum (i.e., 10 000 in our case).
<span class="math">\(\alpha\)</span> can be estimated through the reciprocal of
the mean of the scaled logarithms of our observations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">large_cities</span><span class="o">/</span><span class="n">s</span><span class="p">))</span>
<span class="n">alpha</span>
<span class="c1">## 0.9496171695997675</span>
</pre></div>
</div>
<p><a class="reference internal" href="#fig-fit-pareto"><span class="std std-numref">Figure 6.11</span></a> allows us to compare the theoretical density
and an empirical histogram on the log-scale.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logbins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">geomspace</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">large_cities</span><span class="p">),</span> <span class="mi">21</span><span class="p">)</span>  <span class="c1"># bin boundaries</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">large_cities</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">logbins</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">logbins</span><span class="p">,</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pareto</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">logbins</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s</span><span class="p">),</span>
    <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;PDF of P(</span><span class="si">{</span><span class="n">alpha</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Density&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id43">
<span id="fig-fit-pareto"></span><img alt="../_images/fit-pareto-21.png" src="../_images/fit-pareto-21.png" />
<figcaption>
<p><span class="caption-number">Figure 6.11 </span><span class="caption-text">A histogram of the <code class="docutils literal notranslate"><span class="pre">large_cities</span></code> dataset and the fitted density on a double log-scale.</span><a class="headerlink" href="#id43" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#fig-qq-pareto"><span class="std std-numref">Figure 6.12</span></a> gives the corresponding Q-Q plot on
a double logarithmic scale.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">qq_plot</span><span class="p">(</span>  <span class="c1"># defined above</span>
    <span class="n">large_cities</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">q</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pareto</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantiles of P(</span><span class="si">{</span><span class="n">alpha</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Sample quantiles&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id44">
<span id="fig-qq-pareto"></span><img alt="../_images/qq-pareto-23.png" src="../_images/qq-pareto-23.png" />
<figcaption>
<p><span class="caption-number">Figure 6.12 </span><span class="caption-text">The Q-Q plot for the <code class="docutils literal notranslate"><span class="pre">large_cites</span></code> dataset vs the fitted Paretian model.</span><a class="headerlink" href="#id44" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>We see that the populations of the largest cities are overestimated.
The model could be better, but the cities are still growing, right?</p>
<div class="proof proof-type-example" id="id45">

    <div class="proof-title">
        <span class="proof-type">Example 6.6</span>
        
    </div><div class="proof-content">
<p>(*) It might also be interesting to see how well we can predict the
probability of a randomly selected city being at least a given size.
Let us denote by <span class="math">\(S(x)=1-F(x)\)</span> the <em>complementary
cumulative distribution function</em> (CCDF; sometimes referred to as
the survival function),
and by <span class="math">\(\hat{S}_n(x)=1-\hat{F}_n(x)\)</span> its empirical version.
<a class="reference internal" href="#fig-ccdf-pareto"><span class="std std-numref">Figure 6.13</span></a> compares the empirical and the fitted CCDFs
with probabilities on the linear- and log-scale.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">geomspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">large_cities</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">large_cities</span><span class="p">),</span> <span class="mi">1001</span><span class="p">)</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pareto</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">large_cities</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">probs</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;CCDF of P(</span><span class="si">{</span><span class="n">alpha</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">large_cities</span><span class="p">),</span> <span class="mi">1</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">,</span>
        <span class="n">drawstyle</span><span class="o">=</span><span class="s2">&quot;steps-post&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Empirical CCDF&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">([</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="s2">&quot;log&quot;</span><span class="p">][</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Prob(city size &gt; x)&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id46">
<span id="fig-ccdf-pareto"></span><img alt="../_images/ccdf-pareto-25.png" src="../_images/ccdf-pareto-25.png" />
<figcaption>
<p><span class="caption-number">Figure 6.13 </span><span class="caption-text">The empirical and theoretical complementary cumulative distribution functions for the <code class="docutils literal notranslate"><span class="pre">large_cities</span></code> dataset with probabilities on the linear- (left) and log-scale (right) and city sizes on the log-scale.</span><a class="headerlink" href="#id46" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>In terms of the maximal absolute distance between the two functions,
<span class="math">\(\hat{D}_n\)</span>, from the left plot we see that the fit seems acceptable.
Still, let us stress that the log-scale overemphasises the relatively
minor differences in the right tail and should not be used for judging
the value of <span class="math">\(\hat{D}_n\)</span>.</p>
<p>However, that the Kolmogorov–Smirnov goodness-of-fit test
rejects the hypothesis of Paretianity (at a significance level <span class="math">\(0.1\%\)</span>)
is left as an exercise for the reader.</p>
</div></div></section>
<section id="uniform-distribution">
<h3><span class="section-number">6.3.3. </span>Uniform distribution<a class="headerlink" href="#uniform-distribution" title="Link to this heading">#</a></h3>
<p>Consider the Polish <em>Lotto</em> lottery, where six numbered balls
<span class="math">\(\{1,2,\dots,49\}\)</span> are drawn without replacement from an urn.
We have a dataset that summarises the number of times
each ball has been drawn in all the games in the period 1957–2016.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lotto</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching-data/master/marek/lotto_table.txt&quot;</span><span class="p">)</span>
<span class="n">lotto</span>
<span class="c1">## array([720., 720., 714., 752., 719., 753., 701., 692., 716., 694., 716.,</span>
<span class="c1">##        668., 749., 713., 723., 693., 777., 747., 728., 734., 762., 729.,</span>
<span class="c1">##        695., 761., 735., 719., 754., 741., 750., 701., 744., 729., 716.,</span>
<span class="c1">##        768., 715., 735., 725., 741., 697., 713., 711., 744., 652., 683.,</span>
<span class="c1">##        744., 714., 674., 654., 681.])</span>
</pre></div>
</div>
<p>Each event seems to occur more or less with the same probability.
Of course, the numbers on the balls are integer,
but in our idealised scenario, we may try modelling this dataset
using a continuous <em>uniform distribution</em>,
which yields arbitrary real numbers on a given interval <span class="math">\((a, b)\)</span>,
i.e., between some <span class="math">\(a\)</span> and <span class="math">\(b\)</span>.
We denote such a distribution by <span class="math">\(\mathrm{U}(a, b)\)</span>. It
has the probability density function given for <span class="math">\(x\in(a, b)\)</span> by:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
f(x) = \frac{1}{b-a},
\]</div>
</div>
<p>and <span class="math">\(f(x)=0\)</span> otherwise.</p>
<p>Notice that <strong class="command">scipy.stats.uniform</strong>
uses parameters <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">scale</span></code> equal to <span class="math">\(b-a\)</span> instead.</p>
<p>In our case, it makes sense to set <span class="math">\(a=1\)</span> and <span class="math">\(b=50\)</span> and interpret
an outcome like 49.1253 as representing the 49th ball
(compare the notion of the floor function, <span class="math">\(\lfloor x\rfloor\)</span>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">1001</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">lotto</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">lotto</span><span class="p">),</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s2">&quot;edge&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">49</span><span class="p">),</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;PDF of U(1, 50)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.025</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id47">
<span id="fig-lotto"></span><img alt="../_images/lotto-27.png" src="../_images/lotto-27.png" />
<figcaption>
<p><span class="caption-number">Figure 6.14 </span><span class="caption-text">A histogram of the <code class="docutils literal notranslate"><span class="pre">lotto</span></code> dataset.</span><a class="headerlink" href="#id47" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Visually, see <a class="reference internal" href="#fig-lotto"><span class="std std-numref">Figure 6.14</span></a>, this model makes much sense,
but again, some more rigorous statistical testing would be required to
determine if someone has not been tampering with the lottery results, i.e.,
if data does not deviate from the uniform distribution significantly.</p>
<p>Unfortunately, we cannot use the Kolmogorov–Smirnov test in the version
defined above as data are not continuous.
See, however, <a class="reference internal" href="420-categorical.html#sec-chisq-test"><span class="std std-numref">Section 11.4.3</span></a>
for the Pearson chi-squared test that is applicable here.</p>
<div class="proof proof-type-exercise" id="id48">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.7</span>
        
    </div><div class="proof-content">
<p>Does playing lotteries
and engaging in gambling make <em>rational</em> sense at all,
from the perspective of an individual player?
Well, we see that 16 is the most frequently occurring outcome
in <em>Lotto</em>, maybe there’s some magic in it?
Also, some people sometimes became millionaires, right?</p>
</div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>In data modelling (e.g., Bayesian statistics),
sometimes a uniform distribution is chosen
as a placeholder for “we know nothing about a phenomenon,
so let us just assume that every event is equally likely”.
Nonetheless, it is quite fascinating that the real world tends
to be structured after all. Emerging patterns are plentiful,
most often they are far from being uniformly distributed.
Even more strikingly, they are subject to quantitative analysis.</p>
</div>
</section>
<section id="distribution-mixtures">
<span id="sec-mixtures"></span><h3><span class="section-number">6.3.4. </span>Distribution mixtures (*)<a class="headerlink" href="#distribution-mixtures" title="Link to this heading">#</a></h3>
<p>Some datasets may fail to fit through simple models such as the ones
described above. It may sometimes be due to their non-random behaviour:
statistics gives just one means to create data idealisations,
we also have partial differential equations, approximation theory,
graphs and complex networks, agent-based modelling, and so forth,
which might be worth giving a study (and then try).</p>
<p>Another reason may be that what we observe is, in fact, a <em>mixture</em>
(creative combination) of simpler processes.</p>
<p>The dataset representing the December 2021 hourly averages
pedestrian counts near the Southern Cross Station in Melbourne
is a representative instance of such a scenario;
compare <a class="reference internal" href="210-vector.html#fig-peds-histogram"><span class="std std-numref">Figure 4.5</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">peds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching-data/master/marek/southern_cross_station_peds_2019_dec.txt&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>It might not be a bad idea to
try to fit a probabilistic (convex) combination
of three normal distributions <span class="math">\(f_1\)</span>, <span class="math">\(f_2\)</span>, <span class="math">\(f_3\)</span>,
corresponding to the morning, lunchtime, and evening
pedestrian count peaks. This yields the PDF:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
f(x) = w_1 f_1(x) + w_2 f_2(x) + w_3 f_3(x),
\]</div>
</div>
<p>for some coefficients <span class="math">\(w_1,w_2,w_3\ge 0\)</span> such that <span class="math">\(w_1+w_2+w_3=1\)</span>.</p>
<p><a class="reference internal" href="#fig-mixture"><span class="std std-numref">Figure 6.15</span></a> depicts a mixture of
<span class="math">\(\mathrm{N}(8, 1)\)</span>, <span class="math">\(\mathrm{N}(12, 1)\)</span>, and <span class="math">\(\mathrm{N}(17, 2)\)</span>
with the corresponding weights of <span class="math">\(0.35\)</span>, <span class="math">\(0.1\)</span>, and <span class="math">\(0.55\)</span>.
This dataset is quite coarse-grained
(we only have 24 bar heights at our disposal). Consequently,
the estimated coefficients should be taken with a pinch of chilli pepper.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">24</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">peds</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">peds</span><span class="p">),</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">p1</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">p2</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">p3</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.35</span><span class="o">*</span><span class="n">p1</span> <span class="o">+</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">p2</span> <span class="o">+</span> <span class="mf">0.55</span><span class="o">*</span><span class="n">p3</span>  <span class="c1"># weighted combination of 3 densities</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;PDF of a normal mixture&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id49">
<span id="fig-mixture"></span><img alt="../_images/mixture-29.png" src="../_images/mixture-29.png" />
<figcaption>
<p><span class="caption-number">Figure 6.15 </span><span class="caption-text">A histogram of the <code class="docutils literal notranslate"><span class="pre">peds</span></code> dataset and a guesstimated mixture of three normal distributions.</span><a class="headerlink" href="#id49" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>It will frequently be the case in data wrangling
that more complex entities (models, methods) will be arising as combinations
of simpler (primitive) components.
This is why we ought to spend a great deal of time
studying the <em>fundamentals</em>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some data clustering techniques
(in particular, the <span class="math">\(k\)</span>-means algorithm that we briefly discuss
later in this course)
could be used to split a data sample into disjoint chunks
corresponding to different mixture components.</p>
<p>Also, it might be the case that the mixture components can
be explained by another categorical variable that
divides the dataset into natural groups; compare
<a class="reference internal" href="430-group-by.html#chap-group-by"><span class="std std-numref">Chapter 12</span></a>.</p>
</div>
</section>
</section>
<section id="generating-pseudorandom-numbers">
<span id="sec-pseudorandom"></span><h2><span class="section-number">6.4. </span>Generating pseudorandom numbers<a class="headerlink" href="#generating-pseudorandom-numbers" title="Link to this heading">#</a></h2>
<p>A probability distribution is useful not only for describing a dataset.
It also enables us to perform many experiments on data that we do not
currently have, but we might obtain in the future,
to test various scenarios and hypotheses.</p>
<p>To do this, we can generate a random sample of
independent (not related to each other) observations.</p>
<section id="id14">
<h3><span class="section-number">6.4.1. </span>Uniform distribution<a class="headerlink" href="#id14" title="Link to this heading">#</a></h3>
<p>When most people say <em>random</em>, they implicitly mean
<em>uniformly distributed</em>. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="c1">## array([0.69646919, 0.28613933, 0.22685145, 0.55131477, 0.71946897])</span>
</pre></div>
</div>
<p>gives five observations sampled independently from the uniform distribution
on the unit interval, i.e., <span class="math">\(\mathrm{U}(0, 1)\)</span>.</p>
<p>The same with <strong class="program">scipy</strong>, but this time the support will be
<span class="math">\((-10, 15)\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>  <span class="c1"># from -10 to -10+25</span>
<span class="c1">## array([ 0.5776615 , 14.51910496,  7.12074346,  2.02329754, -0.19706205])</span>
</pre></div>
</div>
<p>Alternatively, we could do that ourselves by shifting
and scaling the output of the random number generator
on the unit interval using the formula
<strong class="command">numpy.random.rand</strong><code class="code docutils literal notranslate"><span class="pre">(5)*25-10</span></code>.</p>
</section>
<section id="not-exactly-random">
<span id="sec-seed"></span><h3><span class="section-number">6.4.2. </span>Not exactly random<a class="headerlink" href="#not-exactly-random" title="Link to this heading">#</a></h3>
<p>We generate numbers using a computer, which is purely
deterministic. Hence, we shall refer to them as <em>pseudorandom</em>
or random-like ones (albeit they are indistinguishable from truly random,
when subject to rigorous tests for randomness).</p>
<p>To prove it, we can set the initial state of the generator
(the <em>seed</em>) via some number and see what values are produced:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>  <span class="c1"># set seed</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="c1">## array([0.69646919, 0.28613933, 0.22685145, 0.55131477, 0.71946897])</span>
</pre></div>
</div>
<p>Then, we set the seed once again via the same number and
see how “random” the next values are:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>  <span class="c1"># set seed</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="c1">## array([0.69646919, 0.28613933, 0.22685145, 0.55131477, 0.71946897])</span>
</pre></div>
</div>
<p>This enables us to perform completely <em>reproducible</em> numerical
experiments. This feature is very welcome. Truly scientific
inquiries tend to nourish identical results under the same conditions.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If we do not set the seed manually,
it will be initialised based on the current wall time, which is
different every… time. As a result, the numbers will <em>seem</em> random to us.</p>
</div>
<p>Many Python packages that we will be using in the future,
including <strong class="program">pandas</strong> and <strong class="program">scikit-learn</strong>, rely
on <strong class="program">numpy</strong>’s random number generator.
We will become used to calling <strong class="command">numpy.random.seed</strong>
to make them predictable.</p>
<p>Additionally, some of them
(e.g., <strong class="command">sklearn.model_selection.train_test_split</strong>
or <strong class="command">pandas.DataFrame.sample</strong>) are equipped with the <code class="docutils literal notranslate"><span class="pre">random_state</span></code>
argument, which behaves as if we <em>temporarily</em> changed
the seed (for just one call to that function). For instance:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="c1">## array([0.69646919, 0.28613933, 0.22685145, 0.55131477, 0.71946897])</span>
</pre></div>
</div>
<p>This gives the same sequence as above.</p>
</section>
<section id="sampling-from-other-distributions">
<h3><span class="section-number">6.4.3. </span>Sampling from other distributions<a class="headerlink" href="#sampling-from-other-distributions" title="Link to this heading">#</a></h3>
<p>Generating data from other distributions is possible too;
there are many <strong class="command">rvs</strong> methods implemented
in <strong class="program">scipy.stats</strong>.
For example, here is a sample from <span class="math">\(\mathrm{N}(100, 16)\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">50489</span><span class="p">)</span>
<span class="c1">## array([113.41134015,  46.99328545, 157.1304154 ])</span>
</pre></div>
</div>
<p>Pseudorandom deviates from the <em>standard</em> normal distribution,
i.e., <span class="math">\(\mathrm{N}(0, 1)\)</span>, can also be generated using
<strong class="command">numpy.random.randn</strong>.
As <span class="math">\(\mathrm{N}(100, 16)\)</span> is a scaled and shifted version thereof,
the above is equivalent to:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">50489</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">*</span><span class="mi">16</span> <span class="o">+</span> <span class="mi">100</span>
<span class="c1">## array([113.41134015,  46.99328545, 157.1304154 ])</span>
</pre></div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Conclusions based on simulated data are trustworthy for they cannot
be manipulated. Or can they?</p>
<p>The pseudorandom number generator’s seed used above,
<code class="docutils literal notranslate"><span class="pre">50489</span></code>, is quite suspicious. It might suggest that someone
wanted to <em>prove</em> some point (in this case, the violation
of the <span class="math">\(3\sigma\)</span> rule).</p>
<p>This is why we recommend sticking to only one seed most of the time,
e.g., <code class="docutils literal notranslate"><span class="pre">123</span></code>, or – when performing simulations – setting
consecutive seeds for each iteration: <code class="docutils literal notranslate"><span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">2</span></code>, ….</p>
</div>
<div class="proof proof-type-exercise" id="id50">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.8</span>
        
    </div><div class="proof-content">
<p>Generate 1000 pseudorandom numbers from the log-normal
distribution and draw a histogram thereof.</p>
</div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>(*)
Having a reliable pseudorandom number generator from the uniform distribution
on the unit interval is crucial as sampling from other distributions
usually involves transforming independent <span class="math">\(\mathrm{U}(0, 1)\)</span> variates.</p>
<p>For instance, realisations of random variables following any continuous
cumulative distribution function <span class="math">\(F\)</span> can be constructed through
the <em>inverse transform sampling</em> (see <span id="id15">[<a class="reference internal" href="999-bibliography.html#id49" title="Gentle, J.E. (2003).  Random Number Generation and Monte Carlo Methods. Springer.">37</a>, <a class="reference internal" href="999-bibliography.html#id51" title="Robert, C.P. and Casella, G. (2004).  Monte Carlo Statistical Methods. Springer-Verlag.">78</a>]</span>):</p>
<ol class="arabic simple">
<li><p>Generate a sample <span class="math">\(x_1,\dots,x_n\)</span> independently from <span class="math">\(\mathrm{U}(0, 1)\)</span>.</p></li>
<li><p>Transform each <span class="math">\(x_i\)</span> by applying the quantile function,
<span class="math">\(y_i=F^{-1}(x_i)\)</span>.</p></li>
</ol>
<p>Now <span class="math">\(y_1,\dots,y_n\)</span> follows the CDF <span class="math">\(F\)</span>.</p>
</div>
<div class="proof proof-type-exercise" id="id51">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.9</span>
        
    </div><div class="proof-content">
<p>(*) Generate 1000 pseudorandom numbers from the log-normal
distribution using inverse transform sampling.</p>
</div></div><div class="proof proof-type-exercise" id="id52">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.10</span>
        
    </div><div class="proof-content">
<p>(**) Generate 1000 pseudorandom numbers from the distribution
mixture discussed in <a class="reference internal" href="#sec-mixtures"><span class="std std-numref">Section 6.3.4</span></a>.</p>
</div></div></section>
<section id="natural-variability">
<span id="sec-natural-variability"></span><h3><span class="section-number">6.4.4. </span>Natural variability<a class="headerlink" href="#natural-variability" title="Link to this heading">#</a></h3>
<p>Even a sample truly generated from a specific distribution
will deviate from it, sometimes considerably.
Such effects will be especially visible for small sample sizes,
but they usually disappear<a class="footnote-reference brackets" href="#footfts" id="id16" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a> when the availability of data increases.</p>
<p>For example, <a class="reference internal" href="#fig-natural-variability"><span class="std std-numref">Figure 6.16</span></a> depicts the histograms of nine
different samples of size 100, all drawn independently from the
standard normal distribution.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.figsize&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># width=height</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id53">
<span id="fig-natural-variability"></span><img alt="../_images/natural-variability-31.png" src="../_images/natural-variability-31.png" />
<figcaption>
<p><span class="caption-number">Figure 6.16 </span><span class="caption-text">All nine samples are normally distributed.</span><a class="headerlink" href="#id53" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>There is some ruggedness in the bar sizes that a naïve observer
might try to interpret as something meaningful.
A competent data scientist must train
their eye to ignore such impurities.
In this case, they are only due to random effects.
Nevertheless, we must always be ready to detect cases
which are worth attention.</p>
<div class="proof proof-type-exercise" id="id54">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.11</span>
        
    </div><div class="proof-content">
<p>Repeat the above experiment for samples of sizes 10, 1 000, and 10 000.</p>
</div></div><div class="proof proof-type-example" id="id55">

    <div class="proof-title">
        <span class="proof-type">Example 6.12</span>
        
    </div><div class="proof-content">
<p>(*) Using a simple Monte Carlo simulation,
we can verify (approximately) that the Kolmogorov–Smirnov
goodness-of-fit test introduced in <a class="reference internal" href="#sec-ks-test"><span class="std std-numref">Section 6.2.3</span></a>
has been calibrated properly,
i.e., that for samples that really follow the assumed distribution,
the null hypothesis is rejected only in roughly 0.1% of the cases.</p>
<p>Let us say we are interested in the null hypothesis
referencing the standard normal distribution, <span class="math">\(\mathrm{N}(0, 1)\)</span>,
and sample size <span class="math">\(n=100\)</span>.
We need to generate many (we assume 10 000 below) such samples for each of
which we compute and store the maximal absolute deviation from
the theoretical CDF, i.e., <span class="math">\(\hat{D}_n\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">distrib</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># assumed distribution: N(0, 1)</span>
<span class="n">Dns</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>  <span class="c1"># increase this for better precision</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">distrib</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># really follows distrib</span>
    <span class="n">Dns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">compute_Dn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">distrib</span><span class="o">.</span><span class="n">cdf</span><span class="p">))</span>
<span class="n">Dns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Dns</span><span class="p">)</span>
</pre></div>
</div>
<p>Now let us compute the proportion of cases which lead to
<span class="math">\(\hat{D}_n\)</span> greater than the critical value <span class="math">\(K_n\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">Dns</span><span class="p">[</span><span class="n">Dns</span> <span class="o">&gt;=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">kstwo</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n</span><span class="p">)])</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">Dns</span><span class="p">)</span>
<span class="c1">## 0.0016</span>
</pre></div>
</div>
<p>In theory, this should be equal to 0.001.
But our values are necessarily approximate because we rely on randomness.
Increasing the number of trials from 10 000 to, say, 1 000 000
will make the above estimate more precise.</p>
<p>It is also worth checking out that
the density histogram of <code class="docutils literal notranslate"><span class="pre">Dns</span></code> resembles the Kolmogorov distribution
that we can compute via <strong class="command">scipy.stats.kstwo.pdf</strong>.</p>
</div></div><div class="proof proof-type-exercise" id="id56">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.13</span>
        
    </div><div class="proof-content">
<p>(*) It might also be interesting to check out the test’s <em>power</em>,
i.e., the probability that when the null hypothesis is false,
it will actually be rejected.
Modify the above code in such a way that <code class="docutils literal notranslate"><span class="pre">x</span></code> in the <strong class="command">for</strong> loop
is not generated from <span class="math">\(\mathrm{N}(0, 1)\)</span>, but
<span class="math">\(\mathrm{N}(0.1, 1)\)</span>, <span class="math">\(\mathrm{N}(0.2, 1)\)</span>, etc.,
and check the proportion of cases
where we deem the sample distribution different from
<span class="math">\(\mathrm{N}(0, 1)\)</span>.
Small differences in the location parameter <span class="math">\(\mu\)</span> are usually
ignored, and this improves with sample size <span class="math">\(n\)</span>.</p>
</div></div></section>
<section id="adding-jitter-white-noise">
<h3><span class="section-number">6.4.5. </span>Adding jitter (white noise)<a class="headerlink" href="#adding-jitter-white-noise" title="Link to this heading">#</a></h3>
<p>We mentioned that measurements might be subject to observational
error. Rounding can also occur as early as
the data collection phase. In particular, our <code class="docutils literal notranslate"><span class="pre">heights</span></code>
dataset is precise up to 1 fractional digit.
However, in statistics, when we say that data follow
a continuous distribution, the probability of having two identical
values in a sample is 0. Therefore, some data analysis methods
might assume that there are no ties in the input vector, i.e.,
all values are unique.</p>
<p>The easiest way to deal with such numerical inconveniences
is to add some white noise with the expected value of 0,
either uniformly or normally distributed.</p>
<p>For example, for <code class="docutils literal notranslate"><span class="pre">heights</span></code> it makes sense to add some jitter from
<span class="math">\(\mathrm{U}[-0.05, 0.05]\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">heights_jitter</span> <span class="o">=</span> <span class="n">heights</span> <span class="o">+</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">heights</span><span class="p">))</span><span class="o">*</span><span class="mf">0.1</span><span class="o">-</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">heights_jitter</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>  <span class="c1"># preview</span>
<span class="c1">## array([160.21704623, 152.68870195, 161.24482407, 157.3675293 ,</span>
<span class="c1">##        154.61663465, 144.68964596])</span>
</pre></div>
</div>
<p>Adding noise also might be performed for aesthetic reasons,
e.g., when drawing scatter plots.</p>
</section>
<section id="independence-assumption">
<h3><span class="section-number">6.4.6. </span>Independence assumption<a class="headerlink" href="#independence-assumption" title="Link to this heading">#</a></h3>
<p>Let us generate nine binary digits in a random fashion:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">251</span><span class="p">)</span>  <span class="c1"># HIDDEN</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="mi">9</span><span class="p">)</span>
<span class="c1">## array([1, 1, 1, 1, 1, 1, 1, 1, 1])</span>
</pre></div>
</div>
<p>We can consider ourselves very lucky; all numbers are the same.
So, the next number <em>must</em> be a “zero”, finally, right?</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1">## array([1])</span>
</pre></div>
</div>
<p>Wrong. The numbers we generate are <em>independent</em> of each other.
There is no history. In the above model of randomness (Bernoulli trials;
two possible outcomes with the same probability), there is a 50% chance
of obtaining a “one” <em>regardless</em> of how many “ones” were observed
previously.</p>
<p>We should not seek patterns where there are none. Our brain forms
expectations about the world, which are difficult to overcome.
But the reality could not care less about what we consider it to be.</p>
</section>
</section>
<section id="further-reading">
<h2><span class="section-number">6.5. </span>Further reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h2>
<p>For an excellent general introductory course on probability
and statistics, see <span id="id17">[<a class="reference internal" href="999-bibliography.html#id50" title="Gentle, J.E. (2009).  Computational Statistics. Springer-Verlag.">38</a>, <a class="reference internal" href="999-bibliography.html#id45" title="Gentle, J.E. (2020).  Theory of Statistics. book draft. URL: https://mason.gmu.edu/~jgentle/books/MathStat.pdf.">40</a>]</span>
and also <span id="id18">[<a class="reference internal" href="999-bibliography.html#id155" title="Ross, S.M. (2020).  Introduction to Probability and Statistics for Engineers and Scientists. Academic Press.">79</a>]</span>.
More advanced students are likely to enjoy other
classics such as <span id="id19">[<a class="reference internal" href="999-bibliography.html#id157" title="Bartoszyński, R. and Niewiadomska-Bugaj, M. (2007).  Probability and Statistical Inference. Wiley.">4</a>, <a class="reference internal" href="999-bibliography.html#id46" title="Billingsley, P. (1995).  Probability and Measure. John Wiley &amp; Sons.">7</a>, <a class="reference internal" href="999-bibliography.html#id44" title="Cramér, H. (1946).  Mathematical Methods of Statistics. Princeton University Press. URL: https://archive.org/details/in.ernet.dli.2015.223699.">17</a>, <a class="reference internal" href="999-bibliography.html#id156" title="Feller, W. (1950).  An Introduction to Probability Theory and Its Applications: Volume I. Wiley.">26</a>]</span>.
To go beyond the fundamentals, check out <span id="id20">[<a class="reference internal" href="999-bibliography.html#id106" title="Efron, B. and Hastie, T. (2016).  Computer Age Statistical Inference: Algorithms, Evidence, and Data Science. Cambridge University Press.">24</a>]</span>.
Topics in random number generation are covered in
<span id="id21">[<a class="reference internal" href="999-bibliography.html#id49" title="Gentle, J.E. (2003).  Random Number Generation and Monte Carlo Methods. Springer.">37</a>, <a class="reference internal" href="999-bibliography.html#id141" title="Knuth, D.E. (1997).  The Art of Computer Programming II: Seminumerical Algorithms. Addison-Wesley.">56</a>, <a class="reference internal" href="999-bibliography.html#id51" title="Robert, C.P. and Casella, G. (2004).  Monte Carlo Statistical Methods. Springer-Verlag.">78</a>]</span>.</p>
<p>For a more detailed introduction to exploratory data analysis,
see the classical books by Tukey <span id="id22">[<a class="reference internal" href="999-bibliography.html#id145" title="Tukey, J.W. (1962).  The future of data analysis. Annals of Mathematical Statistics, 33(1):1–67. URL: https://projecteuclid.org/journalArticle/Download?urlId=10.1214%2Faoms%2F1177704711, DOI: 10.1214/aoms/1177704711.">87</a>, <a class="reference internal" href="999-bibliography.html#id144" title="Tukey, J.W. (1977).  Exploratory Data Analysis. Addison-Wesley.">88</a>]</span>
and Tufte <span id="id23">[<a class="reference internal" href="999-bibliography.html#id143" title="Tufte, E.R. (2001).  The Visual Display of Quantitative Information. Graphics Press.">86</a>]</span>.</p>
<div style="margin-top: 1em"></div><p>We took the logarithm of the log-normally distributed
incomes and obtained a normally distributed sample.
In statistical practice, it is not rare to apply different
non-linear transforms of the input vectors at the data preprocessing stage (see, e.g.,
<a class="reference internal" href="330-relationship.html#sec-linearisation"><span class="std std-numref">Section 9.2.6</span></a>). In particular, the Box–Cox (power) transform
<span id="id24">[<a class="reference internal" href="999-bibliography.html#id146" title="Box, G.E.P. and Cox, D.R. (1964).  An analysis of transformations. Journal of the Royal Statistical Society. Series B (Methodological), 26(2):211–252.">10</a>]</span> is of the form <span class="math">\(x\mapsto \frac{x^\lambda-1}{\lambda}\)</span>
for some <span class="math">\(\lambda\)</span>.
Interestingly, in the limit as <span class="math">\(\lambda\to 0\)</span>, this formula yields
<span class="math">\(x\mapsto \log x\)</span> which is exactly what we were applying in this chapter.</p>
<p><span id="id25">[<a class="reference internal" href="999-bibliography.html#id43" title="Clauset, A., Shalizi, C.R., and Newman, M.E.J. (2009).  Power-law distributions in empirical data. SIAM Review, 51(4):661–703. DOI: 10.1137/070710111.">14</a>, <a class="reference internal" href="999-bibliography.html#id42" title="Newman, M.E.J. (2005).  Power laws, Pareto distributions and Zipf's law. Contemporary Physics, pages 323–351. DOI: 10.1080/00107510500052444.">68</a>]</span> give a nice overview of the power-law-like
behaviour of some “rich” or otherwise extreme datasets.
It is worth noting that the logarithm of a Paretian sample
divided by the minimum follows an exponential distribution
(which we discuss in <a class="reference internal" href="530-time-series.html#chap-time-series"><span class="std std-numref">Chapter 16</span></a>).
For a comprehensive catalogue of statistical distributions,
their properties, and relationships between them,
see <span id="id26">[<a class="reference internal" href="999-bibliography.html#id83" title="Forbes, C., Evans, M., Hastings, N., and Peacock, B. (2010).  Statistical Distributions. Wiley.">27</a>]</span>.</p>
</section>
<section id="exercises">
<h2><span class="section-number">6.6. </span>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<div class="proof proof-type-exercise" id="id57">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.14</span>
        
    </div><div class="proof-content">
<p>Why is the notion of the mean income confusing the general public?</p>
</div></div><div class="proof proof-type-exercise" id="id58">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.15</span>
        
    </div><div class="proof-content">
<p>When manually setting the seed of a random number generator
makes sense?</p>
</div></div><div class="proof proof-type-exercise" id="id59">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.16</span>
        
    </div><div class="proof-content">
<p>Given a log-normally distributed sample <code class="docutils literal notranslate"><span class="pre">x</span></code>, how  can we turn it
to a normally distributed one, i.e., <code class="docutils literal notranslate"><span class="pre">y=</span></code><strong class="command">f</strong><code class="code docutils literal notranslate"><span class="pre">(x)</span></code>,
with <strong class="command">f</strong> being… what?</p>
</div></div><div class="proof proof-type-exercise" id="id60">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.17</span>
        
    </div><div class="proof-content">
<p>What is the <span class="math">\(3\sigma\)</span> rule for normally distributed data?</p>
</div></div><div class="proof proof-type-exercise" id="id61">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.18</span>
        
    </div><div class="proof-content">
<p>(*) How can we verify graphically if a sample follows a hypothesised
theoretical distribution?</p>
</div></div><div class="proof proof-type-exercise" id="id62">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.19</span>
        
    </div><div class="proof-content">
<p>(*) Explain the meaning of the type I error, significance level,
and a test’s power.</p>
</div></div><hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="foothistconvergence" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">1</a><span class="fn-bracket">]</span></span>
<p>(*) This intuition is, of course, theoretically
grounded and is based on the asymptotic behaviour of the histograms
as the estimators of the underlying probability density function,
see, e.g., <span id="id27">[<a class="reference internal" href="999-bibliography.html#id48" title="Freedman, D. and Diaconis, P. (1981).  On the histogram as a density estimator: L₂ theory. Zeitschrift für Wahrscheinlichkeitstheorie und Verwandte Gebiete, 57:453–476.">28</a>]</span> and the many references therein.</p>
</aside>
<aside class="footnote brackets" id="footmle" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">2</a><span class="fn-bracket">]</span></span>
<p>(*) It might be the case that we will have to obtain
the estimates of the probability distribution’s parameters
by numerical optimisation, for example, by minimising
<span class="math">\(
\mathcal{L}(\mu, \sigma) = \sum_{i=1}^n \left(
    \frac{(x_i-\mu)^2}{\sigma^2} + \log \sigma^2
\right)
\)</span>
with respect to <span class="math">\(\mu\)</span> and <span class="math">\(\sigma\)</span> (corresponding to the objective
function in the maximum likelihood estimation problem for the normal
distribution family).  In our case, however, we are lucky;
there exist open-form formulae expressing the solution to the above,
exactly in the form of the sample mean and standard
deviation. For other distributions, things can get a little trickier,
though. Furthermore, sometimes we will have many options for point
estimators to choose from, which might be more suitable if data are
not of top quality (e.g., contain outliers). For instance, in the normal
model, it can be shown that we can also estimate <span class="math">\(\mu\)</span> and <span class="math">\(\sigma\)</span> via
the sample median and <span class="math">\(\mathrm{IQR}/1.349\)</span> (but for different
distributions we will need a different calibrator).</p>
</aside>
<aside class="footnote brackets" id="footcdf" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">3</a><span class="fn-bracket">]</span></span>
<p>The probability distribution of any
real-valued random variable <span class="math">\(X\)</span> can be uniquely defined
by means of a nondecreasing, right (upward) continuous
function <span class="math">\(F:\mathbb{R}\to[0, 1]\)</span> such that
<span class="math">\(\lim_{x\to-\infty} F(x)=0\)</span> and <span class="math">\(\lim_{x\to\infty} F(x)=1\)</span>,
in which case <span class="math">\(\Pr(X\le x)=F(x)\)</span>.
The probability density function only exists for continuous
random variables and is defined as the derivative of <span class="math">\(F\)</span>.</p>
</aside>
<aside class="footnote brackets" id="footsuffsmall" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">4</a><span class="fn-bracket">]</span></span>
<p>The larger the sample size, the less tolerant
regarding the size of this disparity we are;
see <a class="reference internal" href="#sec-ks-test"><span class="std std-numref">Section 6.2.3</span></a>.</p>
</aside>
<aside class="footnote brackets" id="footprobplot" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">5</a><span class="fn-bracket">]</span></span>
<p>(*) <strong class="command">scipy.stats.probplot</strong> uses a slightly
different definition (there are many other ones in common use).</p>
</aside>
<aside class="footnote brackets" id="footqqplotpearson" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">6</a><span class="fn-bracket">]</span></span>
<p>(*) We can quantify (informally)
the goodness of fit by using the Pearson linear correlation coefficient;
see <a class="reference internal" href="330-relationship.html#sec-pearson"><span class="std std-numref">Section 9.1.1</span></a>.</p>
</aside>
<aside class="footnote brackets" id="footsmallalpha" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">7</a><span class="fn-bracket">]</span></span>
<p>See <a class="reference internal" href="430-group-by.html#sec-ks-test2"><span class="std std-numref">Section 12.2.6</span></a> for more details.</p>
</aside>
<aside class="footnote brackets" id="footmost" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">8</a><span class="fn-bracket">]</span></span>
<p>Except for the few richest, who are interesting on their
own; see <a class="reference internal" href="#sec-pareto"><span class="std std-numref">Section 6.3.2</span></a> where we discuss the Pareto distribution.</p>
</aside>
<aside class="footnote brackets" id="footfts" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">9</a><span class="fn-bracket">]</span></span>
<p>Compare the Fundamental Theorem of Statistics
(the Glivenko–Cantelli theorem).</p>
</aside>
</aside>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="310-matrix.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title"><span class="section-number">7. </span>Multidimensional numeric data at a glance</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="220-transform-vector.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title"><span class="section-number">5. </span>Processing unidimensional data</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
              
              
              Copyright &#169; 2022–2023 by <a href="https://www.gagolewski.com/">Marek Gagolewski</a>.
              Some rights reserved. Licensed under <a href='https://creativecommons.org/licenses/by-nc-nd/4.0'>CC BY-NC-ND 4.0</a>.
              Built with <a href="https://sphinx-doc.org/">Sphinx</a>
              and a customised <a href="https://github.com/pradyunsg/furo">Furo</a> theme.
              Last updated on 2023-11-14T15:11:24+1100.
              This site will never display any ads: it is a non-profit project.
              It does not collect any data.
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            In this chapter
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">6. Continuous probability distributions</a><ul>
<li><a class="reference internal" href="#normal-distribution">6.1. Normal distribution</a><ul>
<li><a class="reference internal" href="#estimating-parameters">6.1.1. Estimating parameters</a></li>
<li><a class="reference internal" href="#data-models-are-useful">6.1.2. Data models are useful</a></li>
</ul>
</li>
<li><a class="reference internal" href="#assessing-goodness-of-fit">6.2. Assessing goodness-of-fit</a><ul>
<li><a class="reference internal" href="#comparing-cumulative-distribution-functions">6.2.1. Comparing cumulative distribution functions</a></li>
<li><a class="reference internal" href="#comparing-quantiles">6.2.2. Comparing quantiles</a></li>
<li><a class="reference internal" href="#kolmogorovsmirnov-test">6.2.3. Kolmogorov–Smirnov test (*)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#other-noteworthy-distributions">6.3. Other noteworthy distributions</a><ul>
<li><a class="reference internal" href="#log-normal-distribution">6.3.1. Log-normal distribution</a></li>
<li><a class="reference internal" href="#pareto-distribution">6.3.2. Pareto distribution</a></li>
<li><a class="reference internal" href="#uniform-distribution">6.3.3. Uniform distribution</a></li>
<li><a class="reference internal" href="#distribution-mixtures">6.3.4. Distribution mixtures (*)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#generating-pseudorandom-numbers">6.4. Generating pseudorandom numbers</a><ul>
<li><a class="reference internal" href="#id14">6.4.1. Uniform distribution</a></li>
<li><a class="reference internal" href="#not-exactly-random">6.4.2. Not exactly random</a></li>
<li><a class="reference internal" href="#sampling-from-other-distributions">6.4.3. Sampling from other distributions</a></li>
<li><a class="reference internal" href="#natural-variability">6.4.4. Natural variability</a></li>
<li><a class="reference internal" href="#adding-jitter-white-noise">6.4.5. Adding jitter (white noise)</a></li>
<li><a class="reference internal" href="#independence-assumption">6.4.6. Independence assumption</a></li>
</ul>
</li>
<li><a class="reference internal" href="#further-reading">6.5. Further reading</a></li>
<li><a class="reference internal" href="#exercises">6.6. Exercises</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=aa0d8e41"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=32e29ea5"></script>
    <script src="../_static/proof.js"></script>
    <script src="../_static/katex.min.js?v=deb2f140"></script>
    <script src="../_static/auto-render.min.js?v=8b9f325c"></script>
    <script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
    </body>
</html>