<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta content="Minimalist Data Wrangling with Python" name="citation_title" />
<meta content="Marek Gagolewski" name="citation_author" />
<meta content="2025" name="citation_date" />
<meta content="2025" name="citation_publication_date" />
<meta content="https://datawranglingpy.gagolewski.com/datawranglingpy.pdf" name="citation_pdf_url" />
<meta content="https://datawranglingpy.gagolewski.com/" name="citation_public_url" />
<meta content="10.5281/zenodo.6451068" name="citation_doi" />
<meta content="Minimalist Data Wrangling with Python is envisaged as a student's first introduction to data science, providing a high-level overview as well as discussing key concepts in detail. We explore methods for cleaning data gathered from different sources, transforming, selecting, and extracting features, performing exploratory data analysis and dimensionality reduction, identifying naturally occurring data clusters, modelling patterns in data, comparing data between groups, and reporting the results. This textbook is a non-profit project. Its online and PDF versions are freely available at https://datawranglingpy.gagolewski.com/." name="citation_abstract" />
<meta content="summary" name="twitter:card" />
<meta content="Minimalist Data Wrangling with Python" name="twitter:title" />
<meta content="Minimalist Data Wrangling with Python" name="og:title" />
<meta content="Minimalist Data Wrangling with Python is envisaged as a student's first introduction to data science, providing a high-level overview as well as discussing key concepts in detail. We explore methods for cleaning data gathered from different sources, transforming, selecting, and extracting features, performing exploratory data analysis and dimensionality reduction, identifying naturally occurring data clusters, modelling patterns in data, comparing data between groups, and reporting the results. This textbook is a non-profit project. Its online and PDF versions are freely available at https://datawranglingpy.gagolewski.com/." name="twitter:description" />
<meta content="Minimalist Data Wrangling with Python is envisaged as a student's first introduction to data science, providing a high-level overview as well as discussing key concepts in detail. We explore methods for cleaning data gathered from different sources, transforming, selecting, and extracting features, performing exploratory data analysis and dimensionality reduction, identifying naturally occurring data clusters, modelling patterns in data, comparing data between groups, and reporting the results. This textbook is a non-profit project. Its online and PDF versions are freely available at https://datawranglingpy.gagolewski.com/." name="og:description" />
<meta content="gagolews/datawranglingpy" name="og:site_name" />
<meta content="https://datawranglingpy.gagolewski.com/" name="og:url" />
<meta content="https://datawranglingpy.gagolewski.com/_images/cover.png" name="twitter:image" />
<meta content="https://datawranglingpy.gagolewski.com/_images/cover.png" name="og:image" />
<meta content="https://datawranglingpy.gagolewski.com/" name="DC.identifier" />
<meta content="Marek Gagolewski" name="DC.publisher" />
<meta content="INDEX,FOLLOW" name="robots" />
<meta content="book" name="og:type" />
<meta content="9780645571912" name="og:book:isbn" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="7. From uni- to multidimensional numeric data" href="310-matrix.html" /><link rel="prev" title="5. Processing unidimensional data" href="220-transform-vector.html" />
        <link rel="canonical" href="https://datawranglingpy.gagolewski.com/chapter/230-distribution.html" />

    <link rel="shortcut icon" href="https://www.gagolewski.com/_static/img/datawranglingpy.png"/><!-- Generated with Sphinx 7.3.7 and Furo 2024.05.06 -->
        <title>6. Continuous probability distributions - Minimalist Data Wrangling with Python</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=387cc868" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=36a5483c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=a4cbedb2" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  --admonition-font-size: 95%;
  --admonition-title-font-size: 95%;
  --color-brand-primary: red;
  --color-brand-content: #CC3333;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --admonition-font-size: 95%;
  --admonition-title-font-size: 95%;
  --color-brand-primary: #ff2b53;
  --color-brand-content: #dd3333;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --admonition-font-size: 95%;
  --admonition-title-font-size: 95%;
  --color-brand-primary: #ff2b53;
  --color-brand-content: #dd3333;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" /
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">Minimalist Data Wrangling with Python</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto colour theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky">
<div class="sidebar-logo-container">
  <a class="sidebar-brand" href="../index.html"><img class="sidebar-logo" src="https://www.gagolewski.com/_static/img/datawranglingpy.png" alt="Logo"/></a>
</div>

<span class="sidebar-brand-text">
<a class="sidebar-brand" href="../index.html">Minimalist Data Wrangling with Python</a>
</span>
<div class="sidebar-brand">
An open-access textbook<br />
by <a href='https://www.gagolewski.com/' style="display: contents">Marek Gagolewski</a><br />
v1.1.0.9002
</div>
<form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">About</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.gagolewski.com/">Author</a></li>
<li class="toctree-l1"><a class="reference external" href="https://datawranglingpy.gagolewski.com/datawranglingpy.pdf">This book in PDF</a></li>
<li class="toctree-l1"><a class="reference internal" href="../order-paper-copy.html">Order a paper copy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/gagolews/datawranglingpy">Report bugs or typos (GitHub)</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/gagolews/teaching-data">Datasets</a></li>
<li class="toctree-l1"><a class="reference external" href="https://deepr.gagolewski.com/">Deep R programming</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Start here</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="000-preface.html">Preface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introducing Python</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="110-setup.html">1. Getting started with Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="120-scalar.html">2. Scalar types and control structures in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="130-sequential.html">3. Sequential and other types in Python</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Unidimensional data</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="210-vector.html">4. Unidimensional numeric data and their empirical distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="220-transform-vector.html">5. Processing unidimensional data</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">6. Continuous probability distributions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multidimensional data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="310-matrix.html">7. From uni- to multidimensional numeric data</a></li>
<li class="toctree-l1"><a class="reference internal" href="320-transform-matrix.html">8. Processing multidimensional data</a></li>
<li class="toctree-l1"><a class="reference internal" href="330-relationship.html">9. Exploring relationships between variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Heterogeneous data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="410-data-frame.html">10. Introducing data frames</a></li>
<li class="toctree-l1"><a class="reference internal" href="420-categorical.html">11. Handling categorical data</a></li>
<li class="toctree-l1"><a class="reference internal" href="430-group-by.html">12. Processing data in groups</a></li>
<li class="toctree-l1"><a class="reference internal" href="440-sql.html">13. Accessing databases</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other data types</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="510-text.html">14. Text data</a></li>
<li class="toctree-l1"><a class="reference internal" href="520-missingness.html">15. Missing, censored, and questionable data</a></li>
<li class="toctree-l1"><a class="reference internal" href="530-time-series.html">16. Time series</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="998-changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="999-bibliography.html">References</a></li>
</ul>

</div></div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="edit-this-page">
  <a class="muted-link" href="https://github.com/gagolews/datawranglingpy" title="Edit this page">
    <svg><use href="#svg-pencil"></use></svg>
    <span class="visually-hidden">Edit this page</span>
  </a>
</div><div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto colour theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section class="tex2jax_ignore mathjax_ignore" id="continuous-probability-distributions">
<span id="chap-distribution"></span><h1><span class="section-number">6. </span>Continuous probability distributions<a class="headerlink" href="#continuous-probability-distributions" title="Link to this heading">¶</a></h1>
<blockquote>
<div><p><em>This open-access textbook
is, and will remain, freely available for everyone’s enjoyment
(also in <a class="reference external" href="https://datawranglingpy.gagolewski.com/datawranglingpy.pdf">PDF</a>;
a paper copy can also be <a class="reference internal" href="../order-paper-copy.html"><span class="doc std std-doc">ordered</span></a>).
It is a non-profit project. Although available online, it is a whole course,
and should be read from the beginning to the end.
Refer to the <a class="reference internal" href="000-preface.html#chap-preface"><span class="std std-ref">Preface</span></a> for general introductory remarks. Any
<a class="reference external" href="https://github.com/gagolews/datawranglingpy">bug/typo reports/fixes</a>
are appreciated. Make sure to check out
<a class="reference external" href="https://deepr.gagolewski.com/"><em>Deep R Programming</em></a>
<span id="id1">[<a class="reference internal" href="999-bibliography.html#id2" title="Gagolewski, M. (2025).  Deep R Programming. URL: https://deepr.gagolewski.com/, DOI: 10.5281/zenodo.7490464.">36</a>]</span> too.</em></p>
</div></blockquote>
<p>Successful data analysts deal with hundreds or thousands of datasets
in their lifetimes. In the long run, at some level, most of them will be
deemed <em>boring</em> (datasets, not analysts). This is because only a few common
patterns will be occurring over and over again.
In particular, the previously mentioned bell-shapedness and right-skewness
are prevalent in the so-called real world.
Surprisingly, however, this is exactly when things
become scientific and interesting, allowing us to study various phenomena
at an <em>appropriate level of generality</em>.</p>
<p>Mathematically, such idealised patterns in the histogram shapes
can be formalised using the notion of a
<em>probability density function</em> (PDF) of a <em>continuous, real-valued random
variable</em>.
Intuitively<a class="footnote-reference brackets" href="#foothistconvergence" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>, a PDF is
a smooth curve that would arise if we drew a histogram
for the entire <em>population</em> (e.g., all women living currently on Earth and
beyond or otherwise an extremely large data sample obtained by independently
querying the same underlying data generating process) in such a way that
the total area of all the bars is equal to 1
and the bin sizes are very small.
On the other hand, a <em>real-valued random variable</em> is
a theoretical process that generates quantitative data.
From this perspective, a <em>sample</em> at hand is assumed to be
<em>drawn</em> from a given distribution; it is a <em>realisation</em> of the
underlying process.</p>
<p>We do not intend ours to be a course in
probability theory and mathematical statistics.
Rather, a one that precedes and motivates them
(e.g., <span id="id3">[<a class="reference internal" href="999-bibliography.html#id47" title="Dekking, F.M., Kraaikamp, C., Lopuhaä, H.P., and Meester, L.E. (2005).  A Modern Introduction to Probability and Statistics: Understanding Why and How. Springer.">23</a>, <a class="reference internal" href="999-bibliography.html#id50" title="Gentle, J.E. (2009).  Computational Statistics. Springer-Verlag.">40</a>, <a class="reference internal" href="999-bibliography.html#id45" title="Gentle, J.E. (2020).  Theory of Statistics. book draft. URL: https://mason.gmu.edu/~jgentle/books/MathStat.pdf.">41</a>, <a class="reference internal" href="999-bibliography.html#id158" title="Ross, S.M. (2020).  Introduction to Probability and Statistics for Engineers and Scientists. Academic Press.">82</a>, <a class="reference internal" href="999-bibliography.html#id159" title="Ross, S.M. (2024).  Introduction to Probability Models. Elsevier.">83</a>]</span>). Therefore,
our definitions must be simplified so that they are digestible.
We will thus consider the following characterisation.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>(*) We call an integrable function <span class="math">\(f:\mathbb{R}\to\mathbb{R}\)</span>
a <em>probability density function</em>, if <span class="math">\(f(x)\ge 0\)</span> for all <span class="math">\(x\)</span>
and <span class="math">\(\int_{-\infty}^\infty f(x)\,dx=1\)</span>.
In other words, <span class="math">\(f\)</span> is nonnegative and normalised in such a way that
the total area under the whole curve is 1.</p>
<p>For any <span class="math">\(a &lt; b\)</span>, we treat  <span class="math">\(\int_a^b f(x)\,dx\)</span>,
being the area under the fragment of the <span class="math">\(f(x)\)</span> curve
for <span class="math">\(x\)</span> between <span class="math">\(a\)</span> and <span class="math">\(b\)</span>,
as the probability of the underlying real-valued
random variable’s falling into the <span class="math">\([a, b]\)</span> interval.</p>
</div>
<p>Some distributions arise more frequently than others
and appear to fit empirical data or their parts particularly well <span id="id4">[<a class="reference internal" href="999-bibliography.html#id83" title="Forbes, C., Evans, M., Hastings, N., and Peacock, B. (2010).  Statistical Distributions. Wiley.">29</a>]</span>.
In this chapter, we review a few noteworthy probability distributions:
the normal, log-normal, Pareto, and uniform families
(we will also mention the chi-squared,
Kolmogorov, and exponential ones in this course).</p>
<section id="normal-distribution">
<h2><span class="section-number">6.1. </span>Normal distribution<a class="headerlink" href="#normal-distribution" title="Link to this heading">¶</a></h2>
<p>A <em>normal (Gaussian) distribution</em> has a prototypical,
nicely symmetric, bell-shaped density.
It is described by two parameters:</p>
<ul class="simple">
<li><p><span class="math">\(\mu\in\mathbb{R}\)</span> being its expected value, at which the PDF is centred,</p></li>
<li><p><span class="math">\(\sigma&gt;0\)</span> is the standard deviation, saying how
much the distribution is dispersed around <span class="math">\(\mu\)</span>.</p></li>
</ul>
<p>The probability density function of <span class="math">\(\mathrm{N}(\mu, \sigma)\)</span>,
compare <a class="reference internal" href="#fig-normalpdf"><span class="std std-numref">Figure 6.1</span></a>, is given by:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
f(x) = \frac{1}{\sqrt{2\pi \sigma^2}}
\exp\left( -\frac{(x-\mu)^2}{2\sigma^2} \right).
\]</div>
</div>
<figure class="align-default" id="id29">
<span id="fig-normalpdf"></span><img alt="../_images/normalpdf-1.png" src="../_images/normalpdf-1.png" />
<figcaption>
<p><span class="caption-number">Figure 6.1 </span><span class="caption-text">The probability density functions of some normal distributions <span class="math">\(\mathrm{N}(\mu, \sigma)\)</span>. Note that <span class="math">\(\mu\)</span> is responsible for shifting and <span class="math">\(\sigma\)</span> affects scaling/stretching of the probability mass.</span><a class="headerlink" href="#id29" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<section id="estimating-parameters">
<h3><span class="section-number">6.1.1. </span>Estimating parameters<a class="headerlink" href="#estimating-parameters" title="Link to this heading">¶</a></h3>
<p>A course in mathematical statistics, may tell us
that the sample arithmetic mean <span class="math">\(\bar{x}\)</span> and standard deviation <span class="math">\(s\)</span>
are natural, statistically well-behaving <em>estimators</em> of the said parameters.
If all observations are really drawn independently from
<span class="math">\(\mathrm{N}(\mu, \sigma)\)</span> each, then we will <em>expect</em> <span class="math">\(\bar{x}\)</span> and <span class="math">\(s\)</span>
to be equal to, more or less, <span class="math">\(\mu\)</span> and <span class="math">\(\sigma\)</span>.
Furthermore, the larger the sample size, the smaller the error.</p>
<p>Recall the <code class="docutils literal notranslate"><span class="pre">heights</span></code> (females from the NHANES study)
dataset and its bell-shaped histogram
in <a class="reference internal" href="210-vector.html#fig-heights-histogram-bins11"><span class="std std-numref">Figure 4.2</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">heights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching-data/master/marek/nhanes_adult_female_height_2020.txt&quot;</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">heights</span><span class="p">)</span>
<span class="n">n</span>
<span class="c1">## 4221</span>
</pre></div>
</div>
<p>Let’s estimate the said parameters of the normal distribution:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">heights</span><span class="p">)</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span>
<span class="c1">## (160.13679222932953, 7.062858532891359)</span>
</pre></div>
</div>
<p>Mathematically, we will denote these two by
<span class="math">\(\hat{\mu}\)</span> and <span class="math">\(\hat{\sigma}\)</span> (mu and sigma with a hat) to emphasise that
they are merely guesstimates of the unknown theoretical parameters
<span class="math">\(\mu\)</span> and <span class="math">\(\sigma\)</span> describing the whole population.
On a side note, in this context,
the requested <code class="docutils literal notranslate"><span class="pre">ddof=1</span></code> estimator has slightly better statistical properties.</p>
<p><a class="reference internal" href="#fig-heights-normal"><span class="std std-numref">Figure 6.2</span></a> shows the fitted density function, i.e.,
the PDF of <span class="math">\(\mathrm{N}(160.1, 7.06)\)</span>, which we computed using
<strong class="command">scipy.stats.norm.pdf</strong>, on top of a histogram.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">heights</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">heights</span><span class="p">),</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">),</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;PDF of N(</span><span class="si">{</span><span class="n">mu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Density&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id30">
<span id="fig-heights-normal"></span><img alt="../_images/heights-normal-3.png" src="../_images/heights-normal-3.png" />
<figcaption>
<p><span class="caption-number">Figure 6.2 </span><span class="caption-text">A histogram for the <code class="docutils literal notranslate"><span class="pre">heights</span></code> dataset and the probability density function of the fitted normal distribution.</span><a class="headerlink" href="#id30" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>We passed <code class="docutils literal notranslate"><span class="pre">density=True</span></code> to <strong class="command">matplotlib.pyplot.hist</strong>
to normalise the bars’ heights so that their total area is 1.</p>
<p>At first glimpse, the density matches the histogram nicely.
Before proceeding with an overview of the ways to assess
the goodness-of-fit more rigorously, we should heap praise on the potential
benefits of getting access to idealised <em>models</em> of our datasets.</p>
</section>
<section id="data-models-are-useful">
<span id="sec-three-sigma"></span><h3><span class="section-number">6.1.2. </span>Data models are useful<a class="headerlink" href="#data-models-are-useful" title="Link to this heading">¶</a></h3>
<p><em>If</em> (provided that, assuming that, on condition that)
our sample is <em>really</em> a realisation of the independent random variables
following a given distribution, or a data analyst judges that such an
approximation might be justified or beneficial, then we can
<em>reduce</em> them to merely a few parameters.</p>
<p>We can risk assuming that the heights data follow
the normal distribution (assumption 1)
with parameters <span class="math">\(\mu=160.1\)</span> and <span class="math">\(\sigma=7.06\)</span> (assumption 2).
Note that the choice of the distribution family is one thing,
and the way<a class="footnote-reference brackets" href="#footmle" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> we estimate the underlying parameters
(in our case, we use the aforementioned <span class="math">\(\hat{\mu}\)</span> and <span class="math">\(\hat{\sigma}\)</span>)
is another.</p>
<p>Creating a data model only saves storage space and computational time,
but also – based on what we can learn from a course in probability and
statistics (by appropriately integrating the normal PDF) –
we can imply the facts such as:</p>
<ul class="simple">
<li><p>c. 68% of (i.e., a <em>majority</em>) women are
<span class="math">\(\mu\pm \sigma\)</span> tall (the <span class="math">\(1\sigma\)</span> rule),</p></li>
<li><p>c. 95% of (i.e., <em>the most typical</em>) women are
<span class="math">\(\mu\pm 2\sigma\)</span> tall (the <span class="math">\(2\sigma\)</span> rule),</p></li>
<li><p>c. 99.7% of (i.e., <em>almost all</em>) women are
<span class="math">\(\mu\pm 3\sigma\)</span> tall (the <span class="math">\(3\sigma\)</span> rule).</p></li>
</ul>
<p>Also, if we knew that the distribution of heights of men is also normal
with some other parameters (spoiler alert: <span class="math">\(\mathrm{N}(173.8, 7.66)\)</span>),
we could make some comparisons between the two samples.
For example, we could compute the probability that a passerby
who is 155 cm tall is actually a man.</p>
<div class="proof proof-type-exercise" id="id31">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.1</span>
        
    </div><div class="proof-content">
<p>How different manufacturing industries (e.g., clothing) can make use of
such models? Are simplifications necessary when dealing with complexity
of the real world? What are the alternatives?</p>
</div></div><p>Furthermore, assuming a particular model gives us access to a range of
<em>parametric</em> statistical methods (ones that are derived for the
corresponding family of probability distributions), e.g., the t-test to
compare the expected values.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>We should always verify the assumptions of the tool at hand before
we apply it in practice.  In particular, we will soon discover
that the UK annual incomes are not normally distributed.
Therefore, we must not refer to the aforementioned <span class="math">\(2\sigma\)</span> rule in their
case. A hammer neither barks nor can it serve as a screwdriver. Period.</p>
</div>
</section>
</section>
<section id="assessing-goodness-of-fit">
<h2><span class="section-number">6.2. </span>Assessing goodness-of-fit<a class="headerlink" href="#assessing-goodness-of-fit" title="Link to this heading">¶</a></h2>
<section id="comparing-cumulative-distribution-functions">
<h3><span class="section-number">6.2.1. </span>Comparing cumulative distribution functions<a class="headerlink" href="#comparing-cumulative-distribution-functions" title="Link to this heading">¶</a></h3>
<p>Bell-shaped histograms are encountered fairly frequently in real-world data:
e.g., measurement errors in physical experiments and standardised tests’
results (like IQ and other ability scores) tend to be distributed this way,
at least approximately.
If we yearn for more precision, there is a better way of assessing
the extent to which a sample deviates from a hypothesised distribution.
Namely, we can measure the discrepancy between
some theoretical <em>cumulative distribution function</em> (CDF)
and the empirical one (ECDF which we defined in <a class="reference internal" href="210-vector.html#sec-ecdf"><span class="std std-numref">Section 4.3.8</span></a>).</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If <span class="math">\(f\)</span> is a PDF, then the corresponding
theoretical CDF is defined as <span class="math">\(F(x) = \int_{-\infty}^x f(t)\,dt\)</span>,
i.e., the probability of the corresponding random variable’s
being less than or equal to <span class="math">\(x\)</span>.
By definition<a class="footnote-reference brackets" href="#footcdf" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>, each CDF takes values in the unit interval
(<span class="math">\([0, 1]\)</span>) and is nondecreasing.</p>
</div>
<p>For the normal distribution family, the values of the theoretical CDF
can be computed by calling <strong class="command">scipy.stats.norm.cdf</strong>;
compare <a class="reference internal" href="#fig-normalcdfvsquant"><span class="std std-numref">Figure 6.4</span></a> below.</p>
<p><a class="reference internal" href="#fig-heights-cdf"><span class="std std-numref">Figure 6.3</span></a> depicts the CDF of <span class="math">\(\mathrm{N}(160.1, 7.06)\)</span>
and the empirical CDF of the <code class="docutils literal notranslate"><span class="pre">heights</span></code> dataset.
This <em>looks</em> like a superb match.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">heights</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">heights</span><span class="p">),</span> <span class="mi">1001</span><span class="p">)</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>  <span class="c1"># sample the CDF at many points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;CDF of N(</span><span class="si">{</span><span class="n">mu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">heights_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">heights</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">heights_sorted</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">,</span>
    <span class="n">drawstyle</span><span class="o">=</span><span class="s2">&quot;steps-post&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Empirical CDF&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Prob(height $</span><span class="se">\\</span><span class="s2">leq$ x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id32">
<span id="fig-heights-cdf"></span><img alt="../_images/heights-cdf-5.png" src="../_images/heights-cdf-5.png" />
<figcaption>
<p><span class="caption-number">Figure 6.3 </span><span class="caption-text">The empirical CDF and the fitted normal CDF for the <code class="docutils literal notranslate"><span class="pre">heights</span></code> dataset: the fit is superb.</span><a class="headerlink" href="#id32" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<div class="proof proof-type-example" id="id33">

    <div class="proof-title">
        <span class="proof-type">Example 6.2</span>
        
    </div><div class="proof-content">
<p><span class="math">\(F(b)-F(a)=\int_{a}^b f(t)\,dt\)</span> is the probability
of generating a value in the interval <span class="math">\([a, b]\)</span>.
Let’s compute the probability related to the <span class="math">\(3\sigma\)</span> rule:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">F</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
<span class="n">F</span><span class="p">(</span><span class="n">mu</span><span class="o">+</span><span class="mi">3</span><span class="o">*</span><span class="n">sigma</span><span class="p">)</span> <span class="o">-</span> <span class="n">F</span><span class="p">(</span><span class="n">mu</span><span class="o">-</span><span class="mi">3</span><span class="o">*</span><span class="n">sigma</span><span class="p">)</span>
<span class="c1">## 0.9973002039367398</span>
</pre></div>
</div>
</div></div><p>A common way to summarise the discrepancy between
the empirical CDF <span class="math">\(\hat{F}_n\)</span> and a given theoretical CDF <span class="math">\(F\)</span> is by computing
the greatest absolute deviation:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
\hat{D}_n = \sup_{t\in\mathbb{R}} | \hat{F}_n(t) - F(t) |,
\]</div>
</div>
<p>where the <em>sup</em>remum is a continuous version of the maximum.
We have:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
\hat{D}_n = \max \left\{
    \max_{k=1,\dots,n}\left\{ \left|\tfrac{k-1}{n} - F(x_{(k)})\right| \right\},
    \max_{k=1,\dots,n}\left\{ \left|\tfrac{k}{n} - F(x_{(k)})\right| \right\}
\right\},
\]</div>
</div>
<p>i.e., <span class="math">\(F\)</span> needs to be probed only at the <span class="math">\(n\)</span> points from the sorted
input sample.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">compute_Dn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">F</span><span class="p">):</span>  <span class="c1"># equivalent to scipy.stats.kstest(x, F)[0]</span>
    <span class="n">Fx</span> <span class="o">=</span> <span class="n">F</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 1, 2, ..., n</span>
    <span class="n">Dn1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">n</span> <span class="o">-</span> <span class="n">Fx</span><span class="p">))</span>
    <span class="n">Dn2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">k</span><span class="o">/</span><span class="n">n</span> <span class="o">-</span> <span class="n">Fx</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="n">Dn1</span><span class="p">,</span> <span class="n">Dn2</span><span class="p">)</span>
</pre></div>
</div>
<p>If the difference is <em>sufficiently<a class="footnote-reference brackets" href="#footsuffsmall" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a> small</em>,
then we can assume that the normal model describes data quite well.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Dn</span> <span class="o">=</span> <span class="n">compute_Dn</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="n">F</span><span class="p">)</span>
<span class="n">Dn</span>
<span class="c1">## 0.010470976524201148</span>
</pre></div>
</div>
<p>This is indeed the case here: we may estimate the probability
of someone’s being as tall as the given height
with an error less than about 1.05%.</p>
</section>
<section id="comparing-quantiles">
<span id="sec-qqplot"></span><h3><span class="section-number">6.2.2. </span>Comparing quantiles<a class="headerlink" href="#comparing-quantiles" title="Link to this heading">¶</a></h3>
<p>A <em>Q-Q plot</em> (quantile-quantile or probability plot)
is another graphical method for comparing two distributions.
This time, instead of working with a cumulative distribution function <span class="math">\(F\)</span>,
we will be dealing with the related quantile function <span class="math">\(Q\)</span>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Given a continuous<a class="footnote-reference brackets" href="#footquant" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a> CDF <span class="math">\(F\)</span>, the corresponding <em>quantile function</em>
<span class="math">\(Q\)</span> is exactly its inverse, i.e., we have <span class="math">\(Q(p)=F^{-1}(p)\)</span>
for all <span class="math">\(p\in(0, 1)\)</span>.</p>
</div>
<p>The theoretical quantiles can be generated
by the <strong class="command">scipy.stats.norm.ppf</strong> function; compare <a class="reference internal" href="#fig-normalcdfvsquant"><span class="std std-numref">Figure 6.4</span></a>.
Here, <em>ppf</em> stands for the percent point function which is another
(yet quite esoteric) name for the above <span class="math">\(Q\)</span>.</p>
<figure class="align-default" id="id34">
<span id="fig-normalcdfvsquant"></span><img alt="../_images/normalcdfvsquant-7.png" src="../_images/normalcdfvsquant-7.png" />
<figcaption>
<p><span class="caption-number">Figure 6.4 </span><span class="caption-text">The cumulative distribution functions (left) and the quantile functions (being the inverse of the CDF; right) of some normal distributions.</span><a class="headerlink" href="#id34" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<div class="proof proof-type-example" id="id35">

    <div class="proof-title">
        <span class="proof-type">Example 6.3</span>
        
    </div><div class="proof-content">
<p>In our <span class="math">\(\mathrm{N}(160.1, 7.06)\)</span>-distributed <code class="docutils literal notranslate"><span class="pre">heights</span></code> dataset,
<span class="math">\(Q(0.9)\)</span> is the height not exceeded by 90% of the female population.
In other words, only 10% of American women are taller than:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
<span class="c1">## 169.18820963937648</span>
</pre></div>
</div>
</div></div><p>A Q-Q plot draws the sample quantiles <em>against</em> the corresponding
theoretical quantiles. In <a class="reference internal" href="220-transform-vector.html#sec-quantiles"><span class="std std-numref">Section 5.1.1.3</span></a>, we mentioned that
there are a few possible definitions thereof in the literature.
Thus, we have some degree of flexibility.
For simplicity, instead of using <strong class="command">numpy.quantile</strong>,
we will assume that the <span class="math">\(i/(n+1)\)</span>-quantile<a class="footnote-reference brackets" href="#footprobplot" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a>
is equal to <span class="math">\(x_{(i)}\)</span>, i.e., the <span class="math">\(i\)</span>-th smallest value in
a given sample <span class="math">\((x_1,x_2,\dots,x_n)\)</span>
and consider only <span class="math">\(i=1, 2, \dots, n\)</span>.
This way, we mitigate the problem which arises
when the 0- or 1-quantiles of the theoretical distribution,
i.e., <span class="math">\(Q(0)\)</span> or <span class="math">\(Q(1)\)</span>, are not finite (and this is the case for
the normal distribution family).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">qq_plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Q</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Draws a Q-Q plot, given:</span>
<span class="sd">    * x - a data sample (vector)</span>
<span class="sd">    * Q - a theoretical quantile function</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>    <span class="c1"># 1/(n+1), 2/(n+2), ..., n/(n+1)</span>
    <span class="n">x_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>          <span class="c1"># sample quantiles</span>
    <span class="n">quantiles</span> <span class="o">=</span> <span class="n">Q</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>               <span class="c1"># theoretical quantiles</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">quantiles</span><span class="p">,</span> <span class="n">x_sorted</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axline</span><span class="p">((</span><span class="n">x_sorted</span><span class="p">[</span><span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="n">x_sorted</span><span class="p">[</span><span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">]),</span> <span class="n">slope</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>  <span class="c1"># identity line</span>
</pre></div>
</div>
<p><a class="reference internal" href="#fig-qq-heights"><span class="std std-numref">Figure 6.5</span></a> depicts the Q-Q plot for our example dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">qq_plot</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">q</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantiles of N(</span><span class="si">{</span><span class="n">mu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Sample quantiles&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id36">
<span id="fig-qq-heights"></span><img alt="../_images/qq-heights-9.png" src="../_images/qq-heights-9.png" />
<figcaption>
<p><span class="caption-number">Figure 6.5 </span><span class="caption-text">The Q-Q plot for the <code class="docutils literal notranslate"><span class="pre">heights</span></code> dataset. It is a nice fit.</span><a class="headerlink" href="#id36" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>Ideally, we wish that the points would be arranged on the <span class="math">\(y=x\)</span> line.
In our case, there are small discrepancies<a class="footnote-reference brackets" href="#footqqplotpearson" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a> in the tails
(e.g., the smallest observation was slightly smaller than expected,
and the largest one was larger than expected),
although it is <em>common</em> a behaviour for small samples
and certain distribution families.
However, overall, we can say that we observe a fine fit.</p>
</section>
<section id="kolmogorovsmirnov-test">
<span id="sec-ks-test"></span><h3><span class="section-number">6.2.3. </span>Kolmogorov–Smirnov test (*)<a class="headerlink" href="#kolmogorovsmirnov-test" title="Link to this heading">¶</a></h3>
<p>To be more scientific, we can introduce a more formal method
for assessing the quality of fit. It will enable us to test the null
hypothesis stating that a given empirical distribution <span class="math">\(\hat{F}_n\)</span>
does not differ <em>significantly</em> from the theoretical continuous CDF <span class="math">\(F\)</span>:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
\left\{
\begin{array}{rll}
H_0: & \hat{F}_n = F & \text{(null hypothesis)}\\
H_1: & \hat{F}_n \neq F & \text{(two-sided alternative)} \\
\end{array}
\right.
\]</div>
</div>
<p>The popular goodness-of-fit test by Kolmogorov and Smirnov can give
us a conservative interval of the acceptable values of
the largest deviation between the empirical and theoretical CDF,
<span class="math">\(\hat{D}_n\)</span>, as a function of <span class="math">\(n\)</span>.</p>
<p>Namely, if the <em>test statistic</em> <span class="math">\(\hat{D}_n\)</span> is smaller than some
<em>critical value</em> <span class="math">\(K_n\)</span>, then we shall deem the difference insignificant.
This is to take into account the fact that reality might deviate
from the ideal. <a class="reference internal" href="#sec-natural-variability"><span class="std std-numref">Section 6.4.4</span></a> mentions
that even for samples that truly come from a hypothesised distribution,
there is some inherent variability. We need to be somewhat tolerant.</p>
<p>An authoritative textbook in mathematical statistics will tell us (and prove)
that, under the assumption that <span class="math">\(\hat{F}_n\)</span> is the ECDF of a sample of
<span class="math">\(n\)</span> independent variables <em>really</em> generated from a continuous CDF <span class="math">\(F\)</span>, the
random variable <span class="math">\(\hat{D}_n = \sup_{t\in\mathbb{R}} | \hat{F}_n(t) - F(t) |\)</span>
follows the Kolmogorov distribution with parameter <span class="math">\(n\)</span>.</p>
<p>In other words, if we generate many samples of length <span class="math">\(n\)</span> from <span class="math">\(F\)</span>,
and compute <span class="math">\(\hat{D}_n\)</span>s for each of them,
we expect it to be distributed like in <a class="reference internal" href="#fig-kolmogorovdistr"><span class="std std-numref">Figure 6.6</span></a>,
which we obtained by referring to <strong class="command">scipy.stats.kstwo</strong>.</p>
<figure class="align-default" id="id37">
<span id="fig-kolmogorovdistr"></span><img alt="../_images/kolmogorovdistr-11.png" src="../_images/kolmogorovdistr-11.png" />
<figcaption>
<p><span class="caption-number">Figure 6.6 </span><span class="caption-text">Densities (left) and cumulative distribution functions (right) of some Kolmogorov distributions. The greater the sample size, the smaller the acceptable deviations between the theoretical and empirical CDFs.</span><a class="headerlink" href="#id37" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>The choice of the critical value <span class="math">\(K_n\)</span> involves a trade-off between
our desire to:</p>
<ul class="simple">
<li><p>accept the null hypothesis when it is true
(data <em>really</em> come from <span class="math">\(F\)</span>), and</p></li>
<li><p>reject it when it is false (data follow some other distribution,
i.e., the difference is significant enough).</p></li>
</ul>
<p>These two needs are, unfortunately, mutually exclusive.</p>
<p>In the framework of frequentist hypothesis testing,
we assume some fixed upper bound (<em>significance level</em>)
for making the former kind of mistake, which we call the <em>type-I error</em>.
A nicely conservative (in a good way) value that we
suggest employing is <span class="math">\(\alpha=0.001=0.1\%\)</span>, i.e., only 1 out of 1000 samples
that really come from <span class="math">\(F\)</span> will be rejected as not coming from <span class="math">\(F\)</span>.</p>
<p>Such a <span class="math">\(K_n\)</span> may be determined by considering the inverse of the
CDF of the Kolmogorov distribution, <span class="math">\(\Xi_n\)</span>.
Namely, <span class="math">\(K_n=\Xi_n^{-1}(1-\alpha)\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span>  <span class="c1"># significance level</span>
<span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">kstwo</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="c1">## 0.029964456376393188</span>
</pre></div>
</div>
<p>In our case <span class="math">\(\hat{D}_n &lt; K_n\)</span> because <span class="math">\(0.01047 &lt; 0.02996\)</span>.
We conclude that our empirical (<code class="docutils literal notranslate"><span class="pre">heights</span></code>) distribution
does not differ significantly (at significance level <span class="math">\(0.1\%\)</span>)
from the assumed one, i.e., <span class="math">\(\mathrm{N}(160.1, 7.06)\)</span>.
In other words, we do not have enough evidence against the statement
that data are normally distributed. It is the presumption of innocence:
they are normal enough.</p>
<p>We will return to this discussion in <a class="reference internal" href="#sec-natural-variability"><span class="std std-numref">Section 6.4.4</span></a>
and <a class="reference internal" href="430-group-by.html#sec-ks-test2"><span class="std std-numref">Section 12.2.6</span></a>.</p>
</section>
</section>
<section id="other-noteworthy-distributions">
<h2><span class="section-number">6.3. </span>Other noteworthy distributions<a class="headerlink" href="#other-noteworthy-distributions" title="Link to this heading">¶</a></h2>
<section id="log-normal-distribution">
<span id="sec-log-normal-distribution"></span><h3><span class="section-number">6.3.1. </span>Log-normal distribution<a class="headerlink" href="#log-normal-distribution" title="Link to this heading">¶</a></h3>
<p>We say that a sample is <em>log-normally distributed</em>,
if its logarithm is normally distributed.
Such a behaviour is frequently observed in biology and medicine
(size of living tissue), social sciences (number of sexual partners), or
technology (file sizes).
<a class="reference internal" href="#fig-heights-log"><span class="std std-numref">Figure 6.7</span></a> suggests that it might also be true
in the case of the UK taxpayers’ incomes<a class="footnote-reference brackets" href="#footmost" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">income</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching-data/master/marek/uk_income_simulated_2020.txt&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Count&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id38">
<span id="fig-heights-log"></span><img alt="../_images/heights-log-13.png" src="../_images/heights-log-13.png" />
<figcaption>
<p><span class="caption-number">Figure 6.7 </span><span class="caption-text">A histogram of the logarithm of incomes.</span><a class="headerlink" href="#id38" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>Let’s thus proceed with the fitting of a log-normal model,
<span class="math">\(\mathrm{LN}(\mu, \sigma)\)</span>. The procedure is similar to the normal case,
but this time we determine the mean and standard deviation based on the logarithms of the observations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lmu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">income</span><span class="p">))</span>
<span class="n">lsigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">lmu</span><span class="p">,</span> <span class="n">lsigma</span>
<span class="c1">## (10.314409794364623, 0.5816585197803816)</span>
</pre></div>
</div>
<p>Unintuitively, <strong class="command">scipy.stats.lognorm</strong> identifies a distribution
via the parameter <span class="math">\(s\)</span> equal to <span class="math">\(\sigma\)</span> and <em>scale</em> equal to <span class="math">\(e^\mu\)</span>.
Computing the PDF at different points must thus be done as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="mi">101</span><span class="p">)</span>
<span class="n">fx</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">lognorm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">lsigma</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">lmu</span><span class="p">))</span>
</pre></div>
</div>
<p><a class="reference internal" href="#fig-heights-lognormal"><span class="std std-numref">Figure 6.8</span></a> depicts the histograms on the log- and original scale together with the fitted probability density function.
On the whole, the fit is not too bad;
after all, we are only dealing with a sample of 1000 households.
The original UK Office of National Statistics
<a class="reference external" href="https://www.ons.gov.uk/peoplepopulationandcommunity/personalandhouseholdfinances/incomeandwealth/bulletins/householddisposableincomeandinequality/financialyear2020">data</a>
could tell us more about the quality of this model in general,
but it is beyond the scope of our simple exercise.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">logbins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">geomspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="mi">31</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">income</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">logbins</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">fx</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>  <span class="c1"># log-scale on the x-axis</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Density&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">income</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">fx</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;PDF of LN(</span><span class="si">{</span><span class="n">lmu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">lsigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Density&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id39">
<span id="fig-heights-lognormal"></span><img alt="../_images/heights-lognormal-15.png" src="../_images/heights-lognormal-15.png" />
<figcaption>
<p><span class="caption-number">Figure 6.8 </span><span class="caption-text">A histogram and the probability density function of the fitted log-normal distribution for the <code class="docutils literal notranslate"><span class="pre">income</span></code> dataset, on log- (left) and original (right) scale.</span><a class="headerlink" href="#id39" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>Next, the left side of <a class="reference internal" href="#fig-qq-income"><span class="std std-numref">Figure 6.9</span></a> gives the
quantile-quantile plot for the above log-normal model
(note the double logarithmic scale).
Additionally, on the right, we check the sensibility of the normality
assumption (using a “normal” normal distribution, not its “log” version).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">qq_plot</span><span class="p">(</span>  <span class="c1"># see above for the definition</span>
    <span class="n">income</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">q</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">lognorm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">lsigma</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">lmu</span><span class="p">))</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantiles of LN(</span><span class="si">{</span><span class="n">lmu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">lsigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Sample quantiles&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">income</span><span class="p">)</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">income</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">qq_plot</span><span class="p">(</span><span class="n">income</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">q</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantiles of N(</span><span class="si">{</span><span class="n">mu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id40">
<span id="fig-qq-income"></span><img alt="../_images/qq-income-17.png" src="../_images/qq-income-17.png" />
<figcaption>
<p><span class="caption-number">Figure 6.9 </span><span class="caption-text">The Q-Q plots for the <code class="docutils literal notranslate"><span class="pre">income</span></code> dataset vs the fitted log-normal (good fit; left) and normal (bad fit; right) distribution.</span><a class="headerlink" href="#id40" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<div class="proof proof-type-exercise" id="id41">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.4</span>
        
    </div><div class="proof-content">
<p>Graphically compare the ECDF for <code class="docutils literal notranslate"><span class="pre">income</span></code>
and the CDF of <span class="math">\(\mathrm{LN}(10.3, 0.58)\)</span>.</p>
</div></div><div class="proof proof-type-exercise" id="id42">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.5</span>
        
    </div><div class="proof-content">
<p>(*) Perform the Kolmogorov–Smirnov goodness-of-fit test
as in <a class="reference internal" href="#sec-ks-test"><span class="std std-numref">Section 6.2.3</span></a>, to verify that the hypothesis
of log-normality is not rejected at the <span class="math">\(\alpha=0.001\)</span> significance level.
At the same time, the income distribution significantly differs
from a normal one.</p>
</div></div><p>The hypothesis that our data follow
a normal distribution is most likely false.
On the other hand, the log-normal model might be adequate.
We can again reduce the whole dataset to merely two numbers,
<span class="math">\(\mu\)</span> and <span class="math">\(\sigma\)</span>, based on which (and probability theory),
we may deduce that:</p>
<ul class="simple">
<li><p>the expected average (mean) income is <span class="math">\(e^{\mu + \sigma^2/2}\)</span>,</p></li>
<li><p>median is <span class="math">\(e^\mu\)</span>,</p></li>
<li><p>the most probable value (mode) in <span class="math">\(e^{\mu-\sigma^2}\)</span>,</p></li>
</ul>
<p>and so forth.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Recall again that for skewed a distribution such as the log-normal one,
reporting the mean might be misleading.
This is why <em>most</em> people are sceptical when they read the news
about our prospering economy (“yeah, we’d like to see that
kind of money in our pockets”). It is not only <span class="math">\(\mu\)</span> that matters,
but also <span class="math">\(\sigma\)</span> that quantifies the discrepancy between the rich
and the poor.</p>
<p>For a normal distribution, the situation is vastly different.
The mean, the median, and the most probable outcomes are the same: the distribution is symmetric around <span class="math">\(\mu\)</span>.</p>
</div>
<div class="proof proof-type-exercise" id="id43">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.6</span>
        
    </div><div class="proof-content">
<p>What is the fraction of people with earnings
below the mean in our <span class="math">\(\mathrm{LN}(10.3, 0.58)\)</span> model?
Hint: use <strong class="command">scipy.stats.lognorm.cdf</strong> to get the answer.</p>
</div></div></section>
<section id="pareto-distribution">
<span id="sec-pareto"></span><h3><span class="section-number">6.3.2. </span>Pareto distribution<a class="headerlink" href="#pareto-distribution" title="Link to this heading">¶</a></h3>
<p>Consider again the populations of the US cities:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cities</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching-data/master/other/us_cities_2000.txt&quot;</span><span class="p">)</span>
<span class="nb">len</span><span class="p">(</span><span class="n">cities</span><span class="p">),</span> <span class="nb">sum</span><span class="p">(</span><span class="n">cities</span><span class="p">)</span>  <span class="c1"># number of cities, total population</span>
<span class="c1">## (19447, 175062893.0)</span>
</pre></div>
</div>
<p><a class="reference internal" href="#fig-hist-cities"><span class="std std-numref">Figure 6.10</span></a> gives the histogram
of the city sizes on the log-scale. It looks like a log-normal distribution
again, which the readers can fit themselves when they are feeling
playful and have nothing better to do. (But, honestly, is there anything
more delightful than doing stats?)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logbins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">geomspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">cities</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">cities</span><span class="p">),</span> <span class="mi">21</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">cities</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">logbins</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Count&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id44">
<span id="fig-hist-cities"></span><img alt="../_images/hist-cities-19.png" src="../_images/hist-cities-19.png" />
<figcaption>
<p><span class="caption-number">Figure 6.10 </span><span class="caption-text">A histogram of the unabridged <code class="docutils literal notranslate"><span class="pre">cities</span></code> dataset. Note the log-scale on the x-axis.</span><a class="headerlink" href="#id44" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>This time, however, we will be concerned with not what is <em>typical</em>,
but what is in some sense <em>anomalous</em> or <em>extreme</em>.
Just like in <a class="reference internal" href="210-vector.html#sec-cities"><span class="std std-numref">Section 4.3.7</span></a>, let’s look at the <em>truncated</em> version of
the city size distribution by considering the cities with 10 000
or more inhabitants.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="mi">10_000</span>
<span class="n">large_cities</span> <span class="o">=</span> <span class="n">cities</span><span class="p">[</span><span class="n">cities</span> <span class="o">&gt;=</span> <span class="n">s</span><span class="p">]</span>  <span class="c1"># a right tail of the original dataset</span>
<span class="nb">len</span><span class="p">(</span><span class="n">large_cities</span><span class="p">),</span> <span class="nb">sum</span><span class="p">(</span><span class="n">large_cities</span><span class="p">)</span>  <span class="c1"># number of cities, total population</span>
<span class="c1">## (2696, 146199374.0)</span>
</pre></div>
</div>
<p>Plotting it on a double logarithmic scale
can be performed by calling additionally
<strong class="command">plt.yscale</strong><code class="code docutils literal notranslate"><span class="pre">(&quot;log&quot;)</span></code>, which is left as an exercise.
Doing so will lead to a picture similar to <a class="reference internal" href="#fig-fit-pareto"><span class="std std-numref">Figure 6.11</span></a>,
which reveals something remarkable. The bar tops on the
double log-scale are arranged more or less in a straight line.
There are many datasets that exhibit this behaviour.
We say that they follow a <em>power law</em> (power in the arithmetic sense, not
the political one); see, e.g., <span id="id12">[<a class="reference internal" href="999-bibliography.html#id151" title="Arnold, B.C. (2015).  Pareto Distributions. Chapman and Hall/CRC. DOI: 10.1201/b18141.">3</a>]</span>.</p>
<p>The <em>Pareto distribution</em> (type I) family has a prototypical power
law-like density. It is identified by two parameters:</p>
<ul class="simple">
<li><p>the (what <strong class="program">scipy</strong> calls it) scale parameter <span class="math">\(s&gt;0\)</span> is equal to
the shift from <span class="math">\(0\)</span>,</p></li>
<li><p>the shape parameter, <span class="math">\(\alpha&gt;0\)</span>, controls the slope of the said line on
the double log-scale.</p></li>
</ul>
<p>The probability density function of <span class="math">\(\mathrm{P}(\alpha, s)\)</span>
is given for <span class="math">\(x\ge s\)</span> by:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
f(x) = \frac{\alpha s^\alpha}{x^{\alpha+1}},
\]</div>
</div>
<p>and <span class="math">\(f(x)=0\)</span> if <span class="math">\(x &lt; s\)</span>.</p>
<p><span class="math">\(s\)</span> is usually taken as the sample minimum (i.e., 10 000 in our case).
<span class="math">\(\alpha\)</span> can be estimated through the reciprocal of
the mean of the scaled logarithms of our observations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">large_cities</span><span class="o">/</span><span class="n">s</span><span class="p">))</span>
<span class="n">alpha</span>
<span class="c1">## 0.9496171695997675</span>
</pre></div>
</div>
<p>The left side of <a class="reference internal" href="#fig-fit-pareto"><span class="std std-numref">Figure 6.11</span></a> compares the theoretical density
and an empirical histogram on the double log-scale. The right part gives
the corresponding Q-Q plot on a double logarithmic scale.
We see that the populations of the largest cities are overestimated.
The model could be better, but the cities are still growing, aren’t they?</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">logbins</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">geomspace</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">large_cities</span><span class="p">),</span> <span class="mi">21</span><span class="p">)</span>  <span class="c1"># bin boundaries</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">large_cities</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">logbins</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">logbins</span><span class="p">,</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pareto</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">logbins</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s</span><span class="p">),</span>
    <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;PDF of P(</span><span class="si">{</span><span class="n">alpha</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Density&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">qq_plot</span><span class="p">(</span><span class="n">large_cities</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">q</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pareto</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantiles of P(</span><span class="si">{</span><span class="n">alpha</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Sample quantiles&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id45">
<span id="fig-fit-pareto"></span><img alt="../_images/fit-pareto-21.png" src="../_images/fit-pareto-21.png" />
<figcaption>
<p><span class="caption-number">Figure 6.11 </span><span class="caption-text">A histogram (left) and a Q-Q plot (right) of the <code class="docutils literal notranslate"><span class="pre">large_cities</span></code> dataset vs the fitted density of a Pareto distribution on a double log-scale.</span><a class="headerlink" href="#id45" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<div class="proof proof-type-example" id="id46">

    <div class="proof-title">
        <span class="proof-type">Example 6.7</span>
        
    </div><div class="proof-content">
<p>(*) We might also be keen on verifying how accurately the
probability of a randomly selected city’s being at least of a given size
can be predicted. Let’s denote by <span class="math">\(S(x)=1-F(x)\)</span> the <em>complementary
cumulative distribution function</em> (CCDF; sometimes referred to as
the survival function), and by <span class="math">\(\hat{S}_n(x)=1-\hat{F}_n(x)\)</span>
its empirical version. <a class="reference internal" href="#fig-ccdf-pareto"><span class="std std-numref">Figure 6.12</span></a> compares the empirical
and the fitted CCDFs with probabilities on the linear- and log-scale.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">geomspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">large_cities</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">large_cities</span><span class="p">),</span> <span class="mi">1001</span><span class="p">)</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pareto</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">large_cities</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">probs</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;CCDF of P(</span><span class="si">{</span><span class="n">alpha</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">large_cities</span><span class="p">),</span> <span class="mi">1</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">,</span>
        <span class="n">drawstyle</span><span class="o">=</span><span class="s2">&quot;steps-post&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Empirical CCDF&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">([</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="s2">&quot;log&quot;</span><span class="p">][</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Prob(city size &gt; x)&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id47">
<span id="fig-ccdf-pareto"></span><img alt="../_images/ccdf-pareto-23.png" src="../_images/ccdf-pareto-23.png" />
<figcaption>
<p><span class="caption-number">Figure 6.12 </span><span class="caption-text">The empirical and theoretical complementary cumulative distribution functions for the <code class="docutils literal notranslate"><span class="pre">large_cities</span></code> dataset with probabilities on the linear- (left) and log-scale (right) and city sizes on the log-scale.</span><a class="headerlink" href="#id47" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>In terms of the maximal absolute distance between the two functions,
<span class="math">\(\hat{D}_n\)</span>, from the left plot we see that the fit seems acceptable.
Still, let’s stress that the log-scale overemphasises the relatively
minor differences in the right tail and should not be used for judging
the value of <span class="math">\(\hat{D}_n\)</span>.</p>
<p>However, that the Kolmogorov–Smirnov goodness-of-fit test
rejects the hypothesis of Paretianity (at a significance level <span class="math">\(0.1\%\)</span>)
is left as an exercise for the reader.</p>
</div></div></section>
<section id="uniform-distribution">
<h3><span class="section-number">6.3.3. </span>Uniform distribution<a class="headerlink" href="#uniform-distribution" title="Link to this heading">¶</a></h3>
<p>In the Polish <em>Lotto</em> lottery, six numbered balls
<span class="math">\(\{1,2,\dots,49\}\)</span> are drawn without replacement from an urn.
Here is a dataset that summarises the number of times
each ball has been drawn in all the games in the period 1957–2016:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lotto</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching-data/master/marek/lotto_table.txt&quot;</span><span class="p">)</span>
<span class="n">lotto</span>
<span class="c1">## array([720., 720., 714., 752., 719., 753., 701., 692., 716., 694., 716.,</span>
<span class="c1">##        668., 749., 713., 723., 693., 777., 747., 728., 734., 762., 729.,</span>
<span class="c1">##        695., 761., 735., 719., 754., 741., 750., 701., 744., 729., 716.,</span>
<span class="c1">##        768., 715., 735., 725., 741., 697., 713., 711., 744., 652., 683.,</span>
<span class="c1">##        744., 714., 674., 654., 681.])</span>
</pre></div>
</div>
<p>All events seem to occur more or less with the same probability.
Of course, the numbers on the balls are integer,
but in our idealised scenario, we may try modelling this dataset
using a continuous <em>uniform distribution</em> <span class="math">\(\mathrm{U}(a, b)\)</span>,
which yields arbitrary real numbers on a given interval <span class="math">\((a, b)\)</span>,
i.e., between some <span class="math">\(a\)</span> and <span class="math">\(b\)</span>. Its probability density function is
given for <span class="math">\(x\in(a, b)\)</span> by:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
f(x) = \frac{1}{b-a},
\]</div>
</div>
<p>and <span class="math">\(f(x)=0\)</span> otherwise.
Notice that <strong class="command">scipy.stats.uniform</strong>
uses parameters <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">scale</span></code>, the latter being equal to our <span class="math">\(b-a\)</span>.</p>
<p>In the Lotto case, it makes sense to set <span class="math">\(a=1\)</span> and <span class="math">\(b=50\)</span> and interpret
an outcome like 49.1253 as representing the 49th ball
(compare the notion of the floor function, <span class="math">\(\lfloor x\rfloor\)</span>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">1001</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">lotto</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">lotto</span><span class="p">),</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s2">&quot;edge&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">49</span><span class="p">),</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;PDF of U(1, 50)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.025</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id48">
<span id="fig-lotto"></span><img alt="../_images/lotto-25.png" src="../_images/lotto-25.png" />
<figcaption>
<p><span class="caption-number">Figure 6.13 </span><span class="caption-text">A histogram of the <code class="docutils literal notranslate"><span class="pre">lotto</span></code> dataset.</span><a class="headerlink" href="#id48" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>Visually, see <a class="reference internal" href="#fig-lotto"><span class="std std-numref">Figure 6.13</span></a>, this model makes much sense,
but again, some more rigorous statistical testing would be required to
determine if someone has not been tampering with the lottery results, i.e.,
if data do not deviate from the uniform distribution significantly.
Unfortunately, we cannot use the Kolmogorov–Smirnov test in the
foregoing version as data are not continuous.
See, however, <a class="reference internal" href="420-categorical.html#sec-chisq-test"><span class="std std-numref">Section 11.4.3</span></a>
for the Pearson chi-squared test which is applicable here.</p>
<div class="proof proof-type-exercise" id="id49">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.8</span>
        
    </div><div class="proof-content">
<p>Does playing lotteries
and engaging in gambling make <em>rational</em> sense at all,
from the perspective of an individual player?
Well, we see that 16 is the most frequently occurring outcome
in <em>Lotto</em>, maybe there’s some magic in it?
Also, some people sometimes became millionaires, don’t they?</p>
</div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>In data modelling (e.g., Bayesian statistics),
sometimes a uniform distribution is chosen
as a placeholder for “we know nothing about a phenomenon,
so let’s just assume that every event is equally likely”.
Nonetheless, it is fascinating that in the end, the real world tends
to be structured. Patterns that emerge are plentiful, and
most often they are far from being uniformly distributed.
Even more strikingly, they are subject to quantitative analysis.</p>
</div>
</section>
<section id="distribution-mixtures">
<span id="sec-mixtures"></span><h3><span class="section-number">6.3.4. </span>Distribution mixtures (*)<a class="headerlink" href="#distribution-mixtures" title="Link to this heading">¶</a></h3>
<p>Certain datasets may fail to fit through simple probabilistic models.
It may sometimes be due to their non-random behaviour:
statistics gives one of many means to create data idealisations, but
in data science we can also employ partial differential equations,
graphs and complex networks, agent-based modelling, cellular automata,
amongst many others. They all might be worth giving a study (and then try).</p>
<p>It may also happen that what we observe is, in fact, a <em>mixture</em>
of simpler processes.
The dataset representing the December 2021 hourly averages
pedestrian counts near the Southern Cross Station in Melbourne
is a likely instance of such a scenario;
compare <a class="reference internal" href="210-vector.html#fig-peds-histogram"><span class="std std-numref">Figure 4.5</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">peds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching-data/master/marek/southern_cross_station_peds_2019_dec.txt&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>It might not be a bad idea to
try to fit a probabilistic (convex) combination
of three normal distributions <span class="math">\(f_1\)</span>, <span class="math">\(f_2\)</span>, <span class="math">\(f_3\)</span>,
corresponding to the morning, lunchtime, and evening
pedestrian count peaks. This yields the PDF:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
f(x) = w_1 f_1(x) + w_2 f_2(x) + w_3 f_3(x),
\]</div>
</div>
<p>for some coefficients <span class="math">\(w_1,w_2,w_3\ge 0\)</span> such that <span class="math">\(w_1+w_2+w_3=1\)</span>.</p>
<p><a class="reference internal" href="#fig-mixture"><span class="std std-numref">Figure 6.14</span></a> depicts a mixture of
<span class="math">\(\mathrm{N}(8.4, 0.9)\)</span>, <span class="math">\(\mathrm{N}(14.2, 4)\)</span>, and <span class="math">\(\mathrm{N}(17.3, 0.9)\)</span>
with the corresponding weights of <span class="math">\(0.27\)</span>, <span class="math">\(0.53\)</span>, and <span class="math">\(0.2\)</span>.
This dataset is coarse-grained: we only have 24 bar heights at our
disposal. Consequently, the estimated<a class="footnote-reference brackets" href="#footnotestimated" id="id13" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a> coefficients
should be taken with a pinch of Sichuan pepper.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">24</span><span class="p">)</span><span class="o">+</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">peds</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">peds</span><span class="p">),</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
<span class="n">p1</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span>  <span class="mf">8.4</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
<span class="n">p2</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">14.2</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">)</span>
<span class="n">p3</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">17.3</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.27</span><span class="o">*</span><span class="n">p1</span> <span class="o">+</span> <span class="mf">0.53</span><span class="o">*</span><span class="n">p2</span> <span class="o">+</span> <span class="mf">0.20</span><span class="o">*</span><span class="n">p3</span>  <span class="c1"># a weighted combination of three densities</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;PDF of a normal mixture&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">24</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id50">
<span id="fig-mixture"></span><img alt="../_images/mixture-27.png" src="../_images/mixture-27.png" />
<figcaption>
<p><span class="caption-number">Figure 6.14 </span><span class="caption-text">A histogram of the <code class="docutils literal notranslate"><span class="pre">peds</span></code> dataset and an estimated mixture of three normal distributions.</span><a class="headerlink" href="#id50" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>More complex entities (models, methods) frequently arise as combinations
of simpler (primitive) components. This is why we ought to spend
a great deal of time studying the <em>fundamentals</em>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some data clustering techniques (in particular, the <span class="math">\(k\)</span>-means algorithm
that we briefly discuss later in this course) can be used to split a data
sample into disjoint chunks corresponding to different mixture components.
Also, it might be the case that the subpopulations are identified by another
categorical variable that divides the dataset into natural groups; compare
<a class="reference internal" href="430-group-by.html#chap-group-by"><span class="std std-numref">Chapter 12</span></a>.</p>
</div>
</section>
</section>
<section id="generating-pseudorandom-numbers">
<span id="sec-pseudorandom"></span><h2><span class="section-number">6.4. </span>Generating pseudorandom numbers<a class="headerlink" href="#generating-pseudorandom-numbers" title="Link to this heading">¶</a></h2>
<p>A probability distribution is useful not only for describing a dataset.
It also enables us to perform many experiments on data that we do not
currently have, but we might obtain in the future,
to test various scenarios and hypotheses.
Let’s thus discuss some methods for  generating random samples of
independent (not related to each other) observations.</p>
<section id="id14">
<h3><span class="section-number">6.4.1. </span>Uniform distribution<a class="headerlink" href="#id14" title="Link to this heading">¶</a></h3>
<p>When most people say <em>random</em>, they implicitly mean
<em>uniformly distributed</em>. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="c1">## array([0.69646919, 0.28613933, 0.22685145, 0.55131477, 0.71946897])</span>
</pre></div>
</div>
<p>gives five observations sampled independently from the uniform distribution
on the unit interval, i.e., <span class="math">\(\mathrm{U}(0, 1)\)</span>.
Here is the same with <strong class="program">scipy</strong>, but this time the support is
<span class="math">\((-10, 15)\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>  <span class="c1"># from -10 to -10+25</span>
<span class="c1">## array([ 0.5776615 , 14.51910496,  7.12074346,  2.02329754, -0.19706205])</span>
</pre></div>
</div>
<p>Alternatively, we could have shifted and scaled the output of the
random number generator on the unit interval using the formula
<strong class="command">numpy.random.rand</strong><code class="code docutils literal notranslate"><span class="pre">(5)*25-10</span></code>.</p>
</section>
<section id="not-exactly-random">
<span id="sec-seed"></span><h3><span class="section-number">6.4.2. </span>Not exactly random<a class="headerlink" href="#not-exactly-random" title="Link to this heading">¶</a></h3>
<p>We generate numbers using a computer, which is a purely deterministic
machine. Albeit they are indistinguishable from truly random
when subject to rigorous tests for randomness,
we refer to them as <em>pseudorandom</em> or random-like ones.</p>
<p>To prove that they are not random-random, let’s set a specific initial
state of the generator (the <em>seed</em>) and inspect what values are produced:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>  <span class="c1"># set the seed (the ID of the initial state)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="c1">## array([0.69646919, 0.28613933, 0.22685145, 0.55131477, 0.71946897])</span>
</pre></div>
</div>
<p>Now, let’s set the same seed and see how “random” the next values are:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>  <span class="c1"># set seed</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="c1">## array([0.69646919, 0.28613933, 0.22685145, 0.55131477, 0.71946897])</span>
</pre></div>
</div>
<p>Nobody expected that. Such a behaviour is very welcome, though.
It enables us to perform completely <em>reproducible</em> numerical
experiments, and truly scientific inquiries tend to nourish identical
results under the same conditions.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If we do not set the seed manually,
it will be initialised based on the current wall time, which is
different every… time. As a result, the numbers will <em>seem</em> random to us,
but only because we are slightly ignorant.</p>
</div>
<p>Many Python packages that we refer to in the sequel,
including <strong class="program">pandas</strong> and <strong class="program">scikit-learn</strong>, rely
on <strong class="program">numpy</strong>’s random number generator.
To harness them, we will have to become used to calling
<strong class="command">numpy.random.seed</strong>. Additionally, some of them
(e.g., <strong class="command">sklearn.model_selection.train_test_split</strong>
or <strong class="command">pandas.DataFrame.sample</strong>) will be equipped with
the <code class="docutils literal notranslate"><span class="pre">random_state</span></code> argument, which behaves as if we <em>temporarily</em> changed
the seed (for just one call to that function). For instance:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="c1">## array([0.69646919, 0.28613933, 0.22685145, 0.55131477, 0.71946897])</span>
</pre></div>
</div>
<p>We obtained the same sequence again.</p>
</section>
<section id="sampling-from-other-distributions">
<h3><span class="section-number">6.4.3. </span>Sampling from other distributions<a class="headerlink" href="#sampling-from-other-distributions" title="Link to this heading">¶</a></h3>
<p>Generating data from other distributions is possible too;
there are many <strong class="command">rvs</strong> methods implemented
in <strong class="program">scipy.stats</strong>.
For example, here is a sample from <span class="math">\(\mathrm{N}(160.1, 7.06)\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mf">160.1</span><span class="p">,</span> <span class="mf">7.06</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">50489</span><span class="p">)</span>
<span class="c1">## array([166.01775384, 136.7107872 , 185.30879579])</span>
</pre></div>
</div>
<p>Pseudorandom deviates from the <em>standard</em> normal distribution,
i.e., <span class="math">\(\mathrm{N}(0, 1)\)</span>, can also be generated using
<strong class="command">numpy.random.randn</strong>.
As <span class="math">\(\mathrm{N}(160.1, 7.06)\)</span> is a scaled and shifted version thereof,
the preceding is equivalent to:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">50489</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">*</span><span class="mf">7.06</span> <span class="o">+</span> <span class="mf">160.1</span>
<span class="c1">## array([166.01775384, 136.7107872 , 185.30879579])</span>
</pre></div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Conclusions based on simulated data are trustworthy for they cannot
be manipulated. Or can they?</p>
<p>The above pseudorandom number generator’s seed,
<code class="docutils literal notranslate"><span class="pre">50489</span></code>, is a bit suspicious. It may suggest that someone
wanted to <em>prove</em> some point (in this case, the violation
of the <span class="math">\(3\sigma\)</span> rule).
This is why we recommend sticking to only one seed,
e.g., <code class="docutils literal notranslate"><span class="pre">123</span></code>, or – when performing simulations – setting
the consecutive natural seeds in each iteration
of the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop: <code class="docutils literal notranslate"><span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">2</span></code>, ….</p>
</div>
<div class="proof proof-type-exercise" id="id51">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.9</span>
        
    </div><div class="proof-content">
<p>Generate 1000 pseudorandom numbers from the log-normal
distribution and draw its histogram.</p>
</div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>(*)
Having a reliable pseudorandom number generator from the uniform distribution
on the unit interval is crucial as sampling from other distributions
usually involves transforming the independent <span class="math">\(\mathrm{U}(0, 1)\)</span> variates.
For instance, realisations of random variables following any continuous
cumulative distribution function <span class="math">\(F\)</span> can be constructed through
the <em>inverse transform sampling</em>:</p>
<ol class="arabic simple">
<li><p>Generate a sample <span class="math">\(x_1,\dots,x_n\)</span> independently from <span class="math">\(\mathrm{U}(0, 1)\)</span>.</p></li>
<li><p>Transform each <span class="math">\(x_i\)</span> by applying the quantile function,
<span class="math">\(y_i=F^{-1}(x_i)\)</span>.</p></li>
</ol>
<p>Now <span class="math">\(y_1,\dots,y_n\)</span> follows the CDF <span class="math">\(F\)</span>.</p>
</div>
<div class="proof proof-type-exercise" id="id52">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.10</span>
        
    </div><div class="proof-content">
<p>(*) Generate 1000 pseudorandom numbers from the log-normal
distribution using inverse transform sampling.</p>
</div></div><div class="proof proof-type-exercise" id="id53">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.11</span>
        
    </div><div class="proof-content">
<p>(**) Generate 1000 pseudorandom numbers from the distribution
mixture discussed in <a class="reference internal" href="#sec-mixtures"><span class="std std-numref">Section 6.3.4</span></a>.</p>
</div></div></section>
<section id="natural-variability">
<span id="sec-natural-variability"></span><h3><span class="section-number">6.4.4. </span>Natural variability<a class="headerlink" href="#natural-variability" title="Link to this heading">¶</a></h3>
<p>Even a sample truly generated from a specific distribution
will deviate from it, sometimes considerably.
Such effects will be especially visible for small sample sizes,
but they usually dissolve<a class="footnote-reference brackets" href="#footfts" id="id15" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a> when the availability of data increases.</p>
<p>For example, <a class="reference internal" href="#fig-natural-variability"><span class="std std-numref">Figure 6.15</span></a> depicts the histograms of nine
different samples of size 100, all drawn independently from the
standard normal distribution.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.figsize&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># width=height</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id54">
<span id="fig-natural-variability"></span><img alt="../_images/natural-variability-29.png" src="../_images/natural-variability-29.png" />
<figcaption>
<p><span class="caption-number">Figure 6.15 </span><span class="caption-text">All nine samples are normally distributed.</span><a class="headerlink" href="#id54" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>There is a certain ruggedness in the bar sizes that a naïve observer
would try to interpret as something meaningful.
Competent data scientists train their eyes to ignore such impurities.
In this case, they are only due to random effects.
Nevertheless, we must always be ready to detect <em>deviations</em> from
the assumed model that are worth attention.</p>
<div class="proof proof-type-exercise" id="id55">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.12</span>
        
    </div><div class="proof-content">
<p>Repeat the above experiment for samples of sizes 10, 1 000, and 10 000.</p>
</div></div><div class="proof proof-type-example" id="id56">

    <div class="proof-title">
        <span class="proof-type">Example 6.13</span>
        
    </div><div class="proof-content">
<p>(*) Using a simple Monte Carlo simulation,
we can verify (approximately) that the Kolmogorov–Smirnov
goodness-of-fit test introduced in <a class="reference internal" href="#sec-ks-test"><span class="std std-numref">Section 6.2.3</span></a>
has been calibrated properly,
i.e., that for samples that really follow the assumed distribution,
the null hypothesis is indeed rejected in roughly 0.1% of the cases.</p>
<p>Assume we are interested in the null hypothesis
referencing the standard normal distribution, <span class="math">\(\mathrm{N}(160.1, 7.06)\)</span>,
and sample size <span class="math">\(n=4221\)</span>.
We need to generate many (we assume 10 000 below) such samples.
For each of them, we compute and store the maximal absolute deviation from
the theoretical CDF, i.e., <span class="math">\(\hat{D}_n\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">4221</span>
<span class="n">distrib</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mf">160.1</span><span class="p">,</span> <span class="mf">7.06</span><span class="p">)</span>  <span class="c1"># the assumed distribution</span>
<span class="n">Dns</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>  <span class="c1"># increase this for better precision</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">distrib</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># really follows the distrib.</span>
    <span class="n">Dns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">compute_Dn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">distrib</span><span class="o">.</span><span class="n">cdf</span><span class="p">))</span>
<span class="n">Dns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Dns</span><span class="p">)</span>
</pre></div>
</div>
<p>Now let’s compute the proportion of cases which lead to
<span class="math">\(\hat{D}_n\)</span> greater than the critical value <span class="math">\(K_n\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">Dns</span><span class="p">[</span><span class="n">Dns</span> <span class="o">&gt;=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">kstwo</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n</span><span class="p">)])</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">Dns</span><span class="p">)</span>
<span class="c1">## 0.0008</span>
</pre></div>
</div>
<p>Its expected value is 0.001.
But our approximation is necessarily imprecise because we rely on randomness.
Increasing the number of trials from 10 000 to, say, 1 000 000
should make the above estimate closer to the theoretical expectation.</p>
<p>It is also worth checking that the density histogram of <code class="docutils literal notranslate"><span class="pre">Dns</span></code> resembles
the Kolmogorov distribution that we can compute via
<strong class="command">scipy.stats.kstwo.pdf</strong>.</p>
</div></div><div class="proof proof-type-exercise" id="id57">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.14</span>
        
    </div><div class="proof-content">
<p>(*) It might also be interesting to verify the test’s <em>power</em>,
i.e., the probability that when the null hypothesis is false,
it will actually be rejected.
Modify the above code in such a way that <code class="docutils literal notranslate"><span class="pre">x</span></code> in the <strong class="command">for</strong> loop
is not generated from <span class="math">\(\mathrm{N}(160.1, 7.06)\)</span>, but
<span class="math">\(\mathrm{N}(140, 7.06)\)</span>, <span class="math">\(\mathrm{N}(141, 7.06)\)</span>, etc.,
and check the proportion of cases where we deem the sample distribution <em>significantly</em> different from
<span class="math">\(\mathrm{N}(160.1, 7.06)\)</span>.
Small deviations from the true location parameter <span class="math">\(\mu\)</span> are usually
ignored, but this improves with sample size <span class="math">\(n\)</span>.</p>
</div></div></section>
<section id="adding-jitter-white-noise">
<h3><span class="section-number">6.4.5. </span>Adding jitter (white noise)<a class="headerlink" href="#adding-jitter-white-noise" title="Link to this heading">¶</a></h3>
<p>We mentioned that measurements might be subject to observational
error. Rounding can also occur as early as in
the data collection phase. In particular, our <code class="docutils literal notranslate"><span class="pre">heights</span></code>
dataset is precise up to 1 fractional digit.
However, in statistics, when we say that data follow
a continuous distribution, the probability of having two identical
values in a sample is 0. Therefore, some data analysis methods
might assume no ties in the input vector, i.e.,
that all values are unique.</p>
<p>The easiest way to deal with such numerical inconveniences
is to add some white noise with the expected value of 0,
either uniformly or normally distributed.</p>
<p>For example, for <code class="docutils literal notranslate"><span class="pre">heights</span></code>, it makes sense to add some jitter from
<span class="math">\(\mathrm{U}[-0.05, 0.05]\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">heights_jitter</span> <span class="o">=</span> <span class="n">heights</span> <span class="o">+</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">heights</span><span class="p">))</span><span class="o">*</span><span class="mf">0.1</span><span class="o">-</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">heights_jitter</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>  <span class="c1"># preview</span>
<span class="c1">## array([160.21704623, 152.68870195, 161.24482407, 157.3675293 ,</span>
<span class="c1">##        154.61663465, 144.68964596])</span>
</pre></div>
</div>
<p>Adding noise also might be performed for aesthetic reasons,
e.g., when drawing scatter plots.</p>
</section>
<section id="independence-assumption">
<h3><span class="section-number">6.4.6. </span>Independence assumption<a class="headerlink" href="#independence-assumption" title="Link to this heading">¶</a></h3>
<p>Let’s generate nine binary digits in a pseudorandom fashion:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="mi">9</span><span class="p">)</span>
<span class="c1">## array([1, 1, 1, 1, 1, 1, 1, 1, 1])</span>
</pre></div>
</div>
<p>We can consider ourselves very lucky; all numbers are the same.
So, the next number <em>must</em> finally be a “zero”, right?</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1">## array([1])</span>
</pre></div>
</div>
<p>Wrong. The numbers we generate are <em>independent</em> of each other.
There is no history. In the current model of randomness (Bernoulli trials;
two possible outcomes with the same probability), there is a 50% chance
of obtaining a “one” <em>regardless</em> of how many “ones” were observed
previously.</p>
<p>We should not seek patterns where no regularities exist. Our brain forms
expectations about the world, and overcoming them is hard work.
This must be done as the reality could not care less about
what we consider it to be.</p>
</section>
</section>
<section id="further-reading">
<h2><span class="section-number">6.5. </span>Further reading<a class="headerlink" href="#further-reading" title="Link to this heading">¶</a></h2>
<p>For an excellent general introductory course on probability
and statistics, see <span id="id16">[<a class="reference internal" href="999-bibliography.html#id50" title="Gentle, J.E. (2009).  Computational Statistics. Springer-Verlag.">40</a>, <a class="reference internal" href="999-bibliography.html#id45" title="Gentle, J.E. (2020).  Theory of Statistics. book draft. URL: https://mason.gmu.edu/~jgentle/books/MathStat.pdf.">41</a>]</span>
and also <span id="id17">[<a class="reference internal" href="999-bibliography.html#id158" title="Ross, S.M. (2020).  Introduction to Probability and Statistics for Engineers and Scientists. Academic Press.">82</a>, <a class="reference internal" href="999-bibliography.html#id159" title="Ross, S.M. (2024).  Introduction to Probability Models. Elsevier.">83</a>]</span>.
More advanced students are likely to enjoy other
classics such as <span id="id18">[<a class="reference internal" href="999-bibliography.html#id162" title="Bartoszyński, R. and Niewiadomska-Bugaj, M. (2007).  Probability and Statistical Inference. Wiley.">5</a>, <a class="reference internal" href="999-bibliography.html#id46" title="Billingsley, P. (1995).  Probability and Measure. John Wiley &amp; Sons.">9</a>, <a class="reference internal" href="999-bibliography.html#id44" title="Cramér, H. (1946).  Mathematical Methods of Statistics. Princeton University Press. URL: https://archive.org/details/in.ernet.dli.2015.223699.">19</a>, <a class="reference internal" href="999-bibliography.html#id161" title="Feller, W. (1950).  An Introduction to Probability Theory and Its Applications: Volume I. Wiley.">28</a>, <a class="reference internal" href="999-bibliography.html#id160" title="Grimmett, G.R. and Stirzaker, D.R. (2020).  Probability and Random Processes. Oxford University Press.">46</a>]</span>.
To go beyond the basics, check out <span id="id19">[<a class="reference internal" href="999-bibliography.html#id106" title="Efron, B. and Hastie, T. (2016).  Computer Age Statistical Inference: Algorithms, Evidence, and Data Science. Cambridge University Press.">26</a>]</span>.
Topics in random number generation are covered in
<span id="id20">[<a class="reference internal" href="999-bibliography.html#id49" title="Gentle, J.E. (2003).  Random Number Generation and Monte Carlo Methods. Springer.">39</a>, <a class="reference internal" href="999-bibliography.html#id141" title="Knuth, D.E. (1997).  The Art of Computer Programming II: Seminumerical Algorithms. Addison-Wesley.">59</a>, <a class="reference internal" href="999-bibliography.html#id51" title="Robert, C.P. and Casella, G. (2004).  Monte Carlo Statistical Methods. Springer-Verlag.">81</a>]</span>.</p>
<p>For a more detailed introduction to exploratory data analysis,
see the classical books by Tukey <span id="id21">[<a class="reference internal" href="999-bibliography.html#id147" title="Tukey, J.W. (1962).  The future of data analysis. Annals of Mathematical Statistics, 33(1):1–67. URL: https://projecteuclid.org/journalArticle/Download?urlId=10.1214%2Faoms%2F1177704711, DOI: 10.1214/aoms/1177704711.">92</a>, <a class="reference internal" href="999-bibliography.html#id146" title="Tukey, J.W. (1977).  Exploratory Data Analysis. Addison-Wesley.">93</a>]</span>,
Tufte <span id="id22">[<a class="reference internal" href="999-bibliography.html#id145" title="Tufte, E.R. (2001).  The Visual Display of Quantitative Information. Graphics Press.">91</a>]</span>, and Wainer <span id="id23">[<a class="reference internal" href="999-bibliography.html#id143" title="Wainer, H. (1997).  Visual Revelations: Graphical Tales of Fate and Deception from Napoleon Bonaparte to Ross Perot. Copernicus.">98</a>]</span>.</p>
<div style="margin-top: 1em"></div><p>We took the logarithm of the log-normally distributed
incomes and obtained a normally distributed sample.
In statistical practice, it is not rare to apply different
non-linear transforms of the input vectors at the data preprocessing stage (see, e.g.,
<a class="reference internal" href="330-relationship.html#sec-linearisation"><span class="std std-numref">Section 9.2.6</span></a>). In particular, the Box–Cox (power) transform
<span id="id24">[<a class="reference internal" href="999-bibliography.html#id148" title="Box, G.E.P. and Cox, D.R. (1964).  An analysis of transformations. Journal of the Royal Statistical Society. Series B (Methodological), 26(2):211–252.">12</a>]</span> is of the form <span class="math">\(x\mapsto (x^\lambda-1)/\lambda\)</span>
for some <span class="math">\(\lambda\)</span>.
Interestingly, in the limit as <span class="math">\(\lambda\to 0\)</span>, this formula yields
<span class="math">\(x\mapsto \log x\)</span> which is exactly what we were applying in this chapter.</p>
<p>Newman et al. <span id="id25">[<a class="reference internal" href="999-bibliography.html#id43" title="Clauset, A., Shalizi, C.R., and Newman, M.E.J. (2009).  Power-law distributions in empirical data. SIAM Review, 51(4):661–703. DOI: 10.1137/070710111.">16</a>, <a class="reference internal" href="999-bibliography.html#id42" title="Newman, M.E.J. (2005).  Power laws, Pareto distributions and Zipf's law. Contemporary Physics, pages 323–351. DOI: 10.1080/00107510500052444.">71</a>]</span> give a nice overview of the
power-law-like behaviour of some “rich” or otherwise extreme datasets.
It is worth noting that the logarithm of a Paretian sample
divided by the minimum follows an exponential distribution
(which we discuss in <a class="reference internal" href="530-time-series.html#chap-time-series"><span class="std std-numref">Chapter 16</span></a>).
For a comprehensive catalogue of statistical distributions,
their properties, and relationships between them,
see <span id="id26">[<a class="reference internal" href="999-bibliography.html#id83" title="Forbes, C., Evans, M., Hastings, N., and Peacock, B. (2010).  Statistical Distributions. Wiley.">29</a>]</span>.</p>
</section>
<section id="exercises">
<h2><span class="section-number">6.6. </span>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">¶</a></h2>
<div class="proof proof-type-exercise" id="id58">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.15</span>
        
    </div><div class="proof-content">
<p>Why is the notion of the mean income confusing to the general public?</p>
</div></div><div class="proof proof-type-exercise" id="id59">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.16</span>
        
    </div><div class="proof-content">
<p>When manually setting the seed of a pseudorandom number generator makes sense?</p>
</div></div><div class="proof proof-type-exercise" id="id60">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.17</span>
        
    </div><div class="proof-content">
<p>Given a log-normally distributed sample <code class="docutils literal notranslate"><span class="pre">x</span></code>, how can we turn it
to a normally distributed one, i.e., <code class="docutils literal notranslate"><span class="pre">y=</span></code><strong class="command">f</strong><code class="code docutils literal notranslate"><span class="pre">(x)</span></code>,
with <strong class="command">f</strong> being… what?</p>
</div></div><div class="proof proof-type-exercise" id="id61">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.18</span>
        
    </div><div class="proof-content">
<p>What is the <span class="math">\(3\sigma\)</span> rule for normally distributed data?</p>
</div></div><div class="proof proof-type-exercise" id="id62">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.19</span>
        
    </div><div class="proof-content">
<p>Can the <span class="math">\(3\sigma\)</span> rule be applied for log-normally distributed data?</p>
</div></div><div class="proof proof-type-exercise" id="id63">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.20</span>
        
    </div><div class="proof-content">
<p>(*) How can we verify graphically if a sample follows a hypothesised
theoretical distribution?</p>
</div></div><div class="proof proof-type-exercise" id="id64">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.21</span>
        
    </div><div class="proof-content">
<p>(*) Explain the meaning of the type I error, significance level,
and a test’s power.</p>
</div></div><hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="foothistconvergence" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">1</a><span class="fn-bracket">]</span></span>
<p>(*) This intuition is, of course, theoretically
grounded and is based on the asymptotic behaviour of the histograms
as the estimators of the underlying probability density function;
see, e.g., <span id="id27">[<a class="reference internal" href="999-bibliography.html#id48" title="Freedman, D. and Diaconis, P. (1981).  On the histogram as a density estimator: L₂ theory. Zeitschrift für Wahrscheinlichkeitstheorie und Verwandte Gebiete, 57:453–476.">30</a>]</span> and the many references therein.</p>
</aside>
<aside class="footnote brackets" id="footmle" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">2</a><span class="fn-bracket">]</span></span>
<p>(*) Sometimes we will have many point estimators to choose
from, some being more suitable than others if data are
not of top quality (e.g., contain outliers). For instance, in the normal
model, we can also estimate <span class="math">\(\mu\)</span> and <span class="math">\(\sigma\)</span>
via the sample median and <span class="math">\(\mathrm{IQR}/1.349\)</span>.</p>
<p>(**) It might also be the case that we will have to obtain
the estimates of a probability distribution’s parameters
by numerical optimisation because there are no known open-form formulae
therefor. For example, in the case of the normal family,
the maximum likelihood estimation problem involves minimising
<span class="math">\(
\mathcal{L}(\mu, \sigma) = \sum_{i=1}^n \left(
    \frac{(x_i-\mu)^2}{\sigma^2} + \log \sigma^2
\right)
\)</span>
with respect to <span class="math">\(\mu\)</span> and <span class="math">\(\sigma\)</span> (here, we are lucky
for its solution is exactly the sample mean and standard deviation).</p>
</aside>
<aside class="footnote brackets" id="footcdf" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">3</a><span class="fn-bracket">]</span></span>
<p>The probability distribution of any
real-valued random variable <span class="math">\(X\)</span> can be uniquely defined
by means of a nondecreasing, right (upward) continuous
function <span class="math">\(F:\mathbb{R}\to[0, 1]\)</span> such that
<span class="math">\(\lim_{x\to-\infty} F(x)=0\)</span> and <span class="math">\(\lim_{x\to\infty} F(x)=1\)</span>,
in which case <span class="math">\(\Pr(X\le x)=F(x)\)</span>.
The probability density function only exists for continuous
random variables and is defined as the derivative of <span class="math">\(F\)</span>.</p>
</aside>
<aside class="footnote brackets" id="footsuffsmall" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">4</a><span class="fn-bracket">]</span></span>
<p>The larger the sample size, the less tolerant
regarding the size of this disparity we are;
see <a class="reference internal" href="#sec-ks-test"><span class="std std-numref">Section 6.2.3</span></a>.</p>
</aside>
<aside class="footnote brackets" id="footquant" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">5</a><span class="fn-bracket">]</span></span>
<p>More generally, for an arbitrary <span class="math">\(F\)</span>, <span class="math">\(Q\)</span> is
its <em>generalised</em> inverse, defined
for any <span class="math">\(p\in(0, 1)\)</span> as <span class="math">\(Q(p) = \inf\{ x: F(x) \ge p \}\)</span>,
i.e., the smallest <span class="math">\(x\)</span> such that the probability of drawing
a value not greater than <span class="math">\(x\)</span> is at least <span class="math">\(p\)</span>.</p>
</aside>
<aside class="footnote brackets" id="footprobplot" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">6</a><span class="fn-bracket">]</span></span>
<p>(*) <strong class="command">scipy.stats.probplot</strong> uses a slightly
different definition (there are many other ones in common use).</p>
</aside>
<aside class="footnote brackets" id="footqqplotpearson" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">7</a><span class="fn-bracket">]</span></span>
<p>(*) We can quantify (informally) the goodness of fit
by using the Pearson linear correlation coefficient;
see <a class="reference internal" href="330-relationship.html#sec-pearson"><span class="std std-numref">Section 9.1.1</span></a>.</p>
</aside>
<aside class="footnote brackets" id="footmost" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">8</a><span class="fn-bracket">]</span></span>
<p>Except for the few filthy rich, who are interesting on their
own; see <a class="reference internal" href="#sec-pareto"><span class="std std-numref">Section 6.3.2</span></a> where we discuss the Pareto distribution.</p>
</aside>
<aside class="footnote brackets" id="footnotestimated" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">9</a><span class="fn-bracket">]</span></span>
<p>The estimates were obtained by running
<strong class="program">mixtools</strong><code class="code docutils literal notranslate"><span class="pre">::</span></code><strong class="command">normalmixEM</strong>
in R (expectation-maximisation for mixtures of univariate normals;
<span id="id28">[<a class="reference internal" href="999-bibliography.html#id165" title="Benaglia, T., Chauveau, D., Hunter, D.R., and Young, D.S. (2009).  Mixtools: An R package for analyzing mixture models. Journal of Statistical Software, 32(6):1–29. DOI: 10.18637/jss.v032.i06.">7</a>]</span>).
Note that to turn <code class="docutils literal notranslate"><span class="pre">peds</span></code>, which is a table of counts,
to a real-valued sample, we had to call
<strong class="command">numpy.repeat</strong><code class="code docutils literal notranslate"><span class="pre">(</span></code><strong class="command">numpy.arange</strong><code class="code docutils literal notranslate"><span class="pre">(24)+0.5,</span> <span class="pre">peds)</span></code>.</p>
</aside>
<aside class="footnote brackets" id="footfts" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">10</a><span class="fn-bracket">]</span></span>
<p>Compare the Fundamental Theorem of Statistics
(the Glivenko–Cantelli theorem).</p>
</aside>
</aside>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="310-matrix.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title"><span class="section-number">7. </span>From uni- to multidimensional numeric data</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="220-transform-vector.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title"><span class="section-number">5. </span>Processing unidimensional data</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
              
              
              Copyright &#169; 2022–2025 by <a href="https://www.gagolewski.com/">Marek Gagolewski</a>.
              Some rights reserved. Licensed under <a href='https://creativecommons.org/licenses/by-nc-nd/4.0'>CC BY-NC-ND 4.0</a>.
              Built with <a href="https://sphinx-doc.org/">Sphinx</a>
              and a customised <a href="https://github.com/pradyunsg/furo">Furo</a> theme.
              Last updated on 2025-05-05T15:01:56+0200.
              This site will never display any ads: it is a non-profit project.
              It does not collect any data.
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            In this chapter
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">6. Continuous probability distributions</a><ul>
<li><a class="reference internal" href="#normal-distribution">6.1. Normal distribution</a><ul>
<li><a class="reference internal" href="#estimating-parameters">6.1.1. Estimating parameters</a></li>
<li><a class="reference internal" href="#data-models-are-useful">6.1.2. Data models are useful</a></li>
</ul>
</li>
<li><a class="reference internal" href="#assessing-goodness-of-fit">6.2. Assessing goodness-of-fit</a><ul>
<li><a class="reference internal" href="#comparing-cumulative-distribution-functions">6.2.1. Comparing cumulative distribution functions</a></li>
<li><a class="reference internal" href="#comparing-quantiles">6.2.2. Comparing quantiles</a></li>
<li><a class="reference internal" href="#kolmogorovsmirnov-test">6.2.3. Kolmogorov–Smirnov test (*)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#other-noteworthy-distributions">6.3. Other noteworthy distributions</a><ul>
<li><a class="reference internal" href="#log-normal-distribution">6.3.1. Log-normal distribution</a></li>
<li><a class="reference internal" href="#pareto-distribution">6.3.2. Pareto distribution</a></li>
<li><a class="reference internal" href="#uniform-distribution">6.3.3. Uniform distribution</a></li>
<li><a class="reference internal" href="#distribution-mixtures">6.3.4. Distribution mixtures (*)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#generating-pseudorandom-numbers">6.4. Generating pseudorandom numbers</a><ul>
<li><a class="reference internal" href="#id14">6.4.1. Uniform distribution</a></li>
<li><a class="reference internal" href="#not-exactly-random">6.4.2. Not exactly random</a></li>
<li><a class="reference internal" href="#sampling-from-other-distributions">6.4.3. Sampling from other distributions</a></li>
<li><a class="reference internal" href="#natural-variability">6.4.4. Natural variability</a></li>
<li><a class="reference internal" href="#adding-jitter-white-noise">6.4.5. Adding jitter (white noise)</a></li>
<li><a class="reference internal" href="#independence-assumption">6.4.6. Independence assumption</a></li>
</ul>
</li>
<li><a class="reference internal" href="#further-reading">6.5. Further reading</a></li>
<li><a class="reference internal" href="#exercises">6.6. Exercises</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=ff40c50f"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=4e2eecee"></script>
    <script src="../_static/proof.js"></script>
    <script src="../_static/katex.min.js?v=be8ff15f"></script>
    <script src="../_static/auto-render.min.js?v=ad136472"></script>
    <script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
    </body>
</html>