<!DOCTYPE html>


<html class="writer-html5" lang="en" >
<!-- Copyright (C) 2020-2022, Marek Gagolewski <https://www.gagolewski.com> -->

<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>6. Continuous Probability Distributions &mdash; Minimalist Data Wrangling with Python by Marek Gagolewski</title>
  

  
  
  <script async defer src="https://buttons.github.io/buttons.js"></script>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/proof.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />

  
  

  
  

  
    <link rel="canonical" href="https://datawranglingpy.gagolewski.com/chapter/230-distribution.html" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/proof.js"></script>
        <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="7. Multidimensional Numeric Data at a Glance" href="310-matrix.html" />
    <link rel="prev" title="5. Processing Unidimensional Data" href="220-transform-vector.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html"> Minimalist Data Wrangling with Python [DRAFTv0.4.2]
          

          
          </a>

          <div class="version">
          by <a style="color: inherit" href="https://www.gagolewski.com">Marek Gagolewski</a>
          </div>

<!--
          
            
            
              <div class="version">
                by Marek Gagolewski
              </div>
            
          
-->

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search phrase..." />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Start Here</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="000-preface.html">Preface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introducing Python</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="110-setup.html">1. Getting Started with Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="120-scalar.html">2. Scalar Types and Control Structures in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="130-sequential.html">3. Sequential and Other Types in Python</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Unidimensional Data</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="210-vector.html">4. Unidimensional Numeric Data and Their Distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="220-transform-vector.html">5. Processing Unidimensional Data</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">6. Continuous Probability Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#normal-distribution">6.1. Normal Distribution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#estimating-parameters">6.1.1. Estimating Parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-models-are-useful">6.1.2. Data Models Are Useful</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#assessing-goodness-of-fit">6.2. Assessing Goodness of Fit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#comparing-cumulative-distribution-functions">6.2.1. Comparing Cumulative Distribution Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#kolmogorovsmirnov-test">6.2.2. 🚧 Kolmogorov–Smirnov Test (*)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#comparing-quantiles">6.2.3. Comparing Quantiles</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#other-noteworthy-distributions">6.3. Other Noteworthy Distributions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#log-normal-distribution">6.3.1. Log-normal Distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pareto-distribution">6.3.2. Pareto Distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#uniform-distribution">6.3.3. Uniform Distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#distribution-mixtures">6.3.4. Distribution Mixtures (*)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#generating-pseudorandom-numbers">6.4. Generating Pseudorandom Numbers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id7">6.4.1. Uniform Distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#not-exactly-random">6.4.2. Not Exactly Random</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sampling-from-other-distributions">6.4.3. Sampling from Other Distributions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#natural-variability">6.4.4. Natural Variability</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#exercises">6.5. Exercises</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multidimensional Data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="310-matrix.html">7. Multidimensional Numeric Data at a Glance</a></li>
<li class="toctree-l1"><a class="reference internal" href="320-transform-matrix.html">8. Processing Multidimensional Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="330-relationship.html">9. Exploring Relationships Between Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Heterogeneous Data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="410-data-frame.html">10. Introducing Data Frames</a></li>
<li class="toctree-l1"><a class="reference internal" href="420-categorical.html">11. Handling Categorical Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="430-group-by.html">12. Processing Data in Groups</a></li>
<li class="toctree-l1"><a class="reference internal" href="440-sql.html">13. Accessing Databases</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Types of Data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="510-text.html">14. Text Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="520-missingness.html">15. Missing, Censored, and Questionable Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="530-time-series.html">16. Time Series</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="998-changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="999-bibliography.html">Bibliography</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Links</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://datawranglingpy.gagolewski.com/datawranglingpy.pdf">This Book in PDF</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/gagolews/datawranglingpy/blob/master/CODE_OF_CONDUCT.md">Report Bugs or Typos</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/gagolews/teaching_data">Datasets</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.gagolewski.com">Author</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Minimalist Data Wrangling with Python [DRAFTv0.4.2]</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          



















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li><span class="section-number">6. </span>Continuous Probability Distributions</li>
    
    
      <li class="wy-breadcrumbs-aside">

        
        
        <a class="github-button" href="https://github.com/gagolews/datawranglingpy" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star gagolews/datawranglingpy on GitHub">Star</a>
        


        
      </li>
    
  </ul>

  
  <div class="rst-breadcrumbs-buttons" role="navigation" aria-label="breadcrumb navigation">
      
        <a href="310-matrix.html" class="btn btn-neutral float-right" title="7. Multidimensional Numeric Data at a Glance" accesskey="n">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
      
      
        <a href="220-transform-vector.html" class="btn btn-neutral float-left" title="5. Processing Unidimensional Data" accesskey="p"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
      
  </div>
  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="tex2jax_ignore mathjax_ignore section" id="continuous-probability-distributions">
<span id="chap-distribution"></span><h1><span class="section-number">6. </span>Continuous Probability Distributions<a class="headerlink" href="#continuous-probability-distributions" title="Permalink to this headline"></a></h1>
<blockquote>
<div><p><em>This is an early draft of</em> Minimalist Data Wrangling with Python <em>by
<a class="reference external" href="https://www.gagolewski.com">Marek Gagolewski</a>. It’s distributed
in the hope that it’ll be useful. Any
<a class="reference external" href="https://github.com/gagolews/datawranglingpy/blob/master/CODE_OF_CONDUCT.md">bug/typos reports/fixes</a>
are appreciated. Although available online, this is a whole course,
and should be read from the beginning to the end. In particular,
refer to the <a class="reference internal" href="000-preface.html#chap-preface"><span class="std std-ref">Preface</span></a> for general introductory remarks.</em></p>
</div></blockquote>
<p>Each successful data analyst will deal with hundreds or thousands of
datasets in their lifetime. In the long run, at some level, most of them will
be deemed <em>boring</em>. This is because a few common patterns will be occurring
over and over again.</p>
<p>In particular, the previously mentioned bell-shapedness and right-skewness
is quite prevalent in nature. But, surprisingly, this is exactly when things
become scientific and interesting – allowing us to study various phenomena
at an appropriate level of generality.</p>
<p>Mathematically, such idealised patterns in the histogram shapes
can be formalised using the notion of a
<em>probability distribution of a continuous, real-valued random variable</em>,
and more precisely in this case, a <em>probability density function</em> of
the corresponding random variable.</p>
<p>Intuitively<a class="footnote-reference brackets" href="#foothistconvergence" id="id1">1</a>, a <em>probability density function</em> is
a nicely smooth curve that would arise if we drew a histogram
for the whole <em>population</em> (e.g., all women living currently on Earth and
beyond or otherwise an extremely large data sample obtained by independently
querying the same underlying data generating process) in such a way that
the total area of all the bars are equal to 1
and the the bin sizes being very small.</p>
<p>As stated at the beginning, we do not intend this to be a course in
probability theory and mathematical statistics,
but a one that precedes and motivates them
(e.g., <span id="id2">[<a class="reference internal" href="999-bibliography.html#id39">Bil95</a>, <a class="reference internal" href="999-bibliography.html#id40">DKLM05</a>, <a class="reference internal" href="999-bibliography.html#id43">Gen09</a>, <a class="reference internal" href="999-bibliography.html#id38">Gen20</a>]</span>), therefore
our definitions are out of necessity simplified so that they are digestible.
Hence, for the purpose of our illustrations, let us consider
the following characterisation.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>(*) We call an integrable function <span class="math notranslate nohighlight">\(f:\mathbb{R}\to\mathbb{R}\)</span>
a <em>probability density function</em> (PDF)
if <span class="math notranslate nohighlight">\(f(x)\ge 0\)</span> for all <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(\int_{-\infty}^\infty f(x)\,dx=1\)</span>,
i.e., it is nonnegative and normalised in such a way that
the total area under the whole curve is 1.</p>
<p>For any <span class="math notranslate nohighlight">\(a &lt; b\)</span>, we treat the area under the fragment of
the <span class="math notranslate nohighlight">\(f(x)\)</span> curve for <span class="math notranslate nohighlight">\(x\)</span> between <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, i.e., <span class="math notranslate nohighlight">\(\int_a^b f(x)\,dx\)</span>,
as the probability of the underlying real-valued
random variable (theoretical data generating process)
falling into the <span class="math notranslate nohighlight">\([a, b]\)</span> interval.</p>
</div>
<p>Some distributions appear more frequently than others
and fit empirical data or parts thereof particularly well
(or there is some established wishful thinking/delusion that
they serve as their good-enough approximations;
compare <span id="id3">[<a class="reference internal" href="999-bibliography.html#id75">FEHP10</a>]</span>).
Let us thus review a few noteworthy probability distributions:
the normal, log-normal, Pareto, and uniform families
(in <a class="reference internal" href="530-time-series.html#sec-exp-distrib"><span class="std std-numref">Section 16.2.4</span></a> we also mention the exponential one).</p>
<div class="section" id="normal-distribution">
<h2><span class="section-number">6.1. </span>Normal Distribution<a class="headerlink" href="#normal-distribution" title="Permalink to this headline"></a></h2>
<p>A <em>normal distribution</em> is the one that has a prototypical,
nicely symmetric, bell-shaped density.
It is described by two parameters:
<span class="math notranslate nohighlight">\(\mu\in\mathbb{R}\)</span> (the expected value, at which the density is centred)
and <span class="math notranslate nohighlight">\(\sigma&gt;0\)</span> (the standard deviation, saying how
much the distribution is dispersed around μ).</p>
<p>The probability density function of N(μ, σ) is given by:</p>
<div class="math notranslate nohighlight">
\[
f(x) = \frac{1}{\sqrt{2\pi \sigma^2}}
\exp\left( -\frac{(x-\mu)^2}{2\sigma^2} \right).
\]</div>
<div class="section" id="estimating-parameters">
<h3><span class="section-number">6.1.1. </span>Estimating Parameters<a class="headerlink" href="#estimating-parameters" title="Permalink to this headline"></a></h3>
<p>A course in statistics (which, again, this one is not,
we are merely making an illustration here), may tell us
that the sample arithmetic mean and standard deviation are natural,
statistically well-behaving <em>estimators</em> of the said parameters:
if all samples would really be drawn independently from N(μ, σ) each,
the we <em>expect</em> the mean and standard deviation be equal
to, more or less, μ and σ (the larger the sample size,
the smaller the error).</p>
<p>Recall the <code class="docutils literal notranslate"><span class="pre">heights</span></code> (females from the NHANES study)
dataset and its bell-shaped histogram
in <a class="reference internal" href="210-vector.html#fig-heights-histogram-bins11"><span class="std std-numref">Figure 4.2</span></a>.
Let’s estimate these parameters for this sample:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">heights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching_data/master/marek/nhanes_adult_female_height_2020.txt&quot;</span><span class="p">)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">heights</span><span class="p">)</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span>
<span class="c1">## (160.13679222932953, 7.062858532891359)</span>
</pre></div>
</div>
<p>Mathematically, we will denote these two with
<span class="math notranslate nohighlight">\(\hat{\mu}\)</span> and <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span> (mu and sigma with a hat) to emphasise that
they are merely guesstimates of the unknown respective parameters
<span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>. On a side note, we use <code class="docutils literal notranslate"><span class="pre">ddof=1</span></code> in the latter case,
because this estimator has slightly better
statistical properties.</p>
<p>Let’s draw the fitted density function, i.e., the PDF of N(160.1, 7.06),
on top of the histogram, see <a class="reference internal" href="#fig-heights-normal"><span class="std std-numref">Figure 6.1</span></a>.
We pass <code class="docutils literal notranslate"><span class="pre">stat=&quot;density&quot;</span></code> to <strong class="command">seaborn.histplot</strong>
so that the histogram bars are normalised
(i.e., the total area of these rectangles sums to 1).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">heights</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">heights</span><span class="p">),</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">),</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;PDF of N(</span><span class="si">{</span><span class="n">mu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id11">
<span id="fig-heights-normal"></span><img alt="../_images/heights-normal-1.png" src="../_images/heights-normal-1.png" />
<p class="caption"><span class="caption-number">Figure 6.1 </span><span class="caption-text">A histogram and the probability density function of the fitted normal distribution for the <code class="docutils literal notranslate"><span class="pre">heights</span></code> dataset</span><a class="headerlink" href="#id11" title="Permalink to this image"></a></p>
</div>
<p>At a first glance, this is a very nice match.
Before proceeding with some ways of assessing the goodness of fit slightly
more rigorously, let us praise the potential benefits of having
an idealised <em>model</em> of our dataset at our disposal.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>(*) It might be the case that we will have to obtain
the estimates of the probability distribution’s parameters
by numerical optimisation, for example, by minimising</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\mu, \sigma) = \sum_{i=1}^n \left(
    \frac{(x_i-\mu)^2}{\sigma^2} + \log \sigma^2
\right)
\]</div>
<p>with respect to <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> (corresponding to the objective
function in maximum likelihood estimation problem for the normal
distribution family).  In our case, however, we are lucky;
there exist open-form formulae expressing the solution to the
above in the form of the aforementioned sample mean and standard deviation.
For other distributions things can get a little trickier, though.</p>
<p>Sometimes, we will have many options for point estimators to choose from,
which might be more suitable if data aren’t of top quality
(e.g., contain outliers). For instance, in the normal model,
it can be shown that                                                                                                                                       we can also estimate <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> via
the sample median and <span class="math notranslate nohighlight">\(\mathrm{IQR}/1.349\)</span>.</p>
</div>
</div>
<div class="section" id="data-models-are-useful">
<h3><span class="section-number">6.1.2. </span>Data Models Are Useful<a class="headerlink" href="#data-models-are-useful" title="Permalink to this headline"></a></h3>
<p><em>If</em> (provided that, assuming that, on condition that)
our sample is a realisation of independent random variables
following a given distribution,
or a data analyst judges that such an approximation might be
justified, then we have a set of many numbers <em>reduced</em> to merely few
parameters.</p>
<p>In the above case, we might want to risk the statement
that data follow the normal distribution (assumption 1)
with parameters <span class="math notranslate nohighlight">\(\mu=160.1\)</span> and <span class="math notranslate nohighlight">\(\sigma=7.06\)</span> (assumption 2).
Note that the choice of the distribution family is one thing,
and the way we estimate the underlying parameters
(in our case, we use <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> and <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span>)
is another.</p>
<p>This not only saves storage space and computational time, but also – based
on what we can learn from a course in probability and statistics
(by appropriately integrating the density function) –
we can imply facts such as for normally distributed data:</p>
<ul class="simple">
<li><p>ca. 95% of (i.e., <em>most</em>) women are
<span class="math notranslate nohighlight">\(\mu\pm 2\sigma\)</span> tall (the 2σ rule),</p></li>
<li><p>ca. 99.7% of (i.e., <em>almost all</em>) women are
<span class="math notranslate nohighlight">\(\mu\pm 3\sigma\)</span> tall (the 3σ rule).</p></li>
</ul>
<p>Also, if we knew that the distribution of heights of men
is also normal with some other parameters, we could be able to
make some comparisons between the two samples.
For example, what is the probability that a woman
randomly selected from the crowd is taller than a male passerby.</p>
<p>Furthermore, there is a range of <em>parametric</em> (assuming some distribution
family) of statistical methods that cannot be used if we don’t assume
the data normality, e.g., the <em>t</em>-test to compare the expected values.
So many options.</p>
<div class="proof proof-type-exercise" id="id12">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.1</span>
        
    </div><div class="proof-content">
<p>How different manufacturing industries can make use of such knowledge?
Are simplifications necessary when dealing with complexity?
What are the alternatives?</p>
</div></div></div>
</div>
<div class="section" id="assessing-goodness-of-fit">
<h2><span class="section-number">6.2. </span>Assessing Goodness of Fit<a class="headerlink" href="#assessing-goodness-of-fit" title="Permalink to this headline"></a></h2>
<div class="section" id="comparing-cumulative-distribution-functions">
<h3><span class="section-number">6.2.1. </span>Comparing Cumulative Distribution Functions<a class="headerlink" href="#comparing-cumulative-distribution-functions" title="Permalink to this headline"></a></h3>
<p>A better way of assessing the extent to which
a sample deviates from a hypothesised distribution,
is by comparing the theoretical <em>cumulative distribution function</em> (CDF)
and the empirical one (<span class="math notranslate nohighlight">\(\hat{F}_n\)</span>, see <a class="reference internal" href="210-vector.html#sec-ecdf"><span class="std std-numref">Section 4.3.8</span></a>).</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If <span class="math notranslate nohighlight">\(f\)</span> is a density function, then the corresponding
theoretical CDF is defined as <span class="math notranslate nohighlight">\(F(x) = \int_{-\infty}^x f(t)\,dt\)</span>,
i.e., the probability of the underlying random variable’s
taking a value not greater than <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>By definition<a class="footnote-reference brackets" href="#footcdf" id="id4">2</a>, each CDF takes values in the unit interval (<span class="math notranslate nohighlight">\([0,1]\)</span>)
and is nondecreasing.</p>
</div>
<p>For the normal distribution family,
the values of the theoretical CDF can be computed by calling <strong class="command">scipy.stats.norm.cdf</strong>;
see <a class="reference internal" href="#fig-heights-cdf"><span class="std std-numref">Figure 6.2</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">heights</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">heights</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">heights</span><span class="p">),</span> <span class="mi">1001</span><span class="p">)</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>  <span class="c1"># sample the CDF at many points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;CDF of N(</span><span class="si">{</span><span class="n">mu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">heights_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">heights</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">heights_sorted</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">,</span>
    <span class="n">drawstyle</span><span class="o">=</span><span class="s2">&quot;steps-post&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Empirical CDF&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Prob(height $</span><span class="se">\\</span><span class="s2">leq$ x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id13">
<span id="fig-heights-cdf"></span><img alt="../_images/heights-cdf-3.png" src="../_images/heights-cdf-3.png" />
<p class="caption"><span class="caption-number">Figure 6.2 </span><span class="caption-text">The empirical CDF and the fitted normal CDF for the <code class="docutils literal notranslate"><span class="pre">heights</span></code> dataset</span><a class="headerlink" href="#id13" title="Permalink to this image"></a></p>
</div>
<p>This <em>looks</em> like a superb match.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><span class="math notranslate nohighlight">\(F(b)-F(a)=\int_{a}^b f(t)\,dt\)</span> is the probability
of generating a value in the interval <span class="math notranslate nohighlight">\([a, b]\)</span>.</p>
</div>
<p>Let us empirically verify the aforementioned 3σ rule:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">F</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span>
<span class="n">F</span><span class="p">(</span><span class="n">mu</span><span class="o">+</span><span class="mi">3</span><span class="o">*</span><span class="n">sigma</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> <span class="o">-</span> <span class="n">F</span><span class="p">(</span><span class="n">mu</span><span class="o">-</span><span class="mi">3</span><span class="o">*</span><span class="n">sigma</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
<span class="c1">## 0.9973002039367398</span>
</pre></div>
</div>
<p>Indeed, almost all observations are within
<span class="math notranslate nohighlight">\([\mu-3\sigma, \mu+3\sigma]\)</span>, if data are normally distributed.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A common way to summarise the discrepancy between
the empirical and a given theoretical CDF is by computing
the greatest absolute deviation between these two functions</p>
<div class="math notranslate nohighlight">
\[
D_n = \sup_{t\in\mathbb{R}} | \hat{F}_n(t) - F(t) |.
\]</div>
<p>Note that the supremum is a continuous version of the maximum.</p>
</div>
<p>It holds:</p>
<div class="math notranslate nohighlight">
\[
D_n = \max_{k=1,\dots,n} \left\{
    \max\left\{ \left|\tfrac{k-1}{n} - F(x_{(k)})\right| \right\},
    \max\left\{ \left|\tfrac{k}{n} - F(x_{(k)})\right| \right\}
\right\},
\]</div>
<p>i.e., <span class="math notranslate nohighlight">\(F\)</span> needs to be probed only at the <span class="math notranslate nohighlight">\(n\)</span> points from the sorted
input sample.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Fx</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">heights_sorted</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 1, 2, ..., n</span>
<span class="n">Dn1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">n</span> <span class="o">-</span> <span class="n">Fx</span><span class="p">))</span>
<span class="n">Dn2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">k</span><span class="o">/</span><span class="n">n</span> <span class="o">-</span> <span class="n">Fx</span><span class="p">))</span>
<span class="n">Dn</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">Dn1</span><span class="p">,</span> <span class="n">Dn2</span><span class="p">)</span>
<span class="n">Dn</span>
<span class="c1">## 0.010470976524201148</span>
</pre></div>
</div>
<p>If the difference is <em>sufficiently small</em> (the larger the sample
size, the less tolerant we should be with regards to the
size of this disparity), then we can assume that a normal model
describes data quite well.
This is indeed the case here: we may estimate the probability
of someone being as tall as any given height
with error less than about 1%.</p>
</div>
<div class="section" id="kolmogorovsmirnov-test">
<span id="sec-ks-test"></span><h3><span class="section-number">6.2.2. </span>🚧 Kolmogorov–Smirnov Test (*)<a class="headerlink" href="#kolmogorovsmirnov-test" title="Permalink to this headline"></a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This part is under construction. Please come back later.</p>
</div>
<p>More precisely, the popular goodness-of-fit test by
Kolmogorov and Smirnov can give us a conservative tolerance margin
for the concrete value of <span class="math notranslate nohighlight">\(D_n\)</span> as a function of <span class="math notranslate nohighlight">\(n\)</span>
within the framework of frequentist hypotheses testing.</p>
</div>
<div class="section" id="comparing-quantiles">
<span id="sec-qqplot"></span><h3><span class="section-number">6.2.3. </span>Comparing Quantiles<a class="headerlink" href="#comparing-quantiles" title="Permalink to this headline"></a></h3>
<p>A Q-Q (quantile-quantile) plot is another graphical method
for comparing two distributions.
This time, instead of working with a cumulative distribution function <span class="math notranslate nohighlight">\(F\)</span>,
we will be dealing with their (generalised) inverses, i.e.,
quantile functions.</p>
<p>Given a CDF <span class="math notranslate nohighlight">\(F\)</span>, the corresponding <em>quantile function</em> is defined
for any <span class="math notranslate nohighlight">\(p\in(0,1)\)</span> as</p>
<div class="math notranslate nohighlight">
\[
Q(p) = \inf\{ x: F(x) \ge p \},
\]</div>
<p>i.e., the smallest <span class="math notranslate nohighlight">\(x\)</span> such that the probability of drawing
a value not greater than <span class="math notranslate nohighlight">\(x\)</span> is at least <span class="math notranslate nohighlight">\(p\)</span>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If a CDF <span class="math notranslate nohighlight">\(F\)</span> is continuous, and this is the assumption in the current chapter,
then <span class="math notranslate nohighlight">\(Q\)</span> is exactly its inverse, i.e., it holds <span class="math notranslate nohighlight">\(Q(p)=F^{-1}(p)\)</span> for
all <span class="math notranslate nohighlight">\(p\in(0, 1)\)</span>.</p>
</div>
<p>The theoretical quantiles can be generated
by the <strong class="command">scipy.stats.norm.ppf</strong> function.
Here, <em>ppf</em> stands for the percent point function which is another
(yet quite esoteric) name for the above <span class="math notranslate nohighlight">\(Q\)</span>.</p>
<p>For instance, given our N(160.1, 7.06)-distributed NHANES dataset,
<span class="math notranslate nohighlight">\(Q(0.9)\)</span> is the height not exceeded by 90% of female population.
In other words, only 10% of American women are taller than:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
<span class="c1">## 169.18820963937648</span>
</pre></div>
</div>
<div style="margin-top: 1em"></div><p>The sample quantiles that we have introduced
in <a class="reference internal" href="220-transform-vector.html#sec-quantiles"><span class="std std-numref">Section 5.1.1.3</span></a> are natural estimators
of the theoretical quantile function.
However, we have also mentioned that there are quite a few possible
definitions thereof in the literature, compare <span id="id5">[<a class="reference internal" href="999-bibliography.html#id7">HF96</a>]</span>.</p>
<p>A Q-Q plot draws a version of sample quantiles
as a function of the corresponding theoretical quantiles.
For simplicity, instead of using the <strong class="command">numpy.quantile</strong> function,
we will assume that the <span class="math notranslate nohighlight">\(\frac{i}{n+1}\)</span>-quantile
is equal to <span class="math notranslate nohighlight">\(x_{(i)}\)</span>, i.e., the <em>i</em>-th smallest value in
a given sample <span class="math notranslate nohighlight">\((x_1,x_2,\dots,x_n)\)</span>
and consider only <span class="math notranslate nohighlight">\(i=1, 2, \dots, n\)</span>.</p>
<p>Our simplified setting successfully avoids the problem which arises
when the 0- or 1-quantile of the theoretical distribution,
i.e., <span class="math notranslate nohighlight">\(Q(0)\)</span> and <span class="math notranslate nohighlight">\(Q(1)\)</span> is infinite (and this is the case for
the normal distribution family).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">heights</span><span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 1/(n+1), 2/(n+2), ..., n/(n+1)</span>
<span class="n">heights_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">heights</span><span class="p">)</span>          <span class="c1"># theoretical quantiles</span>
<span class="n">quantiles</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>  <span class="c1"># sample quantiles</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">quantiles</span><span class="p">,</span> <span class="n">heights_sorted</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axline</span><span class="p">((</span><span class="n">heights_sorted</span><span class="p">[</span><span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="n">heights_sorted</span><span class="p">[</span><span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">]),</span> <span class="n">slope</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>  <span class="c1"># identity line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantiles of N(</span><span class="si">{</span><span class="n">mu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Sample quantiles&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id14">
<span id="fig-qq-heights"></span><img alt="../_images/qq-heights-5.png" src="../_images/qq-heights-5.png" />
<p class="caption"><span class="caption-number">Figure 6.3 </span><span class="caption-text">Q-Q plot for the <code class="docutils literal notranslate"><span class="pre">heights</span></code> dataset</span><a class="headerlink" href="#id14" title="Permalink to this image"></a></p>
</div>
<p><a class="reference internal" href="#fig-qq-heights"><span class="std std-numref">Figure 6.3</span></a> depicts the Q-Q plot for our example dataset.</p>
<p>Ideally, the points should be arranged on the <span class="math notranslate nohighlight">\(y=x\)</span> line
(which was added for readability). This happens if
the sample quantiles match the theoretical ones perfectly.</p>
<p>In our case, there are small discrepancies in the tails
(e.g., the smallest observation was slightly smaller than expected,
and the largest one was larger than expected),
although it is quite <em>normal</em> a behaviour for small samples
and certain distribution families</p>
<p>Overall, we can say that this is a very good fit.</p>
</div>
</div>
<div class="section" id="other-noteworthy-distributions">
<h2><span class="section-number">6.3. </span>Other Noteworthy Distributions<a class="headerlink" href="#other-noteworthy-distributions" title="Permalink to this headline"></a></h2>
<div class="section" id="log-normal-distribution">
<h3><span class="section-number">6.3.1. </span>Log-normal Distribution<a class="headerlink" href="#log-normal-distribution" title="Permalink to this headline"></a></h3>
<p>We say that a sample is <em>log-normally distributed</em>,
if its logarithm is normally distributed.</p>
<p>In particular, it is sometimes observed that the income of most individuals
(except the richest) is distributed, at least approximately, log-normally.
Let us investigate whether this is the case for the UK taxpayers.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">income</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching_data/master/marek/uk_income_simulated_2020.txt&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The plotting of the histogram for the logarithm of
income is left as an exercise (we can pass <code class="docutils literal notranslate"><span class="pre">log_scale=True</span></code>
to <strong class="command">seaborn.histplot</strong>; we’ll plot it soon anyway
in a different way).
We proceed directly with the
fitting of a log-normal model, LN(μ, σ).
The fitting process is similar to the normal case, but this time
we determine the mean and standard deviation based on the logarithms of data,
which we can compute using a vectorised mathematical function
discussed in <a class="reference internal" href="220-transform-vector.html#sec-vecfun"><span class="std std-numref">Section 5.2</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lmu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">income</span><span class="p">))</span>
<span class="n">lsigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">lmu</span><span class="p">,</span> <span class="n">lsigma</span>
<span class="c1">## (10.314409794364623, 0.5816585197803816)</span>
</pre></div>
</div>
<p><a class="reference internal" href="#fig-heights-lognormal"><span class="std std-numref">Figure 6.4</span></a> depicts the fitted probability density
function  together with the histograms on the log- and original scale.
When creating this plot, there are two pitfalls, though.
Firstly, <strong class="command">scipy.stats.lognorm</strong>
parametrises the distribution via the parameter <span class="math notranslate nohighlight">\(s\)</span> equal to <span class="math notranslate nohighlight">\(\sigma\)</span>
and scale equal to <span class="math notranslate nohighlight">\(e^\mu\)</span>, hence computing the PDF at different points
must be done as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="mi">101</span><span class="p">)</span>
<span class="n">fx</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">lognorm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">lsigma</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">lmu</span><span class="p">))</span>
</pre></div>
</div>
<p>Second, passing both <code class="docutils literal notranslate"><span class="pre">log_scale=True</span></code> and <code class="docutils literal notranslate"><span class="pre">stat=&quot;density&quot;</span></code>
to <strong class="command">seaborn.histplot</strong> does not normalise the values on the
y-axis correctly. In order to make the histogram on the log-scale
comparable with the true density, we need to turn the log-scale
manually and pass manually generated bins that are equidistant
on the logarithmic scale (via <strong class="command">numpy.geomspace</strong>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">geomspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="mi">30</span><span class="p">)</span>
</pre></div>
</div>
<p>And now:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">income</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">)</span>  <span class="c1"># own bins!</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">fx</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">income</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">fx</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;PDF of LN(</span><span class="si">{</span><span class="n">lmu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">lsigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id15">
<span id="fig-heights-lognormal"></span><img alt="../_images/heights-lognormal-7.png" src="../_images/heights-lognormal-7.png" />
<p class="caption"><span class="caption-number">Figure 6.4 </span><span class="caption-text">A histogram and the probability density function of the fitted log-normal distribution for the <code class="docutils literal notranslate"><span class="pre">income</span></code> dataset, on log- (left) and original (right) scale</span><a class="headerlink" href="#id15" title="Permalink to this image"></a></p>
</div>
<p>Overall, this fit is not too bad.
Note that we deal with only a sample of 1000 households here;
the original UK Office of National Statistics <a class="reference external" href="https://www.ons.gov.uk/peoplepopulationandcommunity/personalandhouseholdfinances/incomeandwealth/bulletins/householddisposableincomeandinequality/financialyear2020">data</a>
could tell us more about the quality of this model in general,
but it’s beyond the scope of our simple exercise.</p>
<div class="proof proof-type-exercise" id="id16">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.2</span>
        
    </div><div class="proof-content">
<p>Compare the empirical CDF for <code class="docutils literal notranslate"><span class="pre">income</span></code>
and the theoretical CDF of LN(10.3, 0.58).</p>
</div></div><p>Furthermore, <a class="reference internal" href="#fig-qq-income"><span class="std std-numref">Figure 6.5</span></a> gives the quantile-quantile plot on a
double-logarithmic scale for the above log-normal model.
Additionally, we (empirically) verify the hypothesis
of normality (using a “normal” normal distribution, not it’s “log”
version).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">income</span><span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="n">income_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">income</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">quantiles</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">lognorm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">lsigma</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">lmu</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">quantiles</span><span class="p">,</span> <span class="n">income_sorted</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axline</span><span class="p">((</span><span class="n">income_sorted</span><span class="p">[</span><span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="n">income_sorted</span><span class="p">[</span><span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">]),</span> <span class="n">slope</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantiles of LN(</span><span class="si">{</span><span class="n">lmu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">lsigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Sample quantiles&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">income</span><span class="p">)</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">income</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">quantiles2</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">quantiles2</span><span class="p">,</span> <span class="n">income_sorted</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axline</span><span class="p">((</span><span class="n">income_sorted</span><span class="p">[</span><span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="n">income_sorted</span><span class="p">[</span><span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">]),</span> <span class="n">slope</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantiles of N(</span><span class="si">{</span><span class="n">mu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id17">
<span id="fig-qq-income"></span><img alt="../_images/qq-income-9.png" src="../_images/qq-income-9.png" />
<p class="caption"><span class="caption-number">Figure 6.5 </span><span class="caption-text">Q-Q plots for the <code class="docutils literal notranslate"><span class="pre">income</span></code> dataset vs a fitted log-normal (good fit; left) and normal (bad fit; right) distribution</span><a class="headerlink" href="#id17" title="Permalink to this image"></a></p>
</div>
<p>We see that definitely the hypothesis that our data follow
a normal distribution (the right subplot) is most likely false.</p>
<p>The log-normal  model (the left subplot), on the other hand,
might be quite usable.
It again reduced the whole dataset to merely two numbers,
μ and σ, based on which (and probability theory),
we may deduce that:</p>
<ul class="simple">
<li><p>the expected average (mean) income is <span class="math notranslate nohighlight">\(e^{\mu + \sigma^2/2}\)</span>,</p></li>
<li><p>median is <span class="math notranslate nohighlight">\(e^\mu\)</span>,</p></li>
<li><p>most probable one (mode) in <span class="math notranslate nohighlight">\(e^{\mu-\sigma^2}\)</span>,</p></li>
</ul>
<p>etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Recall again that for skewed distributions such as this one,
reporting the mean might be misleading.
This is why <em>most</em> people get angry when they read the news
about the prospering economy – “yeah, I’d like to see that
kind of money in my pocket”. Hence, it’s not only μ that matters,
it’s also σ which quantifies the discrepancy between the rich and the poor
(too much inequality is bad, but also too much uniformity is
to be avoided).</p>
</div>
<p>Note that for a normal distribution the situation is very different,
because, the mean, median, and most probable
outcomes tend to be the same – the distribution is
symmetric around μ.</p>
<div class="proof proof-type-exercise" id="id18">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.3</span>
        
    </div><div class="proof-content">
<p>(*) What is the fraction of people with earnings
below the mean in our LN(10.3, 0.58) model?</p>
</div></div></div>
<div class="section" id="pareto-distribution">
<h3><span class="section-number">6.3.2. </span>Pareto Distribution<a class="headerlink" href="#pareto-distribution" title="Permalink to this headline"></a></h3>
<p>Consider again the <a class="reference external" href="https://arxiv.org/abs/0706.1062v2">dataset</a>
on the populations of the US cities in the 2000 US Census:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cities</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching_data/master/other/us_cities_2000.txt&quot;</span><span class="p">)</span>
<span class="nb">len</span><span class="p">(</span><span class="n">cities</span><span class="p">),</span> <span class="nb">sum</span><span class="p">(</span><span class="n">cities</span><span class="p">)</span>  <span class="c1"># number of cities, total population</span>
<span class="c1">## (19447, 175062893.0)</span>
</pre></div>
</div>
<p><a class="reference internal" href="#fig-hist-cities"><span class="std std-numref">Figure 6.6</span></a> gives the histogram
of the city sizes with the populations on the log-scale.
It kind-of looks like a log-normal distribution again,
which the reader can inspect themself when they are feeling playful.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">cities</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">log_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id19">
<span id="fig-hist-cities"></span><img alt="../_images/hist-cities-11.png" src="../_images/hist-cities-11.png" />
<p class="caption"><span class="caption-number">Figure 6.6 </span><span class="caption-text">Histogram for the unabridged <code class="docutils literal notranslate"><span class="pre">cities</span></code> dataset</span><a class="headerlink" href="#id19" title="Permalink to this image"></a></p>
</div>
<p>We, however, this time will be interested in not what’s <em>typical</em>,
but what’s in some sense <em>anomalous</em> or <em>extreme</em>.
Let’s take a look at the <em>truncated</em> version of the city size distribution
by considering the cities with 10,000 or more inhabitants – i.e.,
we will only study the right tail the original data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="mi">10_000</span>
<span class="n">large_cities</span> <span class="o">=</span> <span class="n">cities</span><span class="p">[</span><span class="n">cities</span> <span class="o">&gt;=</span> <span class="n">s</span><span class="p">]</span>
<span class="nb">len</span><span class="p">(</span><span class="n">large_cities</span><span class="p">),</span> <span class="nb">sum</span><span class="p">(</span><span class="n">large_cities</span><span class="p">)</span>  <span class="c1"># number of cities, total population</span>
<span class="c1">## (2696, 146199374.0)</span>
</pre></div>
</div>
<p>Plotting the above on a double-logarithmic scale
can be performed by passing <code class="docutils literal notranslate"><span class="pre">log_scale=(True,</span> <span class="pre">True)</span></code>
to <strong class="command">seaborn.histplot</strong>, which is left as an exercise.
Anyway, doing so will lead to a similar picture
as in <a class="reference internal" href="#fig-fit-pareto"><span class="std std-numref">Figure 6.7</span></a>.
This reveals something very interesting: the bar tops on the
double-log-scale are arranged more or less on a straight line.
There are many datasets
which exhibit this behaviour; we say that they follow a <em>power law</em>
(power in the arithmetic sense, not social one),
see <span id="id6">[<a class="reference internal" href="999-bibliography.html#id37">CSN09</a>, <a class="reference internal" href="999-bibliography.html#id36">New05</a>]</span> for discussion.</p>
<p>Let’s introduce the <em>Pareto distribution</em> family which has a prototypical
power law-like density. It is identified by two parameters:</p>
<ul class="simple">
<li><p>the (what <strong class="program">scipy</strong> calls it) scale parameter <span class="math notranslate nohighlight">\(s&gt;0\)</span> is equal to
the shift from 0,</p></li>
<li><p>the shape parameter, <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, controls the slope of the said line on
the double-log-scale.</p></li>
</ul>
<p>The probability density function of P(α, s) is given by:</p>
<div class="math notranslate nohighlight">
\[
f(x) = \frac{\alpha s^\alpha}{x^{\alpha+1}}
\]</div>
<p>for <span class="math notranslate nohighlight">\(x\ge s\)</span> and <span class="math notranslate nohighlight">\(f(x)=0\)</span> otherwise.</p>
<p><span class="math notranslate nohighlight">\(s\)</span> is usually taken as the sample minimum (i.e., 10000 in our case).
<span class="math notranslate nohighlight">\(\alpha\)</span> can be estimated through the reciprocal of
the mean of the scaled logarithms of our observations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">large_cities</span><span class="o">/</span><span class="n">s</span><span class="p">))</span>
<span class="n">alpha</span>
<span class="c1">## 0.9496171695997675</span>
</pre></div>
</div>
<p>Unfortunately, we have already noted that comparing the theoretical
densities and an empirical histogram on a log-scale is quite problematic
with <strong class="command">seaborn</strong>, therefore we again must apply
logarithmic binning manually in order to come up
with what we see in <a class="reference internal" href="#fig-fit-pareto"><span class="std std-numref">Figure 6.7</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">geomspace</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">large_cities</span><span class="p">),</span> <span class="mi">21</span><span class="p">)</span>  <span class="c1"># bins&#39; boundaries</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">large_cities</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pareto</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s</span><span class="p">),</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;PDF of P(</span><span class="si">{</span><span class="n">alpha</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id20">
<span id="fig-fit-pareto"></span><img alt="../_images/fit-pareto-13.png" src="../_images/fit-pareto-13.png" />
<p class="caption"><span class="caption-number">Figure 6.7 </span><span class="caption-text">Histogram for the <code class="docutils literal notranslate"><span class="pre">large_cities</span></code> dataset and the fitted density on a double log-scale</span><a class="headerlink" href="#id20" title="Permalink to this image"></a></p>
</div>
<p><a class="reference internal" href="#fig-qq-pareto"><span class="std std-numref">Figure 6.8</span></a> gives the corresponding Q-Q plot on
a double-logarithmic scale.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">large_cities</span><span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="n">cities_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">large_cities</span><span class="p">)</span>
<span class="n">quantiles</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pareto</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">quantiles</span><span class="p">,</span> <span class="n">cities_sorted</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axline</span><span class="p">((</span><span class="n">quantiles</span><span class="p">[</span><span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="n">cities_sorted</span><span class="p">[</span><span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">]),</span> <span class="n">slope</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantiles of P(</span><span class="si">{</span><span class="n">alpha</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Sample quantiles&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id21">
<span id="fig-qq-pareto"></span><img alt="../_images/qq-pareto-15.png" src="../_images/qq-pareto-15.png" />
<p class="caption"><span class="caption-number">Figure 6.8 </span><span class="caption-text">Q-Q plot for the <code class="docutils literal notranslate"><span class="pre">large_cites</span></code> dataset vs the fitted Paretian model</span><a class="headerlink" href="#id21" title="Permalink to this image"></a></p>
</div>
<p>We see that the populations of the largest cities are overestimated.
The model could be better, but the cities are still growing, right?</p>
<div class="proof proof-type-example" id="id22">

    <div class="proof-title">
        <span class="proof-type">Example 6.4</span>
        
    </div><div class="proof-content">
<p>(*) It might also be interesting to see how well can we predict the
probability of a randomly selected city being at least a given size.
Let us thus denote with <span class="math notranslate nohighlight">\(S(x)=1-F(x)\)</span> the <em>complementary
cumulative distribution function</em> (CCDF; sometimes referred to as
the survival function)
and with <span class="math notranslate nohighlight">\(\hat{S}_n(x)=1-\hat{F}_n(x)\)</span> its empirical version.
<a class="reference internal" href="#fig-ccdf-pareto"><span class="std std-numref">Figure 6.9</span></a> compares the empirical and the fitted CCDFs
with probabilities on the linear- and log-scale.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">geomspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">large_cities</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">large_cities</span><span class="p">),</span> <span class="mi">1001</span><span class="p">)</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pareto</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">probs</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;CCDF of P(</span><span class="si">{</span><span class="n">alpha</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cities_sorted</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">,</span>
        <span class="n">drawstyle</span><span class="o">=</span><span class="s2">&quot;steps-post&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Empirical Complementary CDF&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">([</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="s2">&quot;log&quot;</span><span class="p">][</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Prob(city size &gt; x)&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id23">
<span id="fig-ccdf-pareto"></span><img alt="../_images/ccdf-pareto-17.png" src="../_images/ccdf-pareto-17.png" />
<p class="caption"><span class="caption-number">Figure 6.9 </span><span class="caption-text">Empirical and theoretical complementary cumulative distribution functions for the <code class="docutils literal notranslate"><span class="pre">large_cities</span></code> dataset with probabilities on the linear- (left) and log-scale (right) and city sizes on the log-scale</span><a class="headerlink" href="#id23" title="Permalink to this image"></a></p>
</div>
<p>In terms of the maximal absolute distance between the two functions,
<span class="math notranslate nohighlight">\(D_n\)</span>, the fit is actually pretty good (this can be seen on the left plot,
note that the log-scale overemphasises the relatively small differences
in the right tail and should not be used for judging the value of <span class="math notranslate nohighlight">\(D_n\)</span>).</p>
</div></div></div>
<div class="section" id="uniform-distribution">
<h3><span class="section-number">6.3.3. </span>Uniform Distribution<a class="headerlink" href="#uniform-distribution" title="Permalink to this headline"></a></h3>
<p>Consider the Polish <em>Lotto</em> lottery, where 6 numbered balls
<span class="math notranslate nohighlight">\(\{1,2,\dots,49\}\)</span> are drawn without replacement from an urn.
We have a data set that summarises the number of times
each ball has been drawn in all the drawings in the period 1957–2016.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lotto</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching_data/master/marek/lotto_table.txt&quot;</span><span class="p">)</span>
<span class="n">lotto</span>
<span class="c1">## array([720., 720., 714., 752., 719., 753., 701., 692., 716., 694., 716.,</span>
<span class="c1">##        668., 749., 713., 723., 693., 777., 747., 728., 734., 762., 729.,</span>
<span class="c1">##        695., 761., 735., 719., 754., 741., 750., 701., 744., 729., 716.,</span>
<span class="c1">##        768., 715., 735., 725., 741., 697., 713., 711., 744., 652., 683.,</span>
<span class="c1">##        744., 714., 674., 654., 681.])</span>
</pre></div>
</div>
<p>Each event seems to occur more or less with the same probability.
Of course, the numbers on the balls are integer,
but in our idealised scenario we may try modelling this dataset
using a continuous <em>uniform distribution</em>,
which yields arbitrary real numbers on a given interval <em>(a, b)</em>,
i.e., between some <em>a</em> and <em>b</em>.
We denote such a distribution with U(<em>a</em>, <em>b</em>). It
has the probability density function given by</p>
<div class="math notranslate nohighlight">
\[
f(x) = \frac{1}{b-a},
\]</div>
<p>for <span class="math notranslate nohighlight">\(x\in(a, b)\)</span> and <span class="math notranslate nohighlight">\(f(x)=0\)</span> otherwise.</p>
<p>Note that <strong class="command">scipy.stats.uniform</strong>
uses parameters <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">scale</span></code> equal to <span class="math notranslate nohighlight">\(b-a\)</span> instead.</p>
<p>In our case, it makes sense to set <em>a=1</em> and <em>b=50</em> and interpret
an outcome like <em>49.1253</em> as representing the 49th ball
(compare the notion of the floor function, <span class="math notranslate nohighlight">\(\lfloor x\rfloor\)</span>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">1001</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">lotto</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">lotto</span><span class="p">),</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s2">&quot;edge&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">49</span><span class="p">),</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;PDF of U(1, 50)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id24">
<span id="fig-lotto"></span><img alt="../_images/lotto-19.png" src="../_images/lotto-19.png" />
<p class="caption"><span class="caption-number">Figure 6.10 </span><span class="caption-text">Histogram for the <code class="docutils literal notranslate"><span class="pre">lotto</span></code> dataset</span><a class="headerlink" href="#id24" title="Permalink to this image"></a></p>
</div>
<p>Visually, see <a class="reference internal" href="#fig-lotto"><span class="std std-numref">Figure 6.10</span></a>, this model makes much sense, but again,
some more rigorous statistical testing would be required to determine
if someone hasn’t been tampering with the lottery results, i.e.,
if data does not deviate from the uniform distribution significantly.</p>
<div class="proof proof-type-exercise" id="id25">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.5</span>
        
    </div><div class="proof-content">
<p>Does playing lotteries
and engaging in gambling make <em>rational</em> sense at all,
from the perspective of an individual player?
Well, we see that 16 is the most frequently occurring outcome
in <em>Lotto</em>, maybe there’s some magic in it?
Also, some people became millionaires after all, right?</p>
</div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>In data modelling (e.g., Bayesian statistics),
sometimes a uniform distribution is chosen
as a placeholder for “we know nothing about a phenomenon,
so let’s just assume that every event is equally likely”.
However, overall, it is quite fascinating that the real world tends
to be structured after all; emerging patterns are plentiful
and, furthermore, they are subject to qualitative analysis.</p>
</div>
</div>
<div class="section" id="distribution-mixtures">
<span id="sec-mixtures"></span><h3><span class="section-number">6.3.4. </span>Distribution Mixtures (*)<a class="headerlink" href="#distribution-mixtures" title="Permalink to this headline"></a></h3>
<p>Some datasets may fail to fit into simple models such as the ones
describe above. It may sometimes be due to their non-random behaviour:
statistics gives just one means to create data idealisations,
we also have partial differential equations, approximation theory,
graphs and complex networks, agent-based modelling, and so forth,
which might be worth giving a study (and then try).</p>
<p>Other reasons may be that what we observe is in fact a <em>mixture</em>
(creative combination) of simpler processes.</p>
<p>The dataset representing the December 2021 hourly averages
<a class="reference external" href="http://www.pedestrian.melbourne.vic.gov.au/">pedestrian counts</a>
near the Southern Cross Station in Melbourne
might be a good instance of such a scenario,
compare <a class="reference internal" href="210-vector.html#fig-peds-histogram"><span class="std std-numref">Figure 4.6</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">peds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching_data/master/marek/southern_cross_station_peds_2019_dec.txt&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>It might not be a bad idea to
try to fit a probabilistic (convex) combination
of three normal distributions <span class="math notranslate nohighlight">\(f_1\)</span>, <span class="math notranslate nohighlight">\(f_2\)</span>, <span class="math notranslate nohighlight">\(f_3\)</span>,
corresponding to the morning, lunch-time, and evening
pedestrian count peaks. This yields the PDF</p>
<div class="math notranslate nohighlight">
\[
f(x) = w_1 f_1(x) + w_2 f_2(x) + w_3 f_3(x)
\]</div>
<p>for some coefficients <span class="math notranslate nohighlight">\(w_1,w_2,w_3\ge 0\)</span> such that <span class="math notranslate nohighlight">\(w_1+w_2+w_3=1\)</span>.</p>
<p><a class="reference internal" href="#fig-mixture"><span class="std std-numref">Figure 6.11</span></a> depicts a mixture of N(8, 1), N(12, 1), and N(17, 2)
with the corresponding weights of 0.35, 0.1, and 0.55.
This particular data set is quite coarse-grained
(we only have 24 bar heights at our disposal), therefore
these estimated parameters and coefficients should be taken
with a pinch of chilli pepper.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">24</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">peds</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">peds</span><span class="p">),</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">p1</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">p2</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">p3</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.35</span><span class="o">*</span><span class="n">p1</span> <span class="o">+</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">p2</span> <span class="o">+</span> <span class="mf">0.55</span><span class="o">*</span><span class="n">p3</span>  <span class="c1"># weighted (convex) combination of 3 densities</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;PDF of a normal mixture&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id26">
<span id="fig-mixture"></span><img alt="../_images/mixture-21.png" src="../_images/mixture-21.png" />
<p class="caption"><span class="caption-number">Figure 6.11 </span><span class="caption-text">Histogram for the <code class="docutils literal notranslate"><span class="pre">peds</span></code> dataset and a guesstimated mixture of three normal distributions</span><a class="headerlink" href="#id26" title="Permalink to this image"></a></p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>It will frequently be the case in data wrangling
that more complex entities (models, methods) will be arising as combinations
of simpler (primitive) components.
This is why we should spend a great deal of time
studying the <em>fundamentals</em>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some data clustering techniques
(in particular, the <em>k</em>-means algorithm that we briefly discuss
later in this course)
could be used to split a data sample into disjoint chunks
corresponding to different mixture components.</p>
<p>Also, it might be the case that the mixture components can
in fact be explained by another categorical variable which
divides the dataset into natural groups, compare
<a class="reference internal" href="430-group-by.html#chap-group-by"><span class="std std-numref">Chapter 12</span></a>.</p>
</div>
</div>
</div>
<div class="section" id="generating-pseudorandom-numbers">
<h2><span class="section-number">6.4. </span>Generating Pseudorandom Numbers<a class="headerlink" href="#generating-pseudorandom-numbers" title="Permalink to this headline"></a></h2>
<p>A probability distribution is useful not only for describing a dataset.
It also enables us to perform many experiments on data that we don’t
currently have, but we might obtain in the future,
to test various scenarios and hypotheses.</p>
<p>To do this, we can generate a random sample of
independent (not related to each other) observations.</p>
<div class="section" id="id7">
<h3><span class="section-number">6.4.1. </span>Uniform Distribution<a class="headerlink" href="#id7" title="Permalink to this headline"></a></h3>
<p>When most people say <em>random</em>, they implicitly mean
<em>uniformly distributed</em>. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="c1">## array([0.69646919, 0.28613933, 0.22685145, 0.55131477, 0.71946897])</span>
</pre></div>
</div>
<p>gives 5 observations sampled independently
from the uniform distribution on the unit interval, i.e., U(0, 1).</p>
<p>The same with <strong class="program">scipy</strong>, but this time the support will be
<em>(-10, 15)</em>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>  <span class="c1"># from -10 to -10+25</span>
<span class="c1">## array([ 0.5776615 , 14.51910496,  7.12074346,  2.02329754, -0.19706205])</span>
</pre></div>
</div>
<p>Which we could as well have generated by manually shifting
and scaling the output the random number generator
on the unit interval using the formula <code class="docutils literal notranslate"><span class="pre">np.random.rand(5)*25-10</span></code>.</p>
</div>
<div class="section" id="not-exactly-random">
<span id="sec-seed"></span><h3><span class="section-number">6.4.2. </span>Not Exactly Random<a class="headerlink" href="#not-exactly-random" title="Permalink to this headline"></a></h3>
<p>Actually, we are generating numbers using a computer, which is purely
deterministic, hence we shall refer to them as <em>pseudorandom</em>
or random-like ones (albeit they are non-distinguishable from truly random,
when subject to rigorous tests for randomness).</p>
<p>To prove it, we can set the initial state of the number generator
(the <em>seed</em>) to some number and see what values are output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>  <span class="c1"># set seed</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="c1">## array([0.69646919, 0.28613933, 0.22685145, 0.55131477, 0.71946897])</span>
</pre></div>
</div>
<p>and then set the seed once again to the same number and
see how “random” the next values are:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>  <span class="c1"># set seed</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="c1">## array([0.69646919, 0.28613933, 0.22685145, 0.55131477, 0.71946897])</span>
</pre></div>
</div>
<p>This enables us to perform completely <em>reproducible</em> numerical
experiments, and this is a very good feature: truly scientific
inquiries should lead to identical results in the same conditions.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If we don’t set the seed manually,
it will be initialised based on the current wall time, which is
different every… time – therefore the numbers will <em>seem</em> random to us.</p>
</div>
<p>Many Python packages that we will be using in the future,
including <strong class="program">pandas</strong> and <strong class="program">sklearn</strong> rely
on <strong class="program">numpy</strong>’s random number generator,
thus we’ll be calling <strong class="command">numpy.random.seed</strong> to make them predictable.</p>
<p>Additionally, many of them
(e.g., <strong class="command">sklearn.model_selection.train_test_split</strong>
or <strong class="command">pandas.DataFrame.sample</strong>) are equipped with the <code class="docutils literal notranslate"><span class="pre">random_state</span></code>
argument, which can <em>temporarily</em> change the seed (for just one
call to that function). For instance:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="c1">## array([0.69646919, 0.28613933, 0.22685145, 0.55131477, 0.71946897])</span>
</pre></div>
</div>
<p>This gives the same sequence as above.</p>
</div>
<div class="section" id="sampling-from-other-distributions">
<h3><span class="section-number">6.4.3. </span>Sampling from Other Distributions<a class="headerlink" href="#sampling-from-other-distributions" title="Permalink to this headline"></a></h3>
<p>Of course, generating data from other distributions is possible too;
there are many <strong class="command">rvs</strong> methods implemented
in <strong class="program">scipy.stats</strong>.
For example, here is a sample from N(100, 16):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">50489</span><span class="p">)</span>
<span class="c1">## array([113.41134015,  46.99328545, 157.1304154 ])</span>
</pre></div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Conclusions based on simulated data are trustworthy,
because they cannot be manipulated.
Or can they?</p>
<p>The pseudorandom number generator’s seed used above,
<code class="docutils literal notranslate"><span class="pre">50489</span></code>, is quite suspicious. It might suggest that someone
wanted to <em>prove</em> some point (in this case, the violation
of the 3σ rule).</p>
<p>This is why we recommend sticking to one and only seed most of the time,
e.g., <code class="docutils literal notranslate"><span class="pre">123</span></code>, or – when performing simulations – setting
consecutive seeds for each iteration, <code class="docutils literal notranslate"><span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">2</span></code>, …</p>
</div>
<div class="proof proof-type-exercise" id="id27">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.6</span>
        
    </div><div class="proof-content">
<p>Generate 1000 pseudorandom numbers from the log-normal
distribution.</p>
</div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>(*)
Having a good pseudorandom number generator from the uniform distribution
on the unit interval is crucial, because sampling from other distributions
usually involves transforming independent U(0, 1) variates.</p>
<p>For instance, samples following any continuous cumulative distribution
function <span class="math notranslate nohighlight">\(F\)</span> can be constructed by means of <em>inverse transform sampling</em>:</p>
<ol class="simple">
<li><p>Generate a sample <span class="math notranslate nohighlight">\(x_1,\dots,x_n\)</span> independently from U(0, 1);</p></li>
<li><p>Transform each <span class="math notranslate nohighlight">\(x_i\)</span> by applying the quantile function,
<span class="math notranslate nohighlight">\(y_i=F^{-1}(x_i)\)</span>;</p></li>
<li><p>Now <span class="math notranslate nohighlight">\(y_1,\dots,y_n\)</span> follows the CDF <span class="math notranslate nohighlight">\(F\)</span>.</p></li>
</ol>
<p>For more topics on random number generation, see <span id="id8">[<a class="reference internal" href="999-bibliography.html#id42">Gen03</a>, <a class="reference internal" href="999-bibliography.html#id44">RC04</a>]</span>.</p>
</div>
<div class="proof proof-type-exercise" id="id28">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.7</span>
        
    </div><div class="proof-content">
<p>(*) Generate 1000 pseudorandom numbers from the log-normal
distribution using inverse transform sampling.</p>
</div></div><div class="proof proof-type-exercise" id="id29">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.8</span>
        
    </div><div class="proof-content">
<p>(**) Generate 1000 pseudorandom numbers from the distribution
mixture discussed in <a class="reference internal" href="#sec-mixtures"><span class="std std-numref">Section 6.3.4</span></a>.</p>
</div></div></div>
<div class="section" id="natural-variability">
<span id="sec-natural-variability"></span><h3><span class="section-number">6.4.4. </span>Natural Variability<a class="headerlink" href="#natural-variability" title="Permalink to this headline"></a></h3>
<p>Note that even a sample which we know that was generated from a
specific distribution will deviate from it, sometimes considerably.
Such effects usually disappear<a class="footnote-reference brackets" href="#footfts" id="id9">3</a> when the the availability of data increases, but definitely will be visible
for small sample sizes.</p>
<p>For example, <a class="reference internal" href="#fig-natural-variability"><span class="std std-numref">Figure 6.12</span></a>
depicts the histogram of 9 different
samples of size 100, all drawn independently from the normal distribution
N(0, 1).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.figsize&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># width=height</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1001</span><span class="p">)</span>
<span class="n">fx</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># PDF of N(0, 1)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">fx</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id30">
<span id="fig-natural-variability"></span><img alt="../_images/natural-variability-23.png" src="../_images/natural-variability-23.png" />
<p class="caption"><span class="caption-number">Figure 6.12 </span><span class="caption-text">All 9 samples are normally distributed!</span><a class="headerlink" href="#id30" title="Permalink to this image"></a></p>
</div>
<p>These is some ruggedness in the bars’ sizes that a naïve observer
might try to interpret as something meaningful;
a competent data scientist must train
their eye to ignore such impurities that are only due to random effects
(but be always ready do detect those which are worth attention).</p>
<p>More specialist tools such as hypothesis tests
enable us to tell if the deviation from the theoretical distribution
is statistically significant or not. This is why, after finishing this
introductory course, we should be yearning for more maths.</p>
<div class="proof proof-type-exercise" id="id31">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.9</span>
        
    </div><div class="proof-content">
<p>Repeat the above experiment for samples of size 10, 1000, and 10000.</p>
</div></div></div>
</div>
<div class="section" id="exercises">
<h2><span class="section-number">6.5. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline"></a></h2>
<div class="proof proof-type-exercise" id="id32">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.10</span>
        
    </div><div class="proof-content">
<p>Why is the notion of the mean income confusing the general public?</p>
</div></div><div class="proof proof-type-exercise" id="id33">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.11</span>
        
    </div><div class="proof-content">
<p>When manually setting the seed of a random number generator
makes sense?</p>
</div></div><div class="proof proof-type-exercise" id="id34">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.12</span>
        
    </div><div class="proof-content">
<p>Given a log-normally distributed sample <code class="docutils literal notranslate"><span class="pre">x</span></code>, how  can we turn it
to a normally distributed one, i.e., <code class="docutils literal notranslate"><span class="pre">y=</span></code><strong class="command">f</strong><code class="code docutils literal notranslate"><span class="pre">(x)</span></code>, with <strong class="command">f</strong> being… what?</p>
</div></div><div class="proof proof-type-exercise" id="id35">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.13</span>
        
    </div><div class="proof-content">
<p>What is the 3σ rule for normally distributed data?</p>
</div></div><hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="foothistconvergence"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>This intuition is of course theoretically
grounded and is based on the asymptotic behaviour of the histograms
as the estimators of the underlying probability density function,
see, e.g., <span id="id10">[<a class="reference internal" href="999-bibliography.html#id41">FD81</a>]</span> and the many references therein.</p>
</dd>
<dt class="label" id="footcdf"><span class="brackets"><a class="fn-backref" href="#id4">2</a></span></dt>
<dd><p>Actually, the probability distribution of any
real-valued random variable <span class="math notranslate nohighlight">\(X\)</span> can be uniquely defined
by means of a nondecreasing, right (upward) continuous
function <span class="math notranslate nohighlight">\(F:\mathbb{R}\to[0,1]\)</span> such that
<span class="math notranslate nohighlight">\(\lim_{x\to-\infty} F(x)=0\)</span> and <span class="math notranslate nohighlight">\(\lim_{x\to\infty} F(x)=1\)</span>,
in which case <span class="math notranslate nohighlight">\(\Pr(X\le x)=F(x)\)</span>.
The probability density function only exists for continuous
random variables and is defined as the derivative of <span class="math notranslate nohighlight">\(F\)</span>.</p>
</dd>
<dt class="label" id="footfts"><span class="brackets"><a class="fn-backref" href="#id9">3</a></span></dt>
<dd><p>Compare the Fundamental Theorem of Statistics
(the Glivenko–Cantelli theorem).</p>
</dd>
</dl>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="310-matrix.html" class="btn btn-neutral float-right" title="7. Multidimensional Numeric Data at a Glance" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="220-transform-vector.html" class="btn btn-neutral float-left" title="5. Processing Unidimensional Data" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2022, Marek Gagolewski. Licensed under CC BY-NC-ND 4.0.
      <span class="lastupdated">
        Last updated on 2022-06-01T15:08:42+1000.
      </span>
    Built with <a href="https://sphinx-doc.org/">Sphinx</a>
    and a customised <a href="https://github.com/rtfd/sphinx_rtd_theme">rtd</a> theme.
    </p>
  </div> 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>