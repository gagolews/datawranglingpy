<!DOCTYPE html>


<html class="writer-html5" lang="en" >
<!-- Copyright (C) 2020-2022, Marek Gagolewski <https://www.gagolewski.com> -->

<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>6. Continuous Probability Distributions &mdash; Minimalist Data Wrangling with Python by Marek Gagolewski</title>
  

  
  
  <script async defer src="https://buttons.github.io/buttons.js"></script>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/proof.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />

  
  

  
  

  
    <link rel="canonical" href="https://datawranglingpy.gagolewski.com/chapter/230-distribution.html" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/proof.js"></script>
        <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="7. Multidimensional Numeric Data at a Glance" href="310-matrix.html" />
    <link rel="prev" title="5. Processing Unidimensional Data" href="220-transform-vector.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html"> Minimalist Data Wrangling with Python [v1.0.1]
          

          
          </a>

          <div class="version">
          An open-access textbook<br />
          by <a style="color: inherit" href="https://www.gagolewski.com">Marek Gagolewski</a>
          </div>

<!--
          
            
            
              <div class="version">
                by Marek Gagolewski
              </div>
            
          
-->

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search phrase..." />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">About</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.gagolewski.com/">Author</a></li>
<li class="toctree-l1"><a class="reference external" href="https://datawranglingpy.gagolewski.com/datawranglingpy.pdf">This Book in PDF</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/gagolews/datawranglingpy/">Report Bugs or Typos (GitHub)</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/gagolews/teaching_data">Datasets</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Start Here</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="000-preface.html">Preface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introducing Python</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="110-setup.html">1. Getting Started with Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="120-scalar.html">2. Scalar Types and Control Structures in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="130-sequential.html">3. Sequential and Other Types in Python</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Unidimensional Data</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="210-vector.html">4. Unidimensional Numeric Data and Their Empirical Distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="220-transform-vector.html">5. Processing Unidimensional Data</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">6. Continuous Probability Distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#normal-distribution">6.1. Normal Distribution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#estimating-parameters">6.1.1. Estimating Parameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-models-are-useful">6.1.2. Data Models Are Useful</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#assessing-goodness-of-fit">6.2. Assessing Goodness-of-Fit</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#comparing-cumulative-distribution-functions">6.2.1. Comparing Cumulative Distribution Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#comparing-quantiles">6.2.2. Comparing Quantiles</a></li>
<li class="toctree-l3"><a class="reference internal" href="#kolmogorovsmirnov-test">6.2.3. Kolmogorov–Smirnov Test (*)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#other-noteworthy-distributions">6.3. Other Noteworthy Distributions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#log-normal-distribution">6.3.1. Log-normal Distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pareto-distribution">6.3.2. Pareto Distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#uniform-distribution">6.3.3. Uniform Distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#distribution-mixtures">6.3.4. Distribution Mixtures (*)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#generating-pseudorandom-numbers">6.4. Generating Pseudorandom Numbers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id13">6.4.1. Uniform Distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#not-exactly-random">6.4.2. Not Exactly Random</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sampling-from-other-distributions">6.4.3. Sampling from Other Distributions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#natural-variability">6.4.4. Natural Variability</a></li>
<li class="toctree-l3"><a class="reference internal" href="#adding-jitter-white-noise">6.4.5. Adding Jitter (White Noise)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#further-reading">6.5. Further Reading</a></li>
<li class="toctree-l2"><a class="reference internal" href="#exercises">6.6. Exercises</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multidimensional Data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="310-matrix.html">7. Multidimensional Numeric Data at a Glance</a></li>
<li class="toctree-l1"><a class="reference internal" href="320-transform-matrix.html">8. Processing Multidimensional Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="330-relationship.html">9. Exploring Relationships Between Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Heterogeneous Data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="410-data-frame.html">10. Introducing Data Frames</a></li>
<li class="toctree-l1"><a class="reference internal" href="420-categorical.html">11. Handling Categorical Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="430-group-by.html">12. Processing Data in Groups</a></li>
<li class="toctree-l1"><a class="reference internal" href="440-sql.html">13. Accessing Databases</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Data Types</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="510-text.html">14. Text Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="520-missingness.html">15. Missing, Censored, and Questionable Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="530-time-series.html">16. Time Series</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="998-changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="999-bibliography.html">Bibliography</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Minimalist Data Wrangling with Python [v1.0.1]</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          



















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li><span class="section-number">6. </span>Continuous Probability Distributions</li>
    
    
      <li class="wy-breadcrumbs-aside">

        
        
        <a class="github-button" href="https://github.com/gagolews/datawranglingpy" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star gagolews/datawranglingpy on GitHub">Star</a>
        


        
      </li>
    
  </ul>

  
  <div class="rst-breadcrumbs-buttons" role="navigation" aria-label="breadcrumb navigation">
      
        <a href="310-matrix.html" class="btn btn-neutral float-right" title="7. Multidimensional Numeric Data at a Glance" accesskey="n">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
      
      
        <a href="220-transform-vector.html" class="btn btn-neutral float-left" title="5. Processing Unidimensional Data" accesskey="p"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
      
  </div>
  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section class="tex2jax_ignore mathjax_ignore" id="continuous-probability-distributions">
<span id="chap-distribution"></span><h1><span class="section-number">6. </span>Continuous Probability Distributions<a class="headerlink" href="#continuous-probability-distributions" title="Permalink to this heading"></a></h1>
<blockquote>
<div><p><em>The open-access textbook</em> Minimalist Data
Wrangling with Python <em>by <a class="reference external" href="https://www.gagolewski.com">Marek Gagolewski</a>
is, and will remain, freely available for everyone’s enjoyment
(also in <a class="reference external" href="https://datawranglingpy.gagolewski.com/datawranglingpy.pdf">PDF</a>).
This is a non-profit project.
Although available online, this is a whole course;
it should be read from the beginning to the end.
Refer to the <a class="reference internal" href="000-preface.html#chap-preface"><span class="std std-ref">Preface</span></a> for general introductory remarks.</em></p>
</div></blockquote>
<p>Each successful data analyst will deal with hundreds or thousands of
datasets in their lifetime. In the long run, at some level, most of them will
be deemed <em>boring</em>. This is because only a few common patterns
will be occurring over and over again.</p>
<p>In particular, the previously mentioned bell-shapedness and right-skewness
are quite prevalent in the so-called real world.
Surprisingly, however, this is exactly when things
become scientific and interesting – allowing us to study various phenomena
at an appropriate level of generality.</p>
<p>Mathematically, such idealised patterns in the histogram shapes
can be formalised using the notion of a
<em>probability density function</em> (PDF) of a <em>continuous, real-valued random
variable</em>.</p>
<p>Intuitively<a class="footnote-reference brackets" href="#foothistconvergence" id="id1">1</a>, a PDF is
a smooth curve that would arise if we drew a histogram
for the entire <em>population</em> (e.g., all women living currently on Earth and
beyond or otherwise an extremely large data sample obtained by independently
querying the same underlying data generating process) in such a way that
the total area of all the bars is equal to 1
and the bin sizes are very small.</p>
<p>As stated at the beginning, we do not intend this to be a course in
probability theory and mathematical statistics.
Rather, it precedes and motivates them
(e.g., <span id="id2">[<a class="reference internal" href="999-bibliography.html#id44" title="F.M. Dekking, C. Kraaikamp, H.P. Lopuhaä, and L.E. Meester. A Modern Introduction to Probability and Statistics: Understanding Why and How. Springer, 2005.">DKLM05</a>, <a class="reference internal" href="999-bibliography.html#id47" title="J.E. Gentle. Computational Statistics. Springer-Verlag, 2009.">Gen09</a>, <a class="reference internal" href="999-bibliography.html#id42" title="J.E. Gentle. Theory of Statistics. book draft, 2020. URL: https://mason.gmu.edu/~jgentle/books/MathStat.pdf.">Gen20</a>]</span>). Therefore,
our definitions are out of necessity simplified so that they are digestible.
For the purpose of our illustrations, we will consider
the following characterisation.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>(*) We call an integrable function <span class="math notranslate nohighlight">\(f:\mathbb{R}\to\mathbb{R}\)</span>
a <em>probability density function</em>
if <span class="math notranslate nohighlight">\(f(x)\ge 0\)</span> for all <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(\int_{-\infty}^\infty f(x)\,dx=1\)</span>,
i.e., it is nonnegative and normalised in such a way that
the total area under the whole curve is 1.</p>
<p>For any <span class="math notranslate nohighlight">\(a &lt; b\)</span>, we treat the area under the fragment of
the <span class="math notranslate nohighlight">\(f(x)\)</span> curve for <span class="math notranslate nohighlight">\(x\)</span> between <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, i.e., <span class="math notranslate nohighlight">\(\int_a^b f(x)\,dx\)</span>,
as the probability of the underlying real-valued
random variable’s (theoretical data generating process’)
falling into the <span class="math notranslate nohighlight">\([a, b]\)</span> interval.</p>
</div>
<p>Some distributions appear more frequently than others
and appear to fit empirical data or parts thereof particularly well;
compare <span id="id3">[<a class="reference internal" href="999-bibliography.html#id81" title="C. Forbes, M. Evans, N. Hastings, and B. Peacock. Statistical Distributions. Wiley, 2010.">FEHP10</a>]</span>.
In this chapter, we review a few noteworthy probability distributions:
the normal, log-normal, Pareto, and uniform families
(we will also mention the chi-squared,
Kolmogorov, and exponential ones in this course).</p>
<section id="normal-distribution">
<h2><span class="section-number">6.1. </span>Normal Distribution<a class="headerlink" href="#normal-distribution" title="Permalink to this heading"></a></h2>
<p>A <em>normal (Gaussian) distribution</em> has a prototypical,
nicely symmetric, bell-shaped density.
It is described by two parameters:
<span class="math notranslate nohighlight">\(\mu\in\mathbb{R}\)</span> (the expected value, at which the PDF is centred)
and <span class="math notranslate nohighlight">\(\sigma&gt;0\)</span> (the standard deviation, saying how
much the distribution is dispersed around μ);
compare <a class="reference internal" href="#fig-normalpdf"><span class="std std-numref">Figure 6.1</span></a>.</p>
<p>The probability density function of N(μ, σ) is given by:</p>
<div class="math notranslate nohighlight">
\[
f(x) = \frac{1}{\sqrt{2\pi \sigma^2}}
\exp\left( -\frac{(x-\mu)^2}{2\sigma^2} \right).
\]</div>
<figure class="align-default" id="id25">
<span id="fig-normalpdf"></span><img alt="../_images/normalpdf-1.png" src="../_images/normalpdf-1.png" />
<figcaption>
<p><span class="caption-number">Figure 6.1 </span><span class="caption-text">The probability density functions of some normal distributions N(μ, σ); note that μ is responsible for shifting and σ affects scaling/stretching of the probability mass</span><a class="headerlink" href="#id25" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<section id="estimating-parameters">
<h3><span class="section-number">6.1.1. </span>Estimating Parameters<a class="headerlink" href="#estimating-parameters" title="Permalink to this heading"></a></h3>
<p>A course in statistics (which, again, this one is not,
we are merely making an illustration here), may tell us
that the sample arithmetic mean <span class="math notranslate nohighlight">\(\bar{x}\)</span> and standard deviation <span class="math notranslate nohighlight">\(s\)</span>
are natural, statistically well-behaving <em>estimators</em> of the said parameters:
if all observations would really be drawn independently from N(μ, σ) each,
then we <em>expect</em> <span class="math notranslate nohighlight">\(\bar{x}\)</span> and <span class="math notranslate nohighlight">\(s\)</span> be equal to, more or less, μ and σ (the larger the sample size,
the smaller the error).</p>
<p>Recall the <code class="docutils literal notranslate"><span class="pre">heights</span></code> (females from the NHANES study)
dataset and its bell-shaped histogram
in <a class="reference internal" href="210-vector.html#fig-heights-histogram-bins11"><span class="std std-numref">Figure 4.2</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">heights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching_data/master/marek/nhanes_adult_female_height_2020.txt&quot;</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">heights</span><span class="p">)</span>
<span class="n">n</span>
<span class="c1">## 4221</span>
</pre></div>
</div>
<p>Let us estimate the said parameters for this sample:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">heights</span><span class="p">)</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span>
<span class="c1">## (160.13679222932953, 7.062858532891359)</span>
</pre></div>
</div>
<p>Mathematically, we will denote these two with
<span class="math notranslate nohighlight">\(\hat{\mu}\)</span> and <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span> (mu and sigma with a hat) to emphasise that
they are merely guesstimates<a class="footnote-reference brackets" href="#footmle" id="id4">2</a> of the unknown respective parameters
<span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>. On a side note, we use <code class="docutils literal notranslate"><span class="pre">ddof=1</span></code> in the latter case,
because this estimator has slightly better statistical properties
in this context.</p>
<p>Let us draw the fitted density function
(i.e., the PDF of N(160.1, 7.06) which we can compute using
<strong class="command">scipy.stats.norm.pdf</strong>),
on top of the histogram; see <a class="reference internal" href="#fig-heights-normal"><span class="std std-numref">Figure 6.2</span></a>.
We pass <code class="docutils literal notranslate"><span class="pre">stat=&quot;density&quot;</span></code> to <strong class="command">seaborn.histplot</strong>
so that the histogram bars are normalised
(i.e., the total area of these rectangles sums to 1).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">heights</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">heights</span><span class="p">),</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">),</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;PDF of N(</span><span class="si">{</span><span class="n">mu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id26">
<span id="fig-heights-normal"></span><img alt="../_images/heights-normal-3.png" src="../_images/heights-normal-3.png" />
<figcaption>
<p><span class="caption-number">Figure 6.2 </span><span class="caption-text">A histogram and the probability density function of the fitted normal distribution for the <code class="docutils literal notranslate"><span class="pre">heights</span></code> dataset</span><a class="headerlink" href="#id26" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>At first glance, this is a genuinely nice match.
Before proceeding with an overview of the ways to assess
the goodness-of-fit more rigorously, we should praise the potential
benefits of having an idealised <em>model</em> of our dataset at our disposal.</p>
</section>
<section id="data-models-are-useful">
<h3><span class="section-number">6.1.2. </span>Data Models Are Useful<a class="headerlink" href="#data-models-are-useful" title="Permalink to this heading"></a></h3>
<p><em>If</em> (provided that, assuming that, on condition that)
our sample is a realisation of independent random variables
following a given distribution, or a data analyst judges that such an
approximation might be justified or beneficial, then we have a set of many
numbers <em>reduced</em> to merely a few parameters.</p>
<p>In the above case, we might want to risk the statement
that data follow the normal distribution (assumption 1)
with parameters <span class="math notranslate nohighlight">\(\mu=160.1\)</span> and <span class="math notranslate nohighlight">\(\sigma=7.06\)</span> (assumption 2).
Still, the choice of the distribution family is one thing,
and the way we estimate the underlying parameters
(in our case, we use <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> and <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span>)
is another.</p>
<p>This not only saves storage space and computational time, but also – based
on what we can learn from a course in probability and statistics
(by appropriately integrating the PDF) –
we can imply facts such as for normally distributed data:</p>
<ul class="simple">
<li><p>ca. 68% of (i.e., a <em>majority</em>) women are
<span class="math notranslate nohighlight">\(\mu\pm \sigma\)</span> tall (the 1σ rule),</p></li>
<li><p>ca. 95% of (i.e., <em>most typical</em>) women are
<span class="math notranslate nohighlight">\(\mu\pm 2\sigma\)</span> tall (the 2σ rule),</p></li>
<li><p>ca. 99.7% of (i.e., <em>almost all</em>) women are
<span class="math notranslate nohighlight">\(\mu\pm 3\sigma\)</span> tall (the 3σ rule).</p></li>
</ul>
<p>Also, if we knew that the distribution of heights of men
is also normal with some other parameters, we could be able to
make some comparisons between the two samples.
For example, we could compute the probability that a woman
randomly selected from the crowd is taller than a male passer-by.</p>
<p>Furthermore, there is a range of <em>parametric</em> (assuming some distribution
family) statistical methods that could <em>additionally</em> be used if we assumed
the data normality, e.g., the <em>t</em>-test to compare the expected values.</p>
<div class="proof proof-type-exercise" id="id27">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.1</span>
        
    </div><div class="proof-content">
<p>How different manufacturing industries (e.g., clothing)
can make use of such models?
Are simplifications necessary when dealing with complexity?
What are the alternatives?</p>
</div></div><div class="admonition important">
<p class="admonition-title">Important</p>
<p>We should always verify the assumptions of a model that we wish to apply
in practice.  In particular, we will soon note that incomes
are not normally distributed. Therefore, we must not
refer to the above 2σ or 3σ rule in their case.
A cow neither barks nor can it serve as a screwdriver. Period.</p>
</div>
</section>
</section>
<section id="assessing-goodness-of-fit">
<h2><span class="section-number">6.2. </span>Assessing Goodness-of-Fit<a class="headerlink" href="#assessing-goodness-of-fit" title="Permalink to this heading"></a></h2>
<section id="comparing-cumulative-distribution-functions">
<h3><span class="section-number">6.2.1. </span>Comparing Cumulative Distribution Functions<a class="headerlink" href="#comparing-cumulative-distribution-functions" title="Permalink to this heading"></a></h3>
<p>In the previous subsection, we were comparing densities and histograms.
It turns out that there is a better way of assessing the extent to which
a sample deviates from a hypothesised distribution.
Namely, we can measure the discrepancy between
some theoretical <em>cumulative distribution function</em> (CDF)
and the empirical one (<span class="math notranslate nohighlight">\(\hat{F}_n\)</span>, see <a class="reference internal" href="210-vector.html#sec-ecdf"><span class="std std-numref">Section 4.3.8</span></a>).</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If <span class="math notranslate nohighlight">\(f\)</span> is a PDF, then the corresponding
theoretical CDF is defined as <span class="math notranslate nohighlight">\(F(x) = \int_{-\infty}^x f(t)\,dt\)</span>,
i.e., the probability of the underlying random variable’s
taking a value less than or equal to <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>By definition<a class="footnote-reference brackets" href="#footcdf" id="id5">3</a>, each CDF takes values in the unit interval (<span class="math notranslate nohighlight">\([0,1]\)</span>)
and is nondecreasing.</p>
</div>
<p>For the normal distribution family,
the values of the theoretical CDF can be computed by calling <strong class="command">scipy.stats.norm.cdf</strong>; see <a class="reference internal" href="#fig-heights-cdf"><span class="std std-numref">Figure 6.3</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">heights</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">heights</span><span class="p">),</span> <span class="mi">1001</span><span class="p">)</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>  <span class="c1"># sample the CDF at many points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;CDF of N(</span><span class="si">{</span><span class="n">mu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">heights_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">heights</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">heights_sorted</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">,</span>
    <span class="n">drawstyle</span><span class="o">=</span><span class="s2">&quot;steps-post&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Empirical CDF&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Prob(height $</span><span class="se">\\</span><span class="s2">leq$ x)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id28">
<span id="fig-heights-cdf"></span><img alt="../_images/heights-cdf-5.png" src="../_images/heights-cdf-5.png" />
<figcaption>
<p><span class="caption-number">Figure 6.3 </span><span class="caption-text">The empirical CDF and the fitted normal CDF for the <code class="docutils literal notranslate"><span class="pre">heights</span></code> dataset; the fit is superb</span><a class="headerlink" href="#id28" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>This <em>looks</em> like a superb match.</p>
<div class="proof proof-type-example" id="id29">

    <div class="proof-title">
        <span class="proof-type">Example 6.2</span>
        
    </div><div class="proof-content">
<p><span class="math notranslate nohighlight">\(F(b)-F(a)=\int_{a}^b f(t)\,dt\)</span> is the probability
of generating a value in the interval <span class="math notranslate nohighlight">\([a, b]\)</span>.</p>
<p>Let us empirically verify the 3σ rule:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">F</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
<span class="n">F</span><span class="p">(</span><span class="n">mu</span><span class="o">+</span><span class="mi">3</span><span class="o">*</span><span class="n">sigma</span><span class="p">)</span> <span class="o">-</span> <span class="n">F</span><span class="p">(</span><span class="n">mu</span><span class="o">-</span><span class="mi">3</span><span class="o">*</span><span class="n">sigma</span><span class="p">)</span>
<span class="c1">## 0.9973002039367398</span>
</pre></div>
</div>
<p>Indeed, almost all observations are within
<span class="math notranslate nohighlight">\([\mu-3\sigma, \mu+3\sigma]\)</span>, if data are normally distributed.</p>
</div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>A common way to summarise the discrepancy between
the empirical and a given theoretical CDF is by computing
the greatest absolute deviation:</p>
<div class="math notranslate nohighlight">
\[
\hat{D}_n = \sup_{t\in\mathbb{R}} | \hat{F}_n(t) - F(t) |,
\]</div>
<p>where the supremum is a continuous version of the maximum.</p>
</div>
<p>It holds:</p>
<div class="math notranslate nohighlight">
\[
\hat{D}_n = \max \left\{
    \max_{k=1,\dots,n}\left\{ \left|\tfrac{k-1}{n} - F(x_{(k)})\right| \right\},
    \max_{k=1,\dots,n}\left\{ \left|\tfrac{k}{n} - F(x_{(k)})\right| \right\}
\right\},
\]</div>
<p>i.e., <span class="math notranslate nohighlight">\(F\)</span> needs to be probed only at the <span class="math notranslate nohighlight">\(n\)</span> points from the sorted
input sample.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_Dn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">F</span><span class="p">):</span>  <span class="c1"># equivalent to scipy.stats.kstest(x, F)[0]</span>
    <span class="n">Fx</span> <span class="o">=</span> <span class="n">F</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 1, 2, ..., n</span>
    <span class="n">Dn1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">n</span> <span class="o">-</span> <span class="n">Fx</span><span class="p">))</span>
    <span class="n">Dn2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">k</span><span class="o">/</span><span class="n">n</span> <span class="o">-</span> <span class="n">Fx</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="n">Dn1</span><span class="p">,</span> <span class="n">Dn2</span><span class="p">)</span>

<span class="n">Dn</span> <span class="o">=</span> <span class="n">compute_Dn</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="n">F</span><span class="p">)</span>
<span class="n">Dn</span>
<span class="c1">## 0.010470976524201148</span>
</pre></div>
</div>
<p>If the difference is <em>sufficiently<a class="footnote-reference brackets" href="#footsuffsmall" id="id6">4</a> small</em>,
then we can assume that a normal model
describes data quite well.
This is indeed the case here: we may estimate the probability
of someone being as tall as any given height
with an error less than about 1.05%.</p>
</section>
<section id="comparing-quantiles">
<span id="sec-qqplot"></span><h3><span class="section-number">6.2.2. </span>Comparing Quantiles<a class="headerlink" href="#comparing-quantiles" title="Permalink to this heading"></a></h3>
<p>A <em>Q-Q plot</em> (quantile-quantile or probability plot)
is another graphical method for comparing two distributions.
This time, instead of working with a cumulative distribution function <span class="math notranslate nohighlight">\(F\)</span>,
we will be dealing with its (generalised) inverse, i.e.,
the quantile function <span class="math notranslate nohighlight">\(Q\)</span>.</p>
<p>Given a CDF <span class="math notranslate nohighlight">\(F\)</span>, the corresponding <em>quantile function</em> is defined
for any <span class="math notranslate nohighlight">\(p\in(0,1)\)</span> as:</p>
<div class="math notranslate nohighlight">
\[
Q(p) = \inf\{ x: F(x) \ge p \},
\]</div>
<p>i.e., the smallest <span class="math notranslate nohighlight">\(x\)</span> such that the probability of drawing
a value not greater than <span class="math notranslate nohighlight">\(x\)</span> is at least <span class="math notranslate nohighlight">\(p\)</span>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If a CDF <span class="math notranslate nohighlight">\(F\)</span> is continuous, and this is the assumption in the current
chapter, then <span class="math notranslate nohighlight">\(Q\)</span> is exactly its inverse, i.e., it holds <span class="math notranslate nohighlight">\(Q(p)=F^{-1}(p)\)</span>
for all <span class="math notranslate nohighlight">\(p\in(0, 1)\)</span>; compare <a class="reference internal" href="#fig-normalcdfvsquant"><span class="std std-numref">Figure 6.4</span></a>.</p>
</div>
<figure class="align-default" id="id30">
<span id="fig-normalcdfvsquant"></span><img alt="../_images/normalcdfvsquant-7.png" src="../_images/normalcdfvsquant-7.png" />
<figcaption>
<p><span class="caption-number">Figure 6.4 </span><span class="caption-text">The cumulative distribution functions (left) and the quantile functions (being the inverse of the CDF; right) of some normal distributions</span><a class="headerlink" href="#id30" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>The theoretical quantiles can be generated
by the <strong class="command">scipy.stats.norm.ppf</strong> function.
Here, <em>ppf</em> stands for the percent point function which is another
(yet quite esoteric) name for the above <span class="math notranslate nohighlight">\(Q\)</span>.</p>
<p>For instance, in our N(160.1, 7.06)-distributed <code class="docutils literal notranslate"><span class="pre">heights</span></code> dataset,
<span class="math notranslate nohighlight">\(Q(0.9)\)</span> is the height not exceeded by 90% of the female population.
In other words, only 10% of American women are taller than:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
<span class="c1">## 169.18820963937648</span>
</pre></div>
</div>
<div style="margin-top: 1em"></div><p>A Q-Q plot draws a version of sample quantiles
as a function of the corresponding theoretical quantiles.
The sample quantiles that we introduced
in <a class="reference internal" href="220-transform-vector.html#sec-quantiles"><span class="std std-numref">Section 5.1.1.3</span></a> are natural estimators
of the theoretical quantile function.
However, we also mentioned that there are quite a few possible
definitions thereof in the literature; compare <span id="id7">[<a class="reference internal" href="999-bibliography.html#id9" title="R.J. Hyndman and Y. Fan. Sample quantiles in statistical packages. American Statistician, 50(4):361–365, 1996. doi:10.2307/2684934.">HF96</a>]</span>.</p>
<p>For simplicity, instead of using <strong class="command">numpy.quantile</strong>,
we will assume that the <span class="math notranslate nohighlight">\(\frac{i}{n+1}\)</span>-quantile<a class="footnote-reference brackets" href="#footprobplot" id="id8">5</a>
is equal to <span class="math notranslate nohighlight">\(x_{(i)}\)</span>, i.e., the <em>i</em>-th smallest value in
a given sample <span class="math notranslate nohighlight">\((x_1,x_2,\dots,x_n)\)</span>
and consider only <span class="math notranslate nohighlight">\(i=1, 2, \dots, n\)</span>.</p>
<p>Our simplified setting avoids the problem which arises
when the 0- or 1-quantiles of the theoretical distribution,
i.e., <span class="math notranslate nohighlight">\(Q(0)\)</span> or <span class="math notranslate nohighlight">\(Q(1)\)</span>, are infinite (and this is the case for
the normal distribution family).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">qq_plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Q</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Draws a Q-Q plot, given:</span>
<span class="sd">    * x - a data sample (vector)</span>
<span class="sd">    * Q - a theoretical quantile function</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>    <span class="c1"># 1/(n+1), 2/(n+2), ..., n/(n+1)</span>
    <span class="n">x_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>          <span class="c1"># sample quantiles</span>
    <span class="n">quantiles</span> <span class="o">=</span> <span class="n">Q</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>               <span class="c1"># theoretical quantiles</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">quantiles</span><span class="p">,</span> <span class="n">x_sorted</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axline</span><span class="p">((</span><span class="n">x_sorted</span><span class="p">[</span><span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="n">x_sorted</span><span class="p">[</span><span class="n">n</span><span class="o">//</span><span class="mi">2</span><span class="p">]),</span> <span class="n">slope</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>  <span class="c1"># identity line</span>
</pre></div>
</div>
<p><a class="reference internal" href="#fig-qq-heights"><span class="std std-numref">Figure 6.5</span></a> depicts the Q-Q plot for our example dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">qq_plot</span><span class="p">(</span><span class="n">heights</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">q</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantiles of N(</span><span class="si">{</span><span class="n">mu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Sample quantiles&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id31">
<span id="fig-qq-heights"></span><img alt="../_images/qq-heights-9.png" src="../_images/qq-heights-9.png" />
<figcaption>
<p><span class="caption-number">Figure 6.5 </span><span class="caption-text">Q-Q plot for the <code class="docutils literal notranslate"><span class="pre">heights</span></code> dataset; it’s a nice fit</span><a class="headerlink" href="#id31" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>Ideally, the points are expected to be arranged on the <span class="math notranslate nohighlight">\(y=x\)</span> line
(which was added for readability). This would happen if
the sample quantiles matched the theoretical ones perfectly.
In our case, there are small discrepancies<a class="footnote-reference brackets" href="#footqqplotpearson" id="id9">6</a> in the tails
(e.g., the smallest observation was slightly smaller than expected,
and the largest one was larger than expected),
although it is quite a <em>normal</em> behaviour for small samples
and certain distribution families.
Still, we can say that we observe a very good fit.</p>
</section>
<section id="kolmogorovsmirnov-test">
<span id="sec-ks-test"></span><h3><span class="section-number">6.2.3. </span>Kolmogorov–Smirnov Test (*)<a class="headerlink" href="#kolmogorovsmirnov-test" title="Permalink to this heading"></a></h3>
<p>To be scientific, we should yearn for some more formal method that will
enable us to test the null hypothesis stating that a given empirical distribution <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> does not differ <em>significantly</em> from the
theoretical continuous CDF <span class="math notranslate nohighlight">\(F\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left\{
\begin{array}{rll}
H_0: &amp; \hat{F}_n = F &amp; \text{(null hypothesis)}\\
H_1: &amp; \hat{F}_n \neq F &amp; \text{(two-sided alternative)} \\
\end{array}
\right.
\end{split}\]</div>
<p>The popular goodness-of-fit test by Kolmogorov and Smirnov can give
us a conservative interval of the acceptable values of <span class="math notranslate nohighlight">\(\hat{D}_n\)</span>
(again: the largest deviation between the empirical and theoretical CDF)
as a function of <span class="math notranslate nohighlight">\(n\)</span> (within the framework of frequentist hypothesis testing).</p>
<p>Namely, if the <em>test statistic</em> <span class="math notranslate nohighlight">\(\hat{D}_n\)</span> is smaller than some
<em>critical value</em> <span class="math notranslate nohighlight">\(K_n\)</span>, then we shall deem the difference insignificant.
This is to take into account the fact that reality might deviate
from the ideal. In <a class="reference internal" href="#sec-natural-variability"><span class="std std-numref">Section 6.4.4</span></a>, we mention
that even for samples that truly come from a hypothesised distribution,
there is some inherent variability. We need to be somewhat tolerant.</p>
<p>A good textbook in statistics will tell us (and prove) that,
under the assumption that <span class="math notranslate nohighlight">\(\hat{F}_n\)</span> is the ECDF of a sample
of <span class="math notranslate nohighlight">\(n\)</span> independent variables <em>really</em> generated from a continuous CDF <span class="math notranslate nohighlight">\(F\)</span>,
the random variable <span class="math notranslate nohighlight">\(\hat{D}_n = \sup_{t\in\mathbb{R}} | \hat{F}_n(t) - F(t) |\)</span>
follows the Kolmogorov distribution with parameter <span class="math notranslate nohighlight">\(n\)</span>
(available via <strong class="command">scipy.stats.kstwo</strong>).</p>
<p>In other words, if we generate many samples of length <span class="math notranslate nohighlight">\(n\)</span> from <span class="math notranslate nohighlight">\(F\)</span>,
and compute <span class="math notranslate nohighlight">\(\hat{D}_n\)</span>s for each of them,
we expect it to be distributed like in <a class="reference internal" href="#fig-kolmogorovdistr"><span class="std std-numref">Figure 6.6</span></a>.</p>
<figure class="align-default" id="id32">
<span id="fig-kolmogorovdistr"></span><img alt="../_images/kolmogorovdistr-11.png" src="../_images/kolmogorovdistr-11.png" />
<figcaption>
<p><span class="caption-number">Figure 6.6 </span><span class="caption-text">Densities (left) and cumulative distribution functions (right) of some Kolmogorov distributions; the greater the sample size, the smaller the acceptable deviations between the theoretical and empirical CDFs</span><a class="headerlink" href="#id32" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>The choice <span class="math notranslate nohighlight">\(K_n\)</span> involves a trade-off between our desire to:</p>
<ul class="simple">
<li><p>accept the null hypothesis when it is true
(data <em>really</em> come from <span class="math notranslate nohighlight">\(F\)</span>), and</p></li>
<li><p>reject it when it is false (data follow some other distribution,
i.e., the difference is significant enough).</p></li>
</ul>
<p>These two needs are, unfortunately, mutually exclusive.</p>
<p>In practice, we assume some fixed upper bound (<em>significance level</em>)
for making the former kind of mistake, the so-called <em>type-I error</em>.
A nicely conservative (in a good way<a class="footnote-reference brackets" href="#footsmallalpha" id="id10">7</a>) value that we
suggest employing is <span class="math notranslate nohighlight">\(\alpha=0.001=0.1\%\)</span>, i.e., only 1 out of 1000 samples
that really come from <span class="math notranslate nohighlight">\(F\)</span> will be rejected as not coming from <span class="math notranslate nohighlight">\(F\)</span>.</p>
<p>Such a <span class="math notranslate nohighlight">\(K_n\)</span> may be determined by considering the inverse of the
CDF of the Kolmogorov distribution, <span class="math notranslate nohighlight">\(\Xi_n\)</span>.
Namely, <span class="math notranslate nohighlight">\(K_n=\Xi_n^{-1}(1-\alpha)\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span>  <span class="c1"># significance level</span>
<span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">kstwo</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="c1">## 0.029964456376393188</span>
</pre></div>
</div>
<p>In our case <span class="math notranslate nohighlight">\(\hat{D}_n &lt; K_n\)</span>, because <span class="math notranslate nohighlight">\(0.01047 &lt; 0.02996\)</span>.
We conclude that our empirical (<code class="docutils literal notranslate"><span class="pre">heights</span></code>) distribution
does not differ significantly (at significance level <span class="math notranslate nohighlight">\(0.1\%\)</span>)
from the assumed one, i.e., N(160.1, 7.06).
In other words, we do not have enough evidence against the statement
that data are normally distributed. It is the presumption of innocence:
they are normal enough.</p>
<p>We will go back to this discussion in <a class="reference internal" href="#sec-natural-variability"><span class="std std-numref">Section 6.4.4</span></a>
and <a class="reference internal" href="430-group-by.html#sec-ks-test2"><span class="std std-numref">Section 12.2.6</span></a>.</p>
</section>
</section>
<section id="other-noteworthy-distributions">
<h2><span class="section-number">6.3. </span>Other Noteworthy Distributions<a class="headerlink" href="#other-noteworthy-distributions" title="Permalink to this heading"></a></h2>
<section id="log-normal-distribution">
<h3><span class="section-number">6.3.1. </span>Log-normal Distribution<a class="headerlink" href="#log-normal-distribution" title="Permalink to this heading"></a></h3>
<p>We say that a sample is <em>log-normally distributed</em>,
if its logarithm is normally distributed.</p>
<p>In particular, it is sometimes observed that the income of most<a class="footnote-reference brackets" href="#footmost" id="id11">8</a>
individuals is distributed, at least approximately, log-normally.
Let us investigate whether this is the case for UK taxpayers.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">income</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching_data/master/marek/uk_income_simulated_2020.txt&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The plotting of the histogram of the logarithm of
income is left as an exercise (we can pass <code class="docutils literal notranslate"><span class="pre">log_scale=True</span></code>
to <strong class="command">seaborn.histplot</strong>; we will plot it soon anyway
in a different way).
We proceed directly with the
fitting of a log-normal model, LN(μ, σ).
The fitting process is similar to the normal case, but this time
we determine the mean and standard deviation based on the logarithms of data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lmu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">income</span><span class="p">))</span>
<span class="n">lsigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">lmu</span><span class="p">,</span> <span class="n">lsigma</span>
<span class="c1">## (10.314409794364623, 0.5816585197803816)</span>
</pre></div>
</div>
<p><a class="reference internal" href="#fig-heights-lognormal"><span class="std std-numref">Figure 6.7</span></a> depicts the fitted probability density
function together with the histograms on the log- and original scale.
When creating this plot, there are two pitfalls, though.
Firstly, <strong class="command">scipy.stats.lognorm</strong>
encodes the distribution via the parameter <span class="math notranslate nohighlight">\(s\)</span> equal to <span class="math notranslate nohighlight">\(\sigma\)</span>
and scale equal to <span class="math notranslate nohighlight">\(e^\mu\)</span>. Computing the PDF at different points
is done as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="mi">101</span><span class="p">)</span>
<span class="n">fx</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">lognorm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">lsigma</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">lmu</span><span class="p">))</span>
</pre></div>
</div>
<p>Second, passing both <code class="docutils literal notranslate"><span class="pre">log_scale=True</span></code> and <code class="docutils literal notranslate"><span class="pre">stat=&quot;density&quot;</span></code>
to <strong class="command">seaborn.histplot</strong> does not normalise the values on the
y-axis correctly. To make the histogram on the log-scale
comparable with the true density, we need to turn on the log-scale
and pass the manually generated bins that are equidistant
on the logarithmic scale (via <strong class="command">numpy.geomspace</strong>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">geomspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">income</span><span class="p">),</span> <span class="mi">30</span><span class="p">)</span>
</pre></div>
</div>
<p>And now:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">income</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">)</span>  <span class="c1"># own bins!</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>  <span class="c1"># log-scale on the x-axis</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">fx</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">income</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">fx</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;PDF of LN(</span><span class="si">{</span><span class="n">lmu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">lsigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id33">
<span id="fig-heights-lognormal"></span><img alt="../_images/heights-lognormal-13.png" src="../_images/heights-lognormal-13.png" />
<figcaption>
<p><span class="caption-number">Figure 6.7 </span><span class="caption-text">A histogram and the probability density function of the fitted log-normal distribution for the <code class="docutils literal notranslate"><span class="pre">income</span></code> dataset, on log- (left) and original (right) scale</span><a class="headerlink" href="#id33" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>Overall, this fit is not too bad.
Nonetheless, we deal with only a sample of 1000 households here;
the original UK Office of National Statistics <a class="reference external" href="https://www.ons.gov.uk/peoplepopulationandcommunity/personalandhouseholdfinances/incomeandwealth/bulletins/householddisposableincomeandinequality/financialyear2020">data</a>
could tell us more about the quality of this model in general,
but it is beyond the scope of our simple exercise.</p>
<p>Furthermore, <a class="reference internal" href="#fig-qq-income"><span class="std std-numref">Figure 6.8</span></a> gives the quantile-quantile plot on a
double logarithmic scale for the above log-normal model.
Additionally, we (empirically) verify the hypothesis
of normality (using a “normal” normal distribution, not its “log”
version).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">qq_plot</span><span class="p">(</span>  <span class="c1"># see above for the definition</span>
    <span class="n">income</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">q</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">lognorm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">lsigma</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">lmu</span><span class="p">))</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantiles of LN(</span><span class="si">{</span><span class="n">lmu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">lsigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Sample quantiles&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">income</span><span class="p">)</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">income</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">qq_plot</span><span class="p">(</span><span class="n">income</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">q</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantiles of N(</span><span class="si">{</span><span class="n">mu</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">sigma</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id34">
<span id="fig-qq-income"></span><img alt="../_images/qq-income-15.png" src="../_images/qq-income-15.png" />
<figcaption>
<p><span class="caption-number">Figure 6.8 </span><span class="caption-text">Q-Q plots for the <code class="docutils literal notranslate"><span class="pre">income</span></code> dataset vs a fitted log-normal (good fit; left) and normal (bad fit; right) distribution</span><a class="headerlink" href="#id34" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<div class="proof proof-type-exercise" id="id35">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.3</span>
        
    </div><div class="proof-content">
<p>Graphically compare the empirical CDF for <code class="docutils literal notranslate"><span class="pre">income</span></code>
and the theoretical CDF of LN(10.3, 0.58).</p>
</div></div><div class="proof proof-type-exercise" id="id36">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.4</span>
        
    </div><div class="proof-content">
<p>(*) Perform the Kolmogorov–Smirnov goodness-of-fit test
as in <a class="reference internal" href="#sec-ks-test"><span class="std std-numref">Section 6.2.3</span></a> to verify that the hypothesis
of log-normality is not rejected at the <span class="math notranslate nohighlight">\(\alpha=0.001\)</span> significance level.
At the same time, the income distribution significantly differs
from a normal one.</p>
</div></div><p>The hypothesis that our data follow
a normal distribution is most likely false.
On the other hand, the log-normal model, might be quite adequate.
It again reduced the whole dataset to merely two numbers,
μ and σ, based on which (and probability theory),
we may deduce that:</p>
<ul class="simple">
<li><p>the expected average (mean) income is <span class="math notranslate nohighlight">\(e^{\mu + \sigma^2/2}\)</span>,</p></li>
<li><p>median is <span class="math notranslate nohighlight">\(e^\mu\)</span>,</p></li>
<li><p>most probable one (mode) in <span class="math notranslate nohighlight">\(e^{\mu-\sigma^2}\)</span>,</p></li>
</ul>
<p>etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Recall again that for skewed distributions such as this one,
reporting the mean might be misleading.
This is why <em>most</em> people get angry when they read the news
about the prospering economy (“yeah, we’d like to see that
kind of money in our pockets”). Hence, it is not only μ that matters,
it is also σ that quantifies the discrepancy between the rich and the poor
(too much inequality is bad, but also too much uniformity is
to be avoided).</p>
</div>
<p>For a normal distribution, the situation is vastly different,
because the mean, the median, and the most probable
outcomes tend to be the same – the distribution is
symmetric around μ.</p>
<div class="proof proof-type-exercise" id="id37">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.5</span>
        
    </div><div class="proof-content">
<p>What is the fraction of people with earnings
below the mean in our LN(10.3, 0.58) model?
Hint: use <strong class="command">scipy.stats.lognorm.cdf</strong> to get the answer.</p>
</div></div></section>
<section id="pareto-distribution">
<span id="sec-pareto"></span><h3><span class="section-number">6.3.2. </span>Pareto Distribution<a class="headerlink" href="#pareto-distribution" title="Permalink to this heading"></a></h3>
<p>Consider again the dataset
on the populations of the US cities in the 2000 US Census:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cities</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching_data/master/other/us_cities_2000.txt&quot;</span><span class="p">)</span>
<span class="nb">len</span><span class="p">(</span><span class="n">cities</span><span class="p">),</span> <span class="nb">sum</span><span class="p">(</span><span class="n">cities</span><span class="p">)</span>  <span class="c1"># number of cities, total population</span>
<span class="c1">## (19447, 175062893.0)</span>
</pre></div>
</div>
<p><a class="reference internal" href="#fig-hist-cities"><span class="std std-numref">Figure 6.9</span></a> gives the histogram
of the city sizes with the populations on the log-scale.
It kind of looks like a log-normal distribution again,
which the reader can inspect themself when they are feeling playful.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">cities</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">log_scale</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id38">
<span id="fig-hist-cities"></span><img alt="../_images/hist-cities-17.png" src="../_images/hist-cities-17.png" />
<figcaption>
<p><span class="caption-number">Figure 6.9 </span><span class="caption-text">Histogram of the unabridged <code class="docutils literal notranslate"><span class="pre">cities</span></code> dataset; note the log-scale on the x-axis</span><a class="headerlink" href="#id38" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>This time, however, will be interested in not what is <em>typical</em>,
but what is in some sense <em>anomalous</em> or <em>extreme</em>.
Let us look again at the <em>truncated</em> version of the city size
distribution by considering the cities with 10,000 or more inhabitants
(i.e., we will only study the right tail of the original data,
just like in <a class="reference internal" href="210-vector.html#sec-cities"><span class="std std-numref">Section 4.3.7</span></a>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="mi">10_000</span>
<span class="n">large_cities</span> <span class="o">=</span> <span class="n">cities</span><span class="p">[</span><span class="n">cities</span> <span class="o">&gt;=</span> <span class="n">s</span><span class="p">]</span>
<span class="nb">len</span><span class="p">(</span><span class="n">large_cities</span><span class="p">),</span> <span class="nb">sum</span><span class="p">(</span><span class="n">large_cities</span><span class="p">)</span>  <span class="c1"># number of cities, total population</span>
<span class="c1">## (2696, 146199374.0)</span>
</pre></div>
</div>
<p>Plotting the above on a double logarithmic scale
can be performed by passing <code class="docutils literal notranslate"><span class="pre">log_scale=(True,</span> <span class="pre">True)</span></code>
to <strong class="command">seaborn.histplot</strong>, which is left as an exercise.
Anyway, doing so will lead to a similar picture
as in <a class="reference internal" href="#fig-fit-pareto"><span class="std std-numref">Figure 6.10</span></a>.
This reveals something remarkable: the bar tops on the
double log-scale are arranged more or less in a straight line.
There are many datasets
that exhibit this behaviour; we say that they follow a <em>power law</em>
(power in the arithmetic sense, not social one);
see <span id="id12">[<a class="reference internal" href="999-bibliography.html#id40" title="A. Clauset, C.R. Shalizi, and M.E.J. Newman. Power-law distributions in empirical data. SIAM Review, 51(4):661–703, 2009. doi:10.1137/070710111.">CSN09</a>, <a class="reference internal" href="999-bibliography.html#id39" title="M.E.J. Newman. Power laws, Pareto distributions and Zipf's law. Contemporary Physics, pages 323–351, 2005. doi:10.1080/00107510500052444.">New05</a>]</span> for discussion.</p>
<p>Let us introduce the <em>Pareto distribution</em> family which has a prototypical
power law-like density. It is identified by two parameters:</p>
<ul class="simple">
<li><p>the (what <strong class="program">scipy</strong> calls it) scale parameter <span class="math notranslate nohighlight">\(s&gt;0\)</span> is equal to
the shift from 0,</p></li>
<li><p>the shape parameter, <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span>, controls the slope of the said line on
the double log-scale.</p></li>
</ul>
<p>The probability density function of P(α, s) is given for <span class="math notranslate nohighlight">\(x\ge s\)</span> by:</p>
<div class="math notranslate nohighlight">
\[
f(x) = \frac{\alpha s^\alpha}{x^{\alpha+1}},
\]</div>
<p>and <span class="math notranslate nohighlight">\(f(x)=0\)</span> otherwise.</p>
<p><span class="math notranslate nohighlight">\(s\)</span> is usually taken as the sample minimum (i.e., 10000 in our case).
<span class="math notranslate nohighlight">\(\alpha\)</span> can be estimated through the reciprocal of
the mean of the scaled logarithms of our observations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">large_cities</span><span class="o">/</span><span class="n">s</span><span class="p">))</span>
<span class="n">alpha</span>
<span class="c1">## 0.9496171695997675</span>
</pre></div>
</div>
<p>We noticed that comparing the theoretical densities and an empirical
histogram on a log-scale is quite problematic for <strong class="command">seaborn</strong>.
Therefore, we again must apply logarithmic binning
manually to generate what we see in <a class="reference internal" href="#fig-fit-pareto"><span class="std std-numref">Figure 6.10</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">geomspace</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">large_cities</span><span class="p">),</span> <span class="mi">21</span><span class="p">)</span>  <span class="c1"># bin boundaries</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">large_cities</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pareto</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s</span><span class="p">),</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;PDF of P(</span><span class="si">{</span><span class="n">alpha</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id39">
<span id="fig-fit-pareto"></span><img alt="../_images/fit-pareto-19.png" src="../_images/fit-pareto-19.png" />
<figcaption>
<p><span class="caption-number">Figure 6.10 </span><span class="caption-text">Histogram of the <code class="docutils literal notranslate"><span class="pre">large_cities</span></code> dataset and the fitted density on a double log-scale</span><a class="headerlink" href="#id39" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#fig-qq-pareto"><span class="std std-numref">Figure 6.11</span></a> gives the corresponding Q-Q plot on
a double logarithmic scale.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">qq_plot</span><span class="p">(</span>  <span class="c1"># defined above</span>
    <span class="n">large_cities</span><span class="p">,</span>
    <span class="k">lambda</span> <span class="n">q</span><span class="p">:</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pareto</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Quantiles of P(</span><span class="si">{</span><span class="n">alpha</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Sample quantiles&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id40">
<span id="fig-qq-pareto"></span><img alt="../_images/qq-pareto-21.png" src="../_images/qq-pareto-21.png" />
<figcaption>
<p><span class="caption-number">Figure 6.11 </span><span class="caption-text">Q-Q plot for the <code class="docutils literal notranslate"><span class="pre">large_cites</span></code> dataset vs the fitted Paretian model</span><a class="headerlink" href="#id40" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>We see that the populations of the largest cities are overestimated.
The model could be better, but the cities are still growing, right?</p>
<div class="proof proof-type-example" id="id41">

    <div class="proof-title">
        <span class="proof-type">Example 6.6</span>
        
    </div><div class="proof-content">
<p>(*) It might also be interesting to see how well we can predict the
probability of a randomly selected city being at least a given size.
Let us denote with <span class="math notranslate nohighlight">\(S(x)=1-F(x)\)</span> the <em>complementary
cumulative distribution function</em> (CCDF; sometimes referred to as
the survival function),
and with <span class="math notranslate nohighlight">\(\hat{S}_n(x)=1-\hat{F}_n(x)\)</span> its empirical version.
<a class="reference internal" href="#fig-ccdf-pareto"><span class="std std-numref">Figure 6.12</span></a> compares the empirical and the fitted CCDFs
with probabilities on the linear- and log-scale.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">geomspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">large_cities</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">large_cities</span><span class="p">),</span> <span class="mi">1001</span><span class="p">)</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pareto</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">s</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">large_cities</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">probs</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;CCDF of P(</span><span class="si">{</span><span class="n">alpha</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">large_cities</span><span class="p">),</span> <span class="mi">1</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">n</span><span class="p">,</span>
        <span class="n">drawstyle</span><span class="o">=</span><span class="s2">&quot;steps-post&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Empirical Complementary CDF&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;$x$&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">([</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span> <span class="s2">&quot;log&quot;</span><span class="p">][</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Prob(city size &gt; x)&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id42">
<span id="fig-ccdf-pareto"></span><img alt="../_images/ccdf-pareto-23.png" src="../_images/ccdf-pareto-23.png" />
<figcaption>
<p><span class="caption-number">Figure 6.12 </span><span class="caption-text">Empirical and theoretical complementary cumulative distribution functions for the <code class="docutils literal notranslate"><span class="pre">large_cities</span></code> dataset with probabilities on the linear- (left) and log-scale (right) and city sizes on the log-scale</span><a class="headerlink" href="#id42" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>In terms of the maximal absolute distance between the two functions,
<span class="math notranslate nohighlight">\(\hat{D}_n\)</span>, from the left plot we see that the fit looks fairly good
(let us stress that the log-scale overemphasises the relatively
minor differences in the right tail and should not be used for judging
the value of <span class="math notranslate nohighlight">\(\hat{D}_n\)</span>).</p>
<p>That the Kolmogorov–Smirnov goodness-of-fit test
rejects the hypothesis of Paretianity (at a significance level <span class="math notranslate nohighlight">\(0.1\%\)</span>)
is left as an exercise to the reader.</p>
</div></div></section>
<section id="uniform-distribution">
<h3><span class="section-number">6.3.3. </span>Uniform Distribution<a class="headerlink" href="#uniform-distribution" title="Permalink to this heading"></a></h3>
<p>Consider the Polish <em>Lotto</em> lottery, where 6 numbered balls
<span class="math notranslate nohighlight">\(\{1,2,\dots,49\}\)</span> are drawn without replacement from an urn.
We have a dataset that summarises the number of times
each ball has been drawn in all the games in the period 1957–2016.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lotto</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching_data/master/marek/lotto_table.txt&quot;</span><span class="p">)</span>
<span class="n">lotto</span>
<span class="c1">## array([720., 720., 714., 752., 719., 753., 701., 692., 716., 694., 716.,</span>
<span class="c1">##        668., 749., 713., 723., 693., 777., 747., 728., 734., 762., 729.,</span>
<span class="c1">##        695., 761., 735., 719., 754., 741., 750., 701., 744., 729., 716.,</span>
<span class="c1">##        768., 715., 735., 725., 741., 697., 713., 711., 744., 652., 683.,</span>
<span class="c1">##        744., 714., 674., 654., 681.])</span>
</pre></div>
</div>
<p>Each event seems to occur more or less with the same probability.
Of course, the numbers on the balls are integer,
but in our idealised scenario, we may try modelling this dataset
using a continuous <em>uniform distribution</em>,
which yields arbitrary real numbers on a given interval <em>(a, b)</em>,
i.e., between some <em>a</em> and <em>b</em>.
We denote such a distribution with U(<em>a</em>, <em>b</em>). It
has the probability density function given for <span class="math notranslate nohighlight">\(x\in(a, b)\)</span> by:</p>
<div class="math notranslate nohighlight">
\[
f(x) = \frac{1}{b-a},
\]</div>
<p>and <span class="math notranslate nohighlight">\(f(x)=0\)</span> otherwise.</p>
<p>Notice that <strong class="command">scipy.stats.uniform</strong>
uses parameters <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">scale</span></code> equal to <span class="math notranslate nohighlight">\(b-a\)</span> instead.</p>
<p>In our case, it makes sense to set <em>a=1</em> and <em>b=50</em> and interpret
an outcome like <em>49.1253</em> as representing the 49th ball
(compare the notion of the floor function, <span class="math notranslate nohighlight">\(\lfloor x\rfloor\)</span>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">1001</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">lotto</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">lotto</span><span class="p">),</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s2">&quot;edge&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">49</span><span class="p">),</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;PDF of U(1, 50)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id43">
<span id="fig-lotto"></span><img alt="../_images/lotto-25.png" src="../_images/lotto-25.png" />
<figcaption>
<p><span class="caption-number">Figure 6.13 </span><span class="caption-text">Histogram of the <code class="docutils literal notranslate"><span class="pre">lotto</span></code> dataset</span><a class="headerlink" href="#id43" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>Visually, see <a class="reference internal" href="#fig-lotto"><span class="std std-numref">Figure 6.13</span></a>, this model makes much sense, but again,
some more rigorous statistical testing would be required to determine
if someone has not been tampering with the lottery results, i.e.,
if data does not deviate from the uniform distribution significantly.</p>
<p>Unfortunately, we cannot use the Kolmogorov–Smirnov test in the version
defined above, because data are not continuous.
See, however, <a class="reference internal" href="420-categorical.html#sec-chisq-test"><span class="std std-numref">Section 11.4.3</span></a>
for the Pearson chi-squared test that is applicable here.</p>
<div class="proof proof-type-exercise" id="id44">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.7</span>
        
    </div><div class="proof-content">
<p>Does playing lotteries
and engaging in gambling make <em>rational</em> sense at all,
from the perspective of an individual player?
Well, we see that 16 is the most frequently occurring outcome
in <em>Lotto</em>, maybe there’s some magic in it?
Also, some people sometimes became millionaires, right?</p>
</div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>In data modelling (e.g., Bayesian statistics),
sometimes a uniform distribution is chosen
as a placeholder for “we know nothing about a phenomenon,
so let us just assume that every event is equally likely”.
Nonetheless, it is quite fascinating that the real world tends
to be structured after all. Emerging patterns are plentiful,
most often they are far from being uniformly distributed.
Even more strikingly, they are subject to quantitative analysis.</p>
</div>
</section>
<section id="distribution-mixtures">
<span id="sec-mixtures"></span><h3><span class="section-number">6.3.4. </span>Distribution Mixtures (*)<a class="headerlink" href="#distribution-mixtures" title="Permalink to this heading"></a></h3>
<p>Some datasets may fail to fit through simple models such as the ones
described above. It may sometimes be due to their non-random behaviour:
statistics gives just one means to create data idealisations,
we also have partial differential equations, approximation theory,
graphs and complex networks, agent-based modelling, and so forth,
which might be worth giving a study (and then try).</p>
<p>Another reason may be that what we observe is in fact a <em>mixture</em>
(creative combination) of simpler processes.</p>
<p>The dataset representing the December 2021 hourly averages
pedestrian counts
near the Southern Cross Station in Melbourne
might be a good instance of such a scenario,
compare <a class="reference internal" href="210-vector.html#fig-peds-histogram"><span class="std std-numref">Figure 4.5</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">peds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching_data/master/marek/southern_cross_station_peds_2019_dec.txt&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>It might not be a bad idea to
try to fit a probabilistic (convex) combination
of three normal distributions <span class="math notranslate nohighlight">\(f_1\)</span>, <span class="math notranslate nohighlight">\(f_2\)</span>, <span class="math notranslate nohighlight">\(f_3\)</span>,
corresponding to the morning, lunch-time, and evening
pedestrian count peaks. This yields the PDF:</p>
<div class="math notranslate nohighlight">
\[
f(x) = w_1 f_1(x) + w_2 f_2(x) + w_3 f_3(x),
\]</div>
<p>for some coefficients <span class="math notranslate nohighlight">\(w_1,w_2,w_3\ge 0\)</span> such that <span class="math notranslate nohighlight">\(w_1+w_2+w_3=1\)</span>.</p>
<p><a class="reference internal" href="#fig-mixture"><span class="std std-numref">Figure 6.14</span></a> depicts a mixture of N(8, 1), N(12, 1), and N(17, 2)
with the corresponding weights of 0.35, 0.1, and 0.55.
This dataset is quite coarse-grained
(we only have 24 bar heights at our disposal). Consequently,
the estimated coefficients should be taken with a pinch of chilli pepper.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">24</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="n">peds</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">peds</span><span class="p">),</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">p1</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">p2</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">p3</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.35</span><span class="o">*</span><span class="n">p1</span> <span class="o">+</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">p2</span> <span class="o">+</span> <span class="mf">0.55</span><span class="o">*</span><span class="n">p3</span>  <span class="c1"># weighted combination of 3 densities</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;PDF of a normal mixture&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id45">
<span id="fig-mixture"></span><img alt="../_images/mixture-27.png" src="../_images/mixture-27.png" />
<figcaption>
<p><span class="caption-number">Figure 6.14 </span><span class="caption-text">Histogram of the <code class="docutils literal notranslate"><span class="pre">peds</span></code> dataset and a guesstimated mixture of three normal distributions</span><a class="headerlink" href="#id45" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>It will frequently be the case in data wrangling
that more complex entities (models, methods) will be arising as combinations
of simpler (primitive) components.
This is why we should spend a great deal of time
studying the <em>fundamentals</em>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some data clustering techniques
(in particular, the <em>k</em>-means algorithm that we briefly discuss
later in this course)
could be used to split a data sample into disjoint chunks
corresponding to different mixture components.</p>
<p>Also, it might be the case that the mixture components can
in fact be explained by another categorical variable that
divides the dataset into natural groups; compare
<a class="reference internal" href="430-group-by.html#chap-group-by"><span class="std std-numref">Chapter 12</span></a>.</p>
</div>
</section>
</section>
<section id="generating-pseudorandom-numbers">
<span id="sec-pseudorandom"></span><h2><span class="section-number">6.4. </span>Generating Pseudorandom Numbers<a class="headerlink" href="#generating-pseudorandom-numbers" title="Permalink to this heading"></a></h2>
<p>A probability distribution is useful not only for describing a dataset.
It also enables us to perform many experiments on data that we do not
currently have, but we might obtain in the future,
to test various scenarios and hypotheses.</p>
<p>To do this, we can generate a random sample of
independent (not related to each other) observations.</p>
<section id="id13">
<h3><span class="section-number">6.4.1. </span>Uniform Distribution<a class="headerlink" href="#id13" title="Permalink to this heading"></a></h3>
<p>When most people say <em>random</em>, they implicitly mean
<em>uniformly distributed</em>. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="c1">## array([0.69646919, 0.28613933, 0.22685145, 0.55131477, 0.71946897])</span>
</pre></div>
</div>
<p>gives 5 observations sampled independently
from the uniform distribution on the unit interval, i.e., U(0, 1).</p>
<p>The same with <strong class="program">scipy</strong>, but this time the support will be
<em>(-10, 15)</em>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>  <span class="c1"># from -10 to -10+25</span>
<span class="c1">## array([ 0.5776615 , 14.51910496,  7.12074346,  2.02329754, -0.19706205])</span>
</pre></div>
</div>
<p>Alternatively, we could do that ourselves by shifting
and scaling the output of the random number generator
on the unit interval using the formula
<strong class="command">numpy.random.rand</strong><code class="code docutils literal notranslate"><span class="pre">(5)*25-10</span></code>.</p>
</section>
<section id="not-exactly-random">
<span id="sec-seed"></span><h3><span class="section-number">6.4.2. </span>Not Exactly Random<a class="headerlink" href="#not-exactly-random" title="Permalink to this heading"></a></h3>
<p>We generate numbers using a computer, which is purely
deterministic. Hence, we shall refer to them as <em>pseudorandom</em>
or random-like ones (albeit they are non-distinguishable from truly random,
when subject to rigorous tests for randomness).</p>
<p>To prove it, we can set the initial state of the generator
(the <em>seed</em>) via some number and see what values are produced:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>  <span class="c1"># set seed</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="c1">## array([0.69646919, 0.28613933, 0.22685145, 0.55131477, 0.71946897])</span>
</pre></div>
</div>
<p>Then, we set the seed once again via the same number and
see how “random” the next values are:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>  <span class="c1"># set seed</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="c1">## array([0.69646919, 0.28613933, 0.22685145, 0.55131477, 0.71946897])</span>
</pre></div>
</div>
<p>This enables us to perform completely <em>reproducible</em> numerical
experiments, and this is a very good feature: truly scientific
inquiries should lead to identical results under the same conditions.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If we do not set the seed manually,
it will be initialised based on the current wall time, which is
different every… time. As a result, the numbers will <em>seem</em> random to us.</p>
</div>
<p>Many Python packages that we will be using in the future,
including <strong class="program">pandas</strong> and <strong class="program">sklearn</strong>, rely
on <strong class="program">numpy</strong>’s random number generator.
We will become used to calling <strong class="command">numpy.random.seed</strong>
to make them predictable.</p>
<p>Additionally, some of them
(e.g., <strong class="command">sklearn.model_selection.train_test_split</strong>
or <strong class="command">pandas.DataFrame.sample</strong>) are equipped with the <code class="docutils literal notranslate"><span class="pre">random_state</span></code>
argument, which behaves as if we <em>temporarily</em> changed
the seed (for just one call to that function). For instance:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="c1">## array([0.69646919, 0.28613933, 0.22685145, 0.55131477, 0.71946897])</span>
</pre></div>
</div>
<p>This gives the same sequence as above.</p>
</section>
<section id="sampling-from-other-distributions">
<h3><span class="section-number">6.4.3. </span>Sampling from Other Distributions<a class="headerlink" href="#sampling-from-other-distributions" title="Permalink to this heading"></a></h3>
<p>Generating data from other distributions is possible too;
there are many <strong class="command">rvs</strong> methods implemented
in <strong class="program">scipy.stats</strong>.
For example, here is a sample from N(100, 16):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">50489</span><span class="p">)</span>
<span class="c1">## array([113.41134015,  46.99328545, 157.1304154 ])</span>
</pre></div>
</div>
<p>Pseudorandom deviates from the <em>standard</em> normal distribution,
i.e., N(0, 1), can also be generated using <strong class="command">numpy.random.randn</strong>.
As N(100, 16) is a scaled and shifted version thereof,
the above is equivalent to:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">50489</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">*</span><span class="mi">16</span> <span class="o">+</span> <span class="mi">100</span>
<span class="c1">## array([113.41134015,  46.99328545, 157.1304154 ])</span>
</pre></div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Conclusions based on simulated data are trustworthy,
because they cannot be manipulated.
Or can they?</p>
<p>The pseudorandom number generator’s seed used above,
<code class="docutils literal notranslate"><span class="pre">50489</span></code>, is quite suspicious. It might suggest that someone
wanted to <em>prove</em> some point (in this case, the violation
of the 3σ rule).</p>
<p>This is why we recommend sticking to only one seed most of the time,
e.g., <code class="docutils literal notranslate"><span class="pre">123</span></code>, or – when performing simulations – setting
consecutive seeds for each iteration: <code class="docutils literal notranslate"><span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">2</span></code>, ….</p>
</div>
<div class="proof proof-type-exercise" id="id46">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.8</span>
        
    </div><div class="proof-content">
<p>Generate 1000 pseudorandom numbers from the log-normal
distribution and draw a histogram thereof.</p>
</div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>(*)
Having a good pseudorandom number generator from the uniform distribution
on the unit interval is crucial, because sampling from other distributions
usually involves transforming independent U(0, 1) variates.</p>
<p>For instance, realisations of random variables following any continuous
cumulative distribution function <span class="math notranslate nohighlight">\(F\)</span> can be constructed through
the <em>inverse transform sampling</em> (see <span id="id14">[<a class="reference internal" href="999-bibliography.html#id46" title="J.E. Gentle. Random Number Generation and Monte Carlo Methods. Springer-Verlag, 2003.">Gen03</a>, <a class="reference internal" href="999-bibliography.html#id48" title="C.P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer-Verlag, 2004.">RC04</a>]</span>):</p>
<ol class="arabic simple">
<li><p>Generate a sample <span class="math notranslate nohighlight">\(x_1,\dots,x_n\)</span> independently from U(0, 1).</p></li>
<li><p>Transform each <span class="math notranslate nohighlight">\(x_i\)</span> by applying the quantile function,
<span class="math notranslate nohighlight">\(y_i=F^{-1}(x_i)\)</span>.</p></li>
</ol>
<p>Now <span class="math notranslate nohighlight">\(y_1,\dots,y_n\)</span> follows the CDF <span class="math notranslate nohighlight">\(F\)</span>.</p>
</div>
<div class="proof proof-type-exercise" id="id47">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.9</span>
        
    </div><div class="proof-content">
<p>(*) Generate 1000 pseudorandom numbers from the log-normal
distribution using inverse transform sampling.</p>
</div></div><div class="proof proof-type-exercise" id="id48">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.10</span>
        
    </div><div class="proof-content">
<p>(**) Generate 1000 pseudorandom numbers from the distribution
mixture discussed in <a class="reference internal" href="#sec-mixtures"><span class="std std-numref">Section 6.3.4</span></a>.</p>
</div></div></section>
<section id="natural-variability">
<span id="sec-natural-variability"></span><h3><span class="section-number">6.4.4. </span>Natural Variability<a class="headerlink" href="#natural-variability" title="Permalink to this heading"></a></h3>
<p>Even a sample truly generated from a specific distribution
will deviate from it, sometimes considerably.
Such effects will be especially visible for small sample sizes,
but they usually disappear<a class="footnote-reference brackets" href="#footfts" id="id15">9</a> when the availability of data increases.</p>
<p>For example, <a class="reference internal" href="#fig-natural-variability"><span class="std std-numref">Figure 6.15</span></a> depicts the histograms of nine
different samples of size 100, all drawn independently from the
standard normal distribution.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.figsize&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># width=height</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id49">
<span id="fig-natural-variability"></span><img alt="../_images/natural-variability-29.png" src="../_images/natural-variability-29.png" />
<figcaption>
<p><span class="caption-number">Figure 6.15 </span><span class="caption-text">All nine samples are normally distributed</span><a class="headerlink" href="#id49" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>There is some ruggedness in the bar sizes that a naïve observer
might try to interpret as something meaningful.
A competent data scientist must train
their eye to ignore such impurities
(but should always be ready to detect those which are worth attention).
In this case, they are only due to random effects.</p>
<div class="proof proof-type-exercise" id="id50">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.11</span>
        
    </div><div class="proof-content">
<p>Repeat the above experiment for samples of sizes 10, 1000, and 10000.</p>
</div></div><div class="proof proof-type-example" id="id51">

    <div class="proof-title">
        <span class="proof-type">Example 6.12</span>
        
    </div><div class="proof-content">
<p>(*) Using a simple Monte Carlo simulation,
we can verify (approximately) that the Kolmogorov–Smirnov
goodness-of-fit test introduced in <a class="reference internal" href="#sec-ks-test"><span class="std std-numref">Section 6.2.3</span></a>
has been calibrated properly,
i.e., that for samples that really follow the assumed distribution,
the null hypothesis is rejected only in ca. 0.1% of the cases.</p>
<p>Let us say we are interested in the null hypothesis
referencing the standard normal distribution, N(0, 1),
and sample size <span class="math notranslate nohighlight">\(n=100\)</span>.
We need to generate many (we assume 10,000 below) such samples for each of
which we compute and store the maximal absolute deviation from
the theoretical CDF, i.e., <span class="math notranslate nohighlight">\(\hat{D}_n\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">distrib</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># assumed distribution - N(0, 1)</span>
<span class="n">Dns</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>  <span class="c1"># increase this for better precision</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">distrib</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># really follows distrib</span>
    <span class="n">Dns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">compute_Dn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">distrib</span><span class="o">.</span><span class="n">cdf</span><span class="p">))</span>
<span class="n">Dns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Dns</span><span class="p">)</span>
</pre></div>
</div>
<p>Now let us compute the proportion of cases which lead to
<span class="math notranslate nohighlight">\(\hat{D}_n\)</span> greater than the critical value <span class="math notranslate nohighlight">\(K_n\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">Dns</span><span class="p">[</span><span class="n">Dns</span> <span class="o">&gt;=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">kstwo</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">n</span><span class="p">)])</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">Dns</span><span class="p">)</span>
<span class="c1">## 0.0016</span>
</pre></div>
</div>
<p>In theory, this should be equal to 0.001.
But our values are necessarily approximate, because we rely on randomness.
Increasing the number of trials from 10,000 to, say, 1,000,000
will make the above estimate more precise.</p>
<p>It is also worth checking out that
the density histogram of <code class="docutils literal notranslate"><span class="pre">Dns</span></code> resembles the Kolmogorov distribution
that we can compute via <strong class="command">scipy.stats.kstwo.pdf</strong>.</p>
</div></div><div class="proof proof-type-exercise" id="id52">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.13</span>
        
    </div><div class="proof-content">
<p>(*) It might also be interesting to check out the test’s <em>power</em>,
i.e., the probability that when the null hypothesis is false,
it will actually be rejected.
Modify the above code in such a way that <code class="docutils literal notranslate"><span class="pre">x</span></code> in the <strong class="command">for</strong> loop
is not generated from N(0, 1), but N(0.1, 1), N(0.2, 1), etc.,
and check the proportion of cases
where we deem the sample distribution different from N(0, 1).
Small differences in the location parameter <span class="math notranslate nohighlight">\(\mu\)</span> are usually
ignored, and this improves with sample size <span class="math notranslate nohighlight">\(n\)</span>.</p>
</div></div></section>
<section id="adding-jitter-white-noise">
<h3><span class="section-number">6.4.5. </span>Adding Jitter (White Noise)<a class="headerlink" href="#adding-jitter-white-noise" title="Permalink to this heading"></a></h3>
<p>We mentioned that measurements might be subject to observational
error. Rounding can also occur as early as
the data collection phase. In particular, our <code class="docutils literal notranslate"><span class="pre">heights</span></code>
dataset is precise up to 1 fractional digit.
However, in statistics, when we say that data follow
a continuous distribution, the probability of having two identical
values in a sample is 0. Therefore, some data analysis methods
might assume that there are no ties in the input vector, i.e.,
all values are unique.</p>
<p>The easiest way to deal with such numerical inconveniences
is to add some white noise with the expected value of 0,
either uniformly or normally distributed.</p>
<p>For example, for <code class="docutils literal notranslate"><span class="pre">heights</span></code> it makes sense to add some jitter from
U[-0.05, 0.05]:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">heights_jitter</span> <span class="o">=</span> <span class="n">heights</span> <span class="o">+</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">heights</span><span class="p">))</span><span class="o">*</span><span class="mf">0.1</span><span class="o">-</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">heights_jitter</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>  <span class="c1"># preview</span>
<span class="c1">## array([160.21704623, 152.68870195, 161.24482407, 157.3675293 ,</span>
<span class="c1">##        154.61663465, 144.68964596])</span>
</pre></div>
</div>
<p>Adding noise also might be performed for aesthetic reasons,
e.g., when drawing scatterplots.</p>
</section>
</section>
<section id="further-reading">
<h2><span class="section-number">6.5. </span>Further Reading<a class="headerlink" href="#further-reading" title="Permalink to this heading"></a></h2>
<p>For an excellent general introductory course on probability
and statistics, see <span id="id16">[<a class="reference internal" href="999-bibliography.html#id47" title="J.E. Gentle. Computational Statistics. Springer-Verlag, 2009.">Gen09</a>, <a class="reference internal" href="999-bibliography.html#id42" title="J.E. Gentle. Theory of Statistics. book draft, 2020. URL: https://mason.gmu.edu/~jgentle/books/MathStat.pdf.">Gen20</a>]</span>.
More advanced students are likely to enjoy the two
classics <span id="id17">[<a class="reference internal" href="999-bibliography.html#id43" title="P. Billingsley. Probability and Measure. John Wiley &amp; Sons, 1995.">Bil95</a>, <a class="reference internal" href="999-bibliography.html#id41" title="H. Cramér. Mathematical Methods of Statistics. Princeton University Press, 1946. URL: https://archive.org/details/in.ernet.dli.2015.223699.">Cra46</a>]</span>.
Topics in random number generation are covered in
<span id="id18">[<a class="reference internal" href="999-bibliography.html#id46" title="J.E. Gentle. Random Number Generation and Monte Carlo Methods. Springer-Verlag, 2003.">Gen03</a>, <a class="reference internal" href="999-bibliography.html#id139" title="D.E. Knuth. The Art of Computer Programming II: Seminumerical Algorithms. Addison-Wesley, 1997.">Knu97</a>, <a class="reference internal" href="999-bibliography.html#id48" title="C.P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer-Verlag, 2004.">RC04</a>]</span>.</p>
<p>For a more comprehensive introduction to exploratory data analysis,
see the classical books by Tukey <span id="id19">[<a class="reference internal" href="999-bibliography.html#id143" title="J.W. Tukey. The future of data analysis. Annals of Mathematical Statistics, 33(1):1–67, 1962. URL: https://projecteuclid.org/journalArticle/Download?urlId=10.1214%2Faoms%2F1177704711, doi:10.1214/aoms/1177704711.">Tuk62</a>, <a class="reference internal" href="999-bibliography.html#id142" title="J.W. Tukey. Exploratory Data Analysis. Addison-Wesley, 1977.">Tuk77</a>]</span>
and Tufte <span id="id20">[<a class="reference internal" href="999-bibliography.html#id141" title="E.R. Tufte. The Visual Display of Quantitative Information. Graphics Press, 2001.">Tuf01</a>]</span>.</p>
<div style="margin-top: 1em"></div><p>We took the logarithm of the log-normally distributed
incomes and obtained a normally distributed sample.
In statistical practice, it is not rare to apply different non-linear
transforms of the input vectors at the data preprocessing stage (see, e.g.,
<a class="reference internal" href="330-relationship.html#sec-linearisation"><span class="std std-numref">Section 9.2.6</span></a>). In particular, the Box–Cox (power) transform
<span id="id21">[<a class="reference internal" href="999-bibliography.html#id144" title="G.E.P. Box and D.R. Cox. An analysis of transformations. Journal of the Royal Statistical Society. Series B (Methodological), 26(2):211–252, 1964.">BC64</a>]</span> is of the form <span class="math notranslate nohighlight">\(x\mapsto \frac{x^\lambda-1}{\lambda}\)</span>
for some <span class="math notranslate nohighlight">\(\lambda\)</span>.
Interestingly, in the limit as <span class="math notranslate nohighlight">\(\lambda\to 0\)</span>, this formula yields
<span class="math notranslate nohighlight">\(x\mapsto \log x\)</span> which is exactly what we were applying in this chapter.</p>
<p><span id="id22">[<a class="reference internal" href="999-bibliography.html#id40" title="A. Clauset, C.R. Shalizi, and M.E.J. Newman. Power-law distributions in empirical data. SIAM Review, 51(4):661–703, 2009. doi:10.1137/070710111.">CSN09</a>, <a class="reference internal" href="999-bibliography.html#id39" title="M.E.J. Newman. Power laws, Pareto distributions and Zipf's law. Contemporary Physics, pages 323–351, 2005. doi:10.1080/00107510500052444.">New05</a>]</span> give a nice overview of the power-law-like
behaviour of some “rich” or otherwise extreme datasets.
It is worth noting that the logarithm of a Paretian sample
divided by the minimum follows an exponential distribution
(which we discuss in <a class="reference internal" href="530-time-series.html#chap-time-series"><span class="std std-numref">Chapter 16</span></a>).
For a comprehensive catalogue of statistical distributions,
their properties, and relationships between them,
see <span id="id23">[<a class="reference internal" href="999-bibliography.html#id81" title="C. Forbes, M. Evans, N. Hastings, and B. Peacock. Statistical Distributions. Wiley, 2010.">FEHP10</a>]</span>.</p>
</section>
<section id="exercises">
<h2><span class="section-number">6.6. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this heading"></a></h2>
<div class="proof proof-type-exercise" id="id53">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.14</span>
        
    </div><div class="proof-content">
<p>Why is the notion of the mean income confusing the general public?</p>
</div></div><div class="proof proof-type-exercise" id="id54">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.15</span>
        
    </div><div class="proof-content">
<p>When manually setting the seed of a random number generator
makes sense?</p>
</div></div><div class="proof proof-type-exercise" id="id55">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.16</span>
        
    </div><div class="proof-content">
<p>Given a log-normally distributed sample <code class="docutils literal notranslate"><span class="pre">x</span></code>, how  can we turn it
to a normally distributed one, i.e., <code class="docutils literal notranslate"><span class="pre">y=</span></code><strong class="command">f</strong><code class="code docutils literal notranslate"><span class="pre">(x)</span></code>, with <strong class="command">f</strong> being… what?</p>
</div></div><div class="proof proof-type-exercise" id="id56">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.17</span>
        
    </div><div class="proof-content">
<p>What is the 3σ rule for normally distributed data?</p>
</div></div><div class="proof proof-type-exercise" id="id57">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.18</span>
        
    </div><div class="proof-content">
<p>(*) How can we verify graphically if a sample follows a hypothesised
theoretical distribution?</p>
</div></div><div class="proof proof-type-exercise" id="id58">

    <div class="proof-title">
        <span class="proof-type">Exercise 6.19</span>
        
    </div><div class="proof-content">
<p>(*) Explain the meaning of type I error, significance level,
and a test’s power.</p>
</div></div><hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="foothistconvergence"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>(*) This intuition is of course theoretically
grounded and is based on the asymptotic behaviour of the histograms
as the estimators of the underlying probability density function,
see, e.g., <span id="id24">[<a class="reference internal" href="999-bibliography.html#id45" title="D. Freedman and P. Diaconis. On the histogram as a density estimator: L₂ theory. Zeitschrift für Wahrscheinlichkeitstheorie und Verwandte Gebiete, 57:453–476, 1981.">FD81</a>]</span> and the many references therein.</p>
</dd>
<dt class="label" id="footmle"><span class="brackets"><a class="fn-backref" href="#id4">2</a></span></dt>
<dd><p>(*) It might be the case that we will have to obtain
the estimates of the probability distribution’s parameters
by numerical optimisation, for example, by minimising
<span class="math notranslate nohighlight">\(
\mathcal{L}(\mu, \sigma) = \sum_{i=1}^n \left(
    \frac{(x_i-\mu)^2}{\sigma^2} + \log \sigma^2
\right)
\)</span>
with respect to <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> (corresponding to the objective
function in the maximum likelihood estimation problem for the normal
distribution family).  In our case, however, we are lucky;
there exist open-form formulae expressing the solution to the above,
exactly in the form of the sample mean and standard
deviation. For other distributions, things can get a little trickier,
though. Furthermore, sometimes we will have many options for point
estimators to choose from, which might be more suitable if data are
not of top quality (e.g., contain outliers). For instance, in the normal
model, it can be shown that we can also estimate <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> via
the sample median and <span class="math notranslate nohighlight">\(\mathrm{IQR}/1.349\)</span>.</p>
</dd>
<dt class="label" id="footcdf"><span class="brackets"><a class="fn-backref" href="#id5">3</a></span></dt>
<dd><p>The probability distribution of any
real-valued random variable <span class="math notranslate nohighlight">\(X\)</span> can be uniquely defined
by means of a nondecreasing, right (upward) continuous
function <span class="math notranslate nohighlight">\(F:\mathbb{R}\to[0,1]\)</span> such that
<span class="math notranslate nohighlight">\(\lim_{x\to-\infty} F(x)=0\)</span> and <span class="math notranslate nohighlight">\(\lim_{x\to\infty} F(x)=1\)</span>,
in which case <span class="math notranslate nohighlight">\(\Pr(X\le x)=F(x)\)</span>.
The probability density function only exists for continuous
random variables and is defined as the derivative of <span class="math notranslate nohighlight">\(F\)</span>.</p>
</dd>
<dt class="label" id="footsuffsmall"><span class="brackets"><a class="fn-backref" href="#id6">4</a></span></dt>
<dd><p>The larger the sample size, the less tolerant
we should be regarding the size of this disparity;
see <a class="reference internal" href="#sec-ks-test"><span class="std std-numref">Section 6.2.3</span></a>.</p>
</dd>
<dt class="label" id="footprobplot"><span class="brackets"><a class="fn-backref" href="#id8">5</a></span></dt>
<dd><p>(*) <strong class="command">scipy.stats.probplot</strong> uses a slightly
different definition (there are many other ones in common use).</p>
</dd>
<dt class="label" id="footqqplotpearson"><span class="brackets"><a class="fn-backref" href="#id9">6</a></span></dt>
<dd><p>(*) We can quantify (informally)
the goodness of fit by using the Pearson linear correlation coefficient;
see <a class="reference internal" href="330-relationship.html#sec-pearson"><span class="std std-numref">Section 9.1.1</span></a>.</p>
</dd>
<dt class="label" id="footsmallalpha"><span class="brackets"><a class="fn-backref" href="#id10">7</a></span></dt>
<dd><p>See <a class="reference internal" href="430-group-by.html#sec-ks-test2"><span class="std std-numref">Section 12.2.6</span></a> for more details.</p>
</dd>
<dt class="label" id="footmost"><span class="brackets"><a class="fn-backref" href="#id11">8</a></span></dt>
<dd><p>Except for the few richest, who are interesting on their
own; see <a class="reference internal" href="#sec-pareto"><span class="std std-numref">Section 6.3.2</span></a> where we discuss the Pareto distribution.</p>
</dd>
<dt class="label" id="footfts"><span class="brackets"><a class="fn-backref" href="#id15">9</a></span></dt>
<dd><p>Compare the Fundamental Theorem of Statistics
(the Glivenko–Cantelli theorem).</p>
</dd>
</dl>
</section>
</section>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="310-matrix.html" class="btn btn-neutral float-right" title="7. Multidimensional Numeric Data at a Glance" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="220-transform-vector.html" class="btn btn-neutral float-left" title="5. Processing Unidimensional Data" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        Copyright &#169; 2022 by <a href="https://www.gagolewski.com">Marek Gagolewski</a>. Some rights reserved. Licensed under <a href='https://creativecommons.org/licenses/by-nc-nd/4.0/'>CC BY-NC-ND 4.0</a>.

    Built with <a href="https://sphinx-doc.org/">Sphinx</a>
    and a customised <a href="https://github.com/rtfd/sphinx_rtd_theme">rtd</a>
    theme.
      <span class="lastupdated">
        Last updated on 2022-08-09T18:19:33+1000.
      </span>


    This site will never display any ads: it is a non-profit project.
    It does not collect any data.

    </p>
  </div> 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>