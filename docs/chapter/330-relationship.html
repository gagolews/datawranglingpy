<!DOCTYPE html>


<html class="writer-html5" lang="en" >
<!-- Copyright (C) 2020-2022, Marek Gagolewski <https://www.gagolewski.com> -->

<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>9. Exploring Relationships Between Variables &mdash; Minimalist Data Wrangling with Python by Marek Gagolewski</title>
  

  
  
  <script async defer src="https://buttons.github.io/buttons.js"></script>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/proof.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />

  
  

  
  

  
    <link rel="canonical" href="https://datawranglingpy.gagolewski.com/chapter/330-relationship.html" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/proof.js"></script>
        <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="10. Introducing Data Frames" href="410-data-frame.html" />
    <link rel="prev" title="8. Processing Multidimensional Data" href="320-transform-matrix.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html"> Minimalist Data Wrangling with Python [v0.5.2]
          

          
          </a>

          <div class="version">
          by <a style="color: inherit" href="https://www.gagolewski.com">Marek Gagolewski</a>
          </div>

<!--
          
            
            
              <div class="version">
                by Marek Gagolewski
              </div>
            
          
-->

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search phrase..." />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Start Here</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="000-preface.html">Preface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introducing Python</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="110-setup.html">1. Getting Started with Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="120-scalar.html">2. Scalar Types and Control Structures in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="130-sequential.html">3. Sequential and Other Types in Python</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Unidimensional Data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="210-vector.html">4. Unidimensional Numeric Data and Their Empirical Distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="220-transform-vector.html">5. Processing Unidimensional Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="230-distribution.html">6. Continuous Probability Distributions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multidimensional Data</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="310-matrix.html">7. Multidimensional Numeric Data at a Glance</a></li>
<li class="toctree-l1"><a class="reference internal" href="320-transform-matrix.html">8. Processing Multidimensional Data</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">9. Exploring Relationships Between Variables</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#measuring-correlation">9.1. Measuring Correlation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pearson-s-linear-correlation-coefficient">9.1.1. Pearson’s Linear Correlation Coefficient</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#perfect-linear-correlation">9.1.1.1. Perfect Linear Correlation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#strong-linear-correlation">9.1.1.2. Strong Linear Correlation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#no-linear-correlation-does-not-imply-independence">9.1.1.3. No Linear Correlation Does Not Imply Independence</a></li>
<li class="toctree-l4"><a class="reference internal" href="#false-linear-correlations">9.1.1.4. False Linear Correlations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#correlation-is-not-causation">9.1.1.5. Correlation Is Not Causation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#correlation-heatmap">9.1.2. Correlation Heatmap</a></li>
<li class="toctree-l3"><a class="reference internal" href="#linear-correlation-coefficients-on-transformed-data">9.1.3. Linear Correlation Coefficients on Transformed Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#spearman-s-rank-correlation-coefficient">9.1.4. Spearman’s Rank Correlation Coefficient</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#regression-tasks">9.2. Regression Tasks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#k-nearest-neighbour-regression">9.2.1. <em>K</em>-Nearest Neighbour Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#from-data-to-linear-models">9.2.2. From Data to (Linear) Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#least-squares-method">9.2.3. Least Squares Method</a></li>
<li class="toctree-l3"><a class="reference internal" href="#analysis-of-residuals">9.2.4. Analysis of Residuals</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multiple-regression">9.2.5. Multiple Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#variable-transformation-and-linearisable-models">9.2.6. Variable Transformation and Linearisable Models (*)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#descriptive-vs-predictive-power">9.2.7. Descriptive vs Predictive Power (*)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#fitting-regression-models-with-scikit-learn">9.2.8. Fitting Regression Models with <strong class="program">scikit-learn</strong> (*)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#ill-conditioned-model-matrices">9.2.9. Ill-Conditioned Model Matrices (*)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#finding-interesting-combinations-of-variables">9.3. Finding Interesting Combinations of Variables (*)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dot-products-angles-collinearity-and-orthogonality">9.3.1. Dot Products, Angles, Collinearity, and Orthogonality</a></li>
<li class="toctree-l3"><a class="reference internal" href="#geometric-transformations-of-points">9.3.2. Geometric Transformations of Points</a></li>
<li class="toctree-l3"><a class="reference internal" href="#matrix-inverse">9.3.3. Matrix Inverse</a></li>
<li class="toctree-l3"><a class="reference internal" href="#singular-value-decomposition">9.3.4. Singular Value Decomposition</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dimensionality-reduction-with-svd">9.3.5. Dimensionality Reduction with SVD</a></li>
<li class="toctree-l3"><a class="reference internal" href="#principal-component-analysis">9.3.6. Principal Component Analysis</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#further-reading">9.4. Further Reading</a></li>
<li class="toctree-l2"><a class="reference internal" href="#exercises">9.5. Exercises</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Heterogeneous Data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="410-data-frame.html">10. Introducing Data Frames</a></li>
<li class="toctree-l1"><a class="reference internal" href="420-categorical.html">11. Handling Categorical Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="430-group-by.html">12. Processing Data in Groups</a></li>
<li class="toctree-l1"><a class="reference internal" href="440-sql.html">13. Accessing Databases (An Interlude)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Data Types</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="510-text.html">14. Text Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="520-missingness.html">15. Missing, Censored, and Questionable Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="530-time-series.html">16. Time Series</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="998-changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="999-bibliography.html">Bibliography</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Links</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://datawranglingpy.gagolewski.com/datawranglingpy.pdf">This Book in PDF</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/gagolews/datawranglingpy/blob/master/CODE_OF_CONDUCT.md">Report Bugs or Typos</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/gagolews/teaching_data">Datasets</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.gagolewski.com">Author</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Minimalist Data Wrangling with Python [v0.5.2]</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          



















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li><span class="section-number">9. </span>Exploring Relationships Between Variables</li>
    
    
      <li class="wy-breadcrumbs-aside">

        
        
        <a class="github-button" href="https://github.com/gagolews/datawranglingpy" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star gagolews/datawranglingpy on GitHub">Star</a>
        


        
      </li>
    
  </ul>

  
  <div class="rst-breadcrumbs-buttons" role="navigation" aria-label="breadcrumb navigation">
      
        <a href="410-data-frame.html" class="btn btn-neutral float-right" title="10. Introducing Data Frames" accesskey="n">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
      
      
        <a href="320-transform-matrix.html" class="btn btn-neutral float-left" title="8. Processing Multidimensional Data" accesskey="p"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
      
  </div>
  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="tex2jax_ignore mathjax_ignore section" id="exploring-relationships-between-variables">
<span id="chap-relationship"></span><h1><span class="section-number">9. </span>Exploring Relationships Between Variables<a class="headerlink" href="#exploring-relationships-between-variables" title="Permalink to this headline"></a></h1>
<blockquote>
<div><p><em>The online version of the open access textbook</em> Minimalist Data
Wrangling with Python <em>by <a class="reference external" href="https://www.gagolewski.com">Marek Gagolewski</a>
is, and will remain, freely available for everyone’s enjoyment
(also in <a class="reference external" href="https://datawranglingpy.gagolewski.com/datawranglingpy.pdf">PDF</a>).
Any <a class="reference external" href="https://github.com/gagolews/datawranglingpy/blob/master/CODE_OF_CONDUCT.md">bug/typos reports/fixes</a>
are appreciated. Although available online, this is a whole course,
and should be read from the beginning to the end. In particular,
refer to the <a class="reference internal" href="000-preface.html#chap-preface"><span class="std std-ref">Preface</span></a> for general introductory remarks.</em></p>
</div></blockquote>
<p>Let us consider the National Health and Nutrition Examination Survey
(NHANES study) excerpt once again:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">body</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching_data/master/marek/nhanes_adult_female_bmx_2020.csv&quot;</span><span class="p">,</span>
    <span class="n">comment</span><span class="o">=</span><span class="s2">&quot;#&quot;</span><span class="p">)</span>
<span class="n">body</span> <span class="o">=</span> <span class="n">body</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>  <span class="c1"># data frames will be covered later</span>
<span class="n">body</span><span class="o">.</span><span class="n">shape</span>
<span class="c1">## (4221, 7)</span>
<span class="n">body</span><span class="p">[:</span><span class="mi">6</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># 6 first rows, all columns</span>
<span class="c1">## array([[ 97.1, 160.2,  34.7,  40.8,  35.8, 126.1, 117.9],</span>
<span class="c1">##        [ 91.1, 152.7,  33.5,  33. ,  38.5, 125.5, 103.1],</span>
<span class="c1">##        [ 73. , 161.2,  37.4,  38. ,  31.8, 106.2,  92. ],</span>
<span class="c1">##        [ 61.7, 157.4,  38. ,  34.7,  29. , 101. ,  90.5],</span>
<span class="c1">##        [ 55.4, 154.6,  34.6,  34. ,  28.3,  92.5,  73.2],</span>
<span class="c1">##        [ 62. , 144.7,  32.5,  34.2,  29.8, 106.7,  84.8]])</span>
</pre></div>
</div>
<p>We thus have <em>n=4221</em> participants and <em>7</em> different
features describing them, in this order:</p>
<ol class="simple">
<li><p>weight (kg),</p></li>
<li><p>standing height (cm),</p></li>
<li><p>upper arm length (cm),</p></li>
<li><p>upper leg length (cm),</p></li>
<li><p>arm circumference (cm),</p></li>
<li><p>hip circumference (cm),</p></li>
<li><p>waist circumference (cm).</p></li>
</ol>
<p>We expect the data in columns to be <em>related</em> to each other
(e.g., a taller person <em>usually tends to</em> weight more).
This is why in this chapter we are interested in quantifying
the degree of association and modelling functional relationships
between the variables
as well as finding new interesting combinations thereof.</p>
<div class="section" id="measuring-correlation">
<h2><span class="section-number">9.1. </span>Measuring Correlation<a class="headerlink" href="#measuring-correlation" title="Permalink to this headline"></a></h2>
<p>Scatterplots let us identify some simple patterns or structure in data.
From <a class="reference internal" href="310-matrix.html#fig-body-pairplot"><span class="std std-numref">Figure 7.4</span></a>, we can note that higher hip
circumferences <em>tend to</em> occur more often
together with higher arm circumferences
and that the latter does not really tell us anything
about height.</p>
<p>Let us explore some basic means for measuring
(expressing as a single number) the degree of association
between a set of pairs of points.</p>
<div class="section" id="pearson-s-linear-correlation-coefficient">
<h3><span class="section-number">9.1.1. </span>Pearson’s Linear Correlation Coefficient<a class="headerlink" href="#pearson-s-linear-correlation-coefficient" title="Permalink to this headline"></a></h3>
<p>First, the
Pearson’s <em>linear correlation</em> coefficient:</p>
<div class="math notranslate nohighlight">
\[
r(\boldsymbol{x}, \boldsymbol{y}) = \frac{1}{n} \sum_{i=1}^n
   \frac{x_i - \bar{x}}{s_{x}}
\,
   \frac{y_i - \bar{y}}{s_{y}},
\]</div>
<p>with <span class="math notranslate nohighlight">\(s_x, s_y\)</span> denoting the standard deviations
and <span class="math notranslate nohighlight">\(\bar{x}, \bar{y}\)</span> being the means of
<span class="math notranslate nohighlight">\(\boldsymbol{x}=(x_1,\dots,x_n)\)</span> and
<span class="math notranslate nohighlight">\(\boldsymbol{y}=(y_1,\dots,y_n)\)</span>, respectively.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Look carefully: we are computing pairwise
products of standardised versions of the two vectors.
It is a normalised measure of how they <em>vary</em> together
(co-variance).</p>
<p>(*) Furthermore, in <a class="reference internal" href="#sec-dot-cosine"><span class="std std-numref">Section 9.3.1</span></a> we will note that
it is nothing else as the cosine of the angle between
centred and normalised versions of the vectors.</p>
</div>
<p>Here is how we can compute it manually:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">body</span><span class="p">[:,</span> <span class="mi">4</span><span class="p">]</span>  <span class="c1"># arm circumference</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">body</span><span class="p">[:,</span> <span class="mi">5</span><span class="p">]</span>  <span class="c1"># hip circumference</span>
<span class="n">x_std</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y_std</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x_std</span><span class="o">*</span><span class="n">y_std</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="c1">## 0.8680627457873239</span>
</pre></div>
</div>
<p>And here is a built-in function that implements the same formula:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pearsonr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1">## 0.8680627457873241</span>
</pre></div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Basic properties of Pearson’s <em>r</em> include:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(r(\boldsymbol{x}, \boldsymbol{y}) = r(\boldsymbol{y}, \boldsymbol{x})\)</span>
(symmetric);</p></li>
<li><p><span class="math notranslate nohighlight">\(|r(\boldsymbol{x}, \boldsymbol{y})| \le 1\)</span>
(bounded from below by -1 and from above by 1);</p></li>
<li><p><span class="math notranslate nohighlight">\(r(\boldsymbol{x}, \boldsymbol{y})=1\)</span> if and only if
<span class="math notranslate nohighlight">\(\boldsymbol{y}=a\boldsymbol{x}+b\)</span> for some <span class="math notranslate nohighlight">\(a&gt;0\)</span> and <span class="math notranslate nohighlight">\(b\)</span>,
(reaches the maximum when one variable is an increasing
linear function of the other one);</p></li>
<li><p><span class="math notranslate nohighlight">\(r(\boldsymbol{x}, -\boldsymbol{y})=-r(\boldsymbol{x}, \boldsymbol{y})\)</span>
(negative scaling (reflection) of one variable changes the sign of the
coefficient);</p></li>
<li><p><span class="math notranslate nohighlight">\(r(\boldsymbol{x}, a\boldsymbol{y}+b)=r(\boldsymbol{x}, \boldsymbol{y})\)</span>
for any <span class="math notranslate nohighlight">\(a&gt;0\)</span> and <span class="math notranslate nohighlight">\(b\)</span> (invariant to translation and scaling of inputs
that does not change the sign of elements).</p></li>
</ol>
</div>
<p>To get more insight, below we shall illustrate some interesting
<em>correlations</em> using the following function
that draws a scatter plot and prints out Pearson’s <em>r</em>
(and Spearman’s <em>ρ</em> which we discuss in <a class="reference internal" href="#sec-spearman"><span class="std std-numref">Section 9.1.4</span></a> –
let us ignore it by then):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pearsonr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">ρ</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">spearmanr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;r = </span><span class="si">{</span><span class="n">r</span><span class="si">:</span><span class="s2">.3</span><span class="si">}</span><span class="se">\n</span><span class="s2">ρ = </span><span class="si">{</span><span class="n">ρ</span><span class="si">:</span><span class="s2">.3</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="k">pass</span>
</pre></div>
</div>
<div class="section" id="perfect-linear-correlation">
<h4><span class="section-number">9.1.1.1. </span>Perfect Linear Correlation<a class="headerlink" href="#perfect-linear-correlation" title="Permalink to this headline"></a></h4>
<p>First of all, note that the above properties imply that
<span class="math notranslate nohighlight">\(r(\boldsymbol{x}, \boldsymbol{y})=-1\)</span> if and only if
<span class="math notranslate nohighlight">\(\boldsymbol{y}=a\boldsymbol{x}+b\)</span> for some <span class="math notranslate nohighlight">\(a&lt;0\)</span> and <span class="math notranslate nohighlight">\(b\)</span>
(reaches the minimum when one variable is
a decreasing linear function of the other one)
Furthermore, a variable is trivially
perfectly correlated with itself <span class="math notranslate nohighlight">\(r(\boldsymbol{x}, \boldsymbol{x})=1\)</span>.</p>
<p>Hence, we get perfect <em>linear correlation</em> (-1 or 1) when
one variable is a scaled and shifted version (linear function)
of the other variable, see <a class="reference internal" href="#fig-corr-ex-0"><span class="std std-numref">Figure 9.1</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># negative slope</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># positive slope</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id11">
<span id="fig-corr-ex-0"></span><img alt="../_images/corr-ex-0-1.png" src="../_images/corr-ex-0-1.png" />
<p class="caption"><span class="caption-number">Figure 9.1 </span><span class="caption-text">Perfect linear correlation (negative and positive)</span><a class="headerlink" href="#id11" title="Permalink to this image"></a></p>
</div>
<p>Note that negative correlation means that when one variable
increases, the other one decreases (like: a car’s braking distance vs
velocity).</p>
</div>
<div class="section" id="strong-linear-correlation">
<h4><span class="section-number">9.1.1.2. </span>Strong Linear Correlation<a class="headerlink" href="#strong-linear-correlation" title="Permalink to this headline"></a></h4>
<p>Next, if two variables are <em>more or less</em> linear functions
of themselves, the correlations will be close to -1 or 1,
with the degree of association diminishing as the linear relationship
becomes less and less present,
see <a class="reference internal" href="#fig-corr-ex-1"><span class="std std-numref">Figure 9.2</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">x</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># random white noise (of mean 0)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.figsize&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># width=height</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">+</span><span class="mf">0.05</span><span class="o">*</span><span class="n">e</span><span class="p">)</span>  <span class="c1"># add some noise</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">+</span><span class="mf">0.1</span><span class="o">*</span><span class="n">e</span><span class="p">)</span>   <span class="c1"># more noise</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">+</span><span class="mf">0.25</span><span class="o">*</span><span class="n">e</span><span class="p">)</span>  <span class="c1"># even more noise</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id12">
<span id="fig-corr-ex-1"></span><img alt="../_images/corr-ex-1-3.png" src="../_images/corr-ex-1-3.png" />
<p class="caption"><span class="caption-number">Figure 9.2 </span><span class="caption-text">Linear correlation coefficients for data with different amounts of noise</span><a class="headerlink" href="#id12" title="Permalink to this image"></a></p>
</div>
<p>Note again that the arm and hip circumferences enjoy
quite high positive degree of linear correlation.</p>
<div class="proof proof-type-exercise" id="id13">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.1</span>
        
    </div><div class="proof-content">
<p>Draw a  series of similar plots but
for the case of negatively correlated point pairs.</p>
</div></div><div class="admonition important">
<p class="admonition-title">Important</p>
<p>As a rule of thumb, linear correlation degree of
0.9 or greater (or -0.9 or smaller) is quite decent.
Between -0.8 and 0.8 we probably should not be talking about two variables
being linearly correlated at all.
Some textbooks are more lenient, but we have higher standards.
In particular, it is not uncommon in social sciences to
consider 0.6 a decent degree of correlation, but this is like
building on sand. If a dataset at hand does not provide us with
strong evidence, it is our ethical duty to refrain ourselves
from publishing unjustified statements.</p>
</div>
</div>
<div class="section" id="no-linear-correlation-does-not-imply-independence">
<h4><span class="section-number">9.1.1.3. </span>No Linear Correlation Does Not Imply Independence<a class="headerlink" href="#no-linear-correlation-does-not-imply-independence" title="Permalink to this headline"></a></h4>
<p>We should stress that correlation close to 0 does not necessarily
mean that two variables are not related to each other,
although for two independent variables
we definitely expect the correlation coefficient be approximately equal to 0.
Pearson’s <em>r</em> is a <em>linear</em> correlation coefficient, so we
are only quantifying these types of relationships.
See <a class="reference internal" href="#fig-corr-ex-2"><span class="std std-numref">Figure 9.3</span></a> for an illustration of this fact.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.figsize&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># width=height</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>  <span class="c1"># independent (not correlated)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># quadratic dependence</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># another form of dependence</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">10</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># another</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id14">
<span id="fig-corr-ex-2"></span><img alt="../_images/corr-ex-2-5.png" src="../_images/corr-ex-2-5.png" />
<p class="caption"><span class="caption-number">Figure 9.3 </span><span class="caption-text">Are all of these really uncorrelated?</span><a class="headerlink" href="#id14" title="Permalink to this image"></a></p>
</div>
</div>
<div class="section" id="false-linear-correlations">
<h4><span class="section-number">9.1.1.4. </span>False Linear Correlations<a class="headerlink" href="#false-linear-correlations" title="Permalink to this headline"></a></h4>
<p>What is more, sometimes we can detect <em>false</em> correlations –
when data are functionally dependent, the relationship
is not linear, but it kind of looks like linear.
Refer to <a class="reference internal" href="#fig-corr-ex-3"><span class="std std-numref">Figure 9.4</span></a> for some examples.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.figsize&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># width=height</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">0.6</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="mi">2</span><span class="o">+</span><span class="mf">0.2</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id15">
<span id="fig-corr-ex-3"></span><img alt="../_images/corr-ex-3-7.png" src="../_images/corr-ex-3-7.png" />
<p class="caption"><span class="caption-number">Figure 9.4 </span><span class="caption-text">Example non-linear relationships look like linear to Pearson’s <em>r</em></span><a class="headerlink" href="#id15" title="Permalink to this image"></a></p>
</div>
<p>No single measure is perfect – we are trying to compress
<em>2n</em> data points into a single number — it is obvious that
there will be many different datasets, sometimes very diverse,
that will yield the same correlation value.</p>
</div>
<div class="section" id="correlation-is-not-causation">
<h4><span class="section-number">9.1.1.5. </span>Correlation Is Not Causation<a class="headerlink" href="#correlation-is-not-causation" title="Permalink to this headline"></a></h4>
<p>Note that high correlation degree (either positive or negative) does not
mean that there is any <em>casual</em> relationship between the two variables.
We cannot say that having large arm circumference affects hip size
or the other way around. There might be some <em>latent</em> variable
that influences these two (e.g., maybe also related to weight?).</p>
<div class="proof proof-type-exercise" id="id16">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.2</span>
        
    </div><div class="proof-content">
<p>Quite often, medical advice is formulated
based on correlations and similar association-measuring tools.
We should know how to interpret them, as it is never
a true cause-effect relationship; rather,
it is all about detecting common patterns in larger populations.
For instance, in “obesity increases the likelihood of lower back
pain and diabetes” we does not say that one necessarily
<em>implies</em> another or that if you are not overweight, there is no
risk of getting the two said conditions.
It might also work the other way around, as lower back pain
may lead to less exercise and then weight gain. Reality
is complex.
Find similar patterns related to other sets of conditions.
Which are stronger than others?</p>
</div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Measuring correlations can aid in constructing
regression models, where we would like to
identify the transformation that expresses one variable
as a function of one or more other ones.
When we say that <span class="math notranslate nohighlight">\(y\)</span> can be modelled by <span class="math notranslate nohighlight">\(ax+b\)</span>,
regression analysis will identify some concrete <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>
coefficients – see <a class="reference internal" href="#sec-least-squares"><span class="std std-numref">Section 9.2.3</span></a> for more details.</p>
<p>In particular, we would like to include some variables
that are correlated with the modelled variable
but avoid modelling with the features that are highly
correlated with each other, because they do not bring anything
interesting to the table and can cause the solution to be numerically
unstable.</p>
</div>
</div>
</div>
<div class="section" id="correlation-heatmap">
<h3><span class="section-number">9.1.2. </span>Correlation Heatmap<a class="headerlink" href="#correlation-heatmap" title="Permalink to this headline"></a></h3>
<p>Calling <strong class="command">numpy.corrcoef</strong><code class="code docutils literal notranslate"><span class="pre">(body.T)</span></code> (note the matrix
transpose) allows for determining the linear correlation
coefficients between all pairs of variables.</p>
<p>We can nicely depict them on a heatmap, see <a class="reference internal" href="#fig-cor-heat"><span class="std std-numref">Figure 9.5</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="n">order</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">cols</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="s2">&quot;arm len&quot;</span><span class="p">,</span>
    <span class="s2">&quot;leg len&quot;</span><span class="p">,</span> <span class="s2">&quot;arm circ&quot;</span><span class="p">,</span> <span class="s2">&quot;hip circ&quot;</span><span class="p">,</span> <span class="s2">&quot;waist circ&quot;</span><span class="p">])</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">body</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span>
    <span class="n">C</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">order</span><span class="p">,</span> <span class="n">order</span><span class="p">)],</span>
    <span class="n">xticklabels</span><span class="o">=</span><span class="n">cols</span><span class="p">[</span><span class="n">order</span><span class="p">],</span>
    <span class="n">yticklabels</span><span class="o">=</span><span class="n">cols</span><span class="p">[</span><span class="n">order</span><span class="p">],</span>
    <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;.2f&quot;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s2">&quot;copper&quot;</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id17">
<span id="fig-cor-heat"></span><img alt="../_images/cor-heat-9.png" src="../_images/cor-heat-9.png" />
<p class="caption"><span class="caption-number">Figure 9.5 </span><span class="caption-text">A correlation heatmap</span><a class="headerlink" href="#id17" title="Permalink to this image"></a></p>
</div>
<p>Note that we have ordered the columns to reveal some naturally
occurring variable <em>clusters</em>: for instance,
arm, hip, waist circumference and weight are all quite strongly
correlated.</p>
<p>Of course, we have 1.0s on the main diagonal because a variable
is trivially correlated with itself.
Also, note that this heatmap is symmetric
which is due to the property
<span class="math notranslate nohighlight">\(r(\boldsymbol{x}, \boldsymbol{y}) = r(\boldsymbol{y}, \boldsymbol{x})\)</span>.</p>
<div class="proof proof-type-example" id="id18">

    <div class="proof-title">
        <span class="proof-type">Example 9.3</span>
        
    </div><div class="proof-content">
<p>(*)
To fetch the row and column index of the most correlated pair of
variables (either positively or negatively),
we should first take the upper (or lower) triangle of the correlation matrix
(see <strong class="command">numpy.triu</strong> or <strong class="command">numpy.tril</strong>) to ignore the irrelevant
and repeating items:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Cu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">C</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">Cu</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1">## array([[0.  , 0.35, 0.55, 0.19, 0.91, 0.95, 0.9 ],</span>
<span class="c1">##        [0.  , 0.  , 0.67, 0.66, 0.15, 0.2 , 0.13],</span>
<span class="c1">##        [0.  , 0.  , 0.  , 0.48, 0.45, 0.46, 0.43],</span>
<span class="c1">##        [0.  , 0.  , 0.  , 0.  , 0.08, 0.1 , 0.03],</span>
<span class="c1">##        [0.  , 0.  , 0.  , 0.  , 0.  , 0.87, 0.85],</span>
<span class="c1">##        [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.9 ],</span>
<span class="c1">##        [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])</span>
</pre></div>
</div>
<p>and then find the location of the maximum:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">unravel_index</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Cu</span><span class="p">),</span> <span class="n">Cu</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1">## (0, 5)</span>
</pre></div>
</div>
<p>Thus, weight and hip circumference is the most strongly correlated.</p>
<p>Note that <strong class="command">numpy.argmax</strong> returns an index in the
flattened (unidimensional) array, therefore we had to use
<strong class="command">numpy.unravel_index</strong> to convert it to a two-dimensional
one.</p>
</div></div></div>
<div class="section" id="linear-correlation-coefficients-on-transformed-data">
<h3><span class="section-number">9.1.3. </span>Linear Correlation Coefficients on Transformed Data<a class="headerlink" href="#linear-correlation-coefficients-on-transformed-data" title="Permalink to this headline"></a></h3>
<p>Pearson’s coefficient can of course also be applied
on nonlinearly transformed versions of variables,
e.g., logarithms (remember incomes?), squares, square roots, etc.</p>
<p>Let us consider an excerpt from the 2020 CIA
<a class="reference external" href="https://www.cia.gov/library/publications/the-world-factbook/docs/rankorderguide.html">World Factbook</a>,
where we have data on gross domestic product per capita (based
on purchasing power parity) and life expectancy at birth in many countries.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">world</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching_data/master/marek/world_factbook_2020_subset1.csv&quot;</span><span class="p">,</span>
    <span class="n">comment</span><span class="o">=</span><span class="s2">&quot;#&quot;</span><span class="p">)</span>
<span class="n">world</span> <span class="o">=</span> <span class="n">world</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">world</span><span class="p">[:</span><span class="mi">6</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># preview</span>
<span class="c1">## array([[ 2000. ,    52.8],</span>
<span class="c1">##        [12500. ,    79. ],</span>
<span class="c1">##        [15200. ,    77.5],</span>
<span class="c1">##        [11200. ,    74.8],</span>
<span class="c1">##        [49900. ,    83. ],</span>
<span class="c1">##        [ 6800. ,    61.3]])</span>
</pre></div>
</div>
<p><a class="reference internal" href="#fig-worldfactbook-gdp-life"><span class="std std-numref">Figure 9.6</span></a> depicts these data on
a scatterplot.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;per capita GDP PPP&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;life expectancy (years)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;log(per capita GDP PPP)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id19">
<span id="fig-worldfactbook-gdp-life"></span><img alt="../_images/WorldFactbook-gdp-life-11.png" src="../_images/WorldFactbook-gdp-life-11.png" />
<p class="caption"><span class="caption-number">Figure 9.6 </span><span class="caption-text">Scatterplots for life expectancy vs gross domestic product (purchasing power parity) on linear (left-) and log-scale (righthand side)</span><a class="headerlink" href="#id19" title="Permalink to this image"></a></p>
</div>
<p>Computing Pearson’s <em>r</em>
between these two indicates a quite weak linear correlation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pearsonr</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1">## 0.656471945486374</span>
</pre></div>
</div>
<p>However, already the logarithm of GDP is slightly more strongly linearly
correlated with life expectancy:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pearsonr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1">## 0.8066505089380016</span>
</pre></div>
</div>
<p>which means that modelling our data
via <span class="math notranslate nohighlight">\(\boldsymbol{y}=a \log\boldsymbol{x}+b\)</span>
can be an idea worth considering.</p>
</div>
<div class="section" id="spearman-s-rank-correlation-coefficient">
<span id="sec-spearman"></span><h3><span class="section-number">9.1.4. </span>Spearman’s Rank Correlation Coefficient<a class="headerlink" href="#spearman-s-rank-correlation-coefficient" title="Permalink to this headline"></a></h3>
<p>Sometimes we might be interested in measuring
the degree of any kind of <em>monotonic</em> correlation – to what extent
one variable is an increasing or decreasing function
of another one (linear, logarithmic, quadratic over the positive
domain, etc.).</p>
<p>Spearman’s rank correlation coefficient  is frequently used
in such a scenario:</p>
<div class="math notranslate nohighlight">
\[
\rho(\boldsymbol{x}, \boldsymbol{y}) = r(R(\boldsymbol{x}), R(\boldsymbol{y}))
\]</div>
<p>which is<a class="footnote-reference brackets" href="#footspearman" id="id1">1</a> the Pearson coefficient
computed over vectors of the corresponding
ranks of all the elements in <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>
(denoted with <span class="math notranslate nohighlight">\(R(\boldsymbol{x})\)</span> and <span class="math notranslate nohighlight">\(R(\boldsymbol{y})\)</span>).</p>
<p>Hence, the two following calls are equivalent:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">spearmanr</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1">## 0.8275220380818622</span>
<span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pearsonr</span><span class="p">(</span>
    <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">rankdata</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]),</span>
    <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">rankdata</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1">## 0.8275220380818621</span>
</pre></div>
</div>
<p>Let us point out that this measure is invariant with respect to
monotone transformations of the input variables
(up to the sign):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">spearmanr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]),</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]))[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1">## -0.8275220380818622</span>
</pre></div>
</div>
<div class="proof proof-type-exercise" id="id20">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.4</span>
        
    </div><div class="proof-content">
<p>We have included the <em>ρ</em>s in all the outputs generated
by our <strong class="command">plot_corr</strong> functions. Review all the figures listed
above.</p>
</div></div><div class="proof proof-type-exercise" id="id21">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.5</span>
        
    </div><div class="proof-content">
<p>Apply <strong class="command">numpy.corrcoef</strong>
and <strong class="command">scipy.stats.rankdata</strong> (with an appropriate <code class="docutils literal notranslate"><span class="pre">axis</span></code> argument)
to compute the Spearman correlation matrix for all the variable
pairs in <code class="docutils literal notranslate"><span class="pre">body</span></code>. Draw it on a heatmap.</p>
</div></div><div class="proof proof-type-exercise" id="id22">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.6</span>
        
    </div><div class="proof-content">
<p>(*) Draw the scatterplots of the ranks of columns
in the <code class="docutils literal notranslate"><span class="pre">world</span></code> and <code class="docutils literal notranslate"><span class="pre">body</span></code> datasets.</p>
</div></div></div>
</div>
<div class="section" id="regression-tasks">
<span id="sec-regression"></span><h2><span class="section-number">9.2. </span>Regression Tasks<a class="headerlink" href="#regression-tasks" title="Permalink to this headline"></a></h2>
<p>Given a <em>training</em> set of <span class="math notranslate nohighlight">\(n\)</span> points in an <span class="math notranslate nohighlight">\(m\)</span>-dimensional space
represented as an <span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{n\times m}\)</span> matrix
and a set of <span class="math notranslate nohighlight">\(n\)</span> reference numeric outputs <span class="math notranslate nohighlight">\(\boldsymbol{y}\in\mathbb{R}^n\)</span>,
<em>regression</em> aims to find function
between the <span class="math notranslate nohighlight">\(m\)</span> <em>independent/explanatory/predictor</em> variables
and a chosen <em>dependent/response/predicted</em> variable:</p>
<div class="math notranslate nohighlight">
\[
y = f(x_1, x_2, \dots, x_m),
\]</div>
<p>that <em>approximates</em> the given dataset in a <em>usable</em> way.</p>
<div class="section" id="k-nearest-neighbour-regression">
<span id="sec-knn-regression"></span><h3><span class="section-number">9.2.1. </span><em>K</em>-Nearest Neighbour Regression<a class="headerlink" href="#k-nearest-neighbour-regression" title="Permalink to this headline"></a></h3>
<p>A quite straightforward approach to regression
relies on aggregating the reference outputs
of the <em>k</em> nearest neighbours of the point tested
(compare <a class="reference internal" href="320-transform-matrix.html#sec-knn-vs-radius"><span class="std std-numref">Section 8.4.4</span></a>).</p>
<p>For a fixed <span class="math notranslate nohighlight">\(k\ge 1\)</span> and a given <span class="math notranslate nohighlight">\(\boldsymbol{x}'\in\mathbb{R}^m\)</span>,
<span class="math notranslate nohighlight">\(f(\boldsymbol{x}')\)</span> is computed by averaging the reference outputs
in the point’s local neighbourhood.</p>
<ol>
<li><p>Find the indices <span class="math notranslate nohighlight">\(N_k(\boldsymbol{y})=\{i_1,\dots,i_k\}\)</span>
of the <span class="math notranslate nohighlight">\(k\)</span> points from
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span> closest to <span class="math notranslate nohighlight">\(\boldsymbol{x}'\)</span>, i.e., ones that fulfil
for all <span class="math notranslate nohighlight">\(j\not\in\{i_1,\dots,i_k\}\)</span></p>
<div class="math notranslate nohighlight">
\[ \|\mathbf{x}_{i_1,\cdot}-\boldsymbol{x}'\|
    \le\dots\le
    \| \mathbf{x}_{i_k,\cdot} -\boldsymbol{x}' \|
    \le
    \| \mathbf{x}_{j,\cdot} -\boldsymbol{x}' \|.
    \]</div>
</li>
<li><p>Return the arithmetic mean of <span class="math notranslate nohighlight">\((y_{i_1},\dots,y_{i_k})\)</span>
as the result, i.e., assign the average of the outputs
corresponding to its <span class="math notranslate nohighlight">\(k\)</span> nearest neighbours.</p></li>
</ol>
<p>Here is a straightforward implementation
that generates the predictions for each point in <code class="docutils literal notranslate"><span class="pre">x_test</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">knn_regress</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">spatial</span><span class="o">.</span><span class="n">KDTree</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">k</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># indices of NNs</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="c1"># corresponding reference outputs</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>For example, let us try expressing weight (the 1st column) as a function
of hip circumference (the 6th column) in our NHANES study
(<code class="docutils literal notranslate"><span class="pre">body</span></code> dataset):</p>
<div class="math notranslate nohighlight">
\[
\text{weight}=f_1(\text{hip circumference})   \qquad  (+ \text{some error}).
\]</div>
<p>We will also model the life expectancy at birth in different countries
(<code class="docutils literal notranslate"><span class="pre">world</span></code> dataset) as a function of their GDP per capita (PPP):</p>
<div class="math notranslate nohighlight">
\[
\text{life expectancy}=f_2(\text{GDP per capita})   \qquad  (+ \text{some error}).
\]</div>
<p><a class="reference internal" href="#fig-knn-reg"><span class="std std-numref">Figure 9.7</span></a> depicts the fitted functions for a few different
<em>k</em>s.</p>
<p>We have obtained a <em>smoothened</em> version of the original dataset.
The fact that we do not reproduce the data points in an exact manner
is reflected by the (figurative) error term in the above equations.
Its role is to emphasise the existence of some natural data variability;
after all, one’s weight is not purely determined by their hip size.</p>
<p>Let us note that for small <em>k</em> we adapt better to the data points,
which can be a good thing unless data are very noisy.
The greater the <em>k</em>, the smoother the approximation
at the cost of losing fine detail and restricted usability
at the domain boundary.</p>
<p>Usually, the number of neighbours is chosen by trial and error
(just like the number of bins in a histogram; compare
<a class="reference internal" href="210-vector.html#sec-how-many-bins"><span class="std std-numref">Section 4.3.3</span></a>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">body</span><span class="p">[:,</span> <span class="mi">5</span><span class="p">]</span>  <span class="c1"># hip circumference</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">body</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># weight</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">1001</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">100</span><span class="p">]:</span>
    <span class="n">_y</span> <span class="o">=</span> <span class="n">knn_regress</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;$k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;hip circumference&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;weight&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># GDP</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># life expectancy</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">1001</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">100</span><span class="p">]:</span>
    <span class="n">_y</span> <span class="o">=</span> <span class="n">knn_regress</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;$k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">$&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;per capita GDP PPP&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;life expectancy (years)&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id23">
<span id="fig-knn-reg"></span><img alt="../_images/knn-reg-13.png" src="../_images/knn-reg-13.png" />
<p class="caption"><span class="caption-number">Figure 9.7 </span><span class="caption-text"><em>K</em>-nearest neighbour regression curves for example datasets; the greater the <em>k</em>, the more coarse-grained the approximation</span><a class="headerlink" href="#id23" title="Permalink to this image"></a></p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>(**) Some methods use weighted arithmetic means
for aggregating the <em>k</em> reference outputs, with weights proportional
to the actual distances to the neighbours (closer inputs are
considered more important).</p>
<p>Also, instead of few nearest neighbours, we could
easily implement some form of fixed-radius search regression
(compare <a class="reference internal" href="320-transform-matrix.html#sec-knn-vs-radius"><span class="std std-numref">Section 8.4.4</span></a>).</p>
<p>These are left as an enjoyable exercise to the skilled reader.</p>
</div>
</div>
<div class="section" id="from-data-to-linear-models">
<h3><span class="section-number">9.2.2. </span>From Data to (Linear) Models<a class="headerlink" href="#from-data-to-linear-models" title="Permalink to this headline"></a></h3>
<p>Unfortunately, in order to generate predictions for new data points,
<em>k</em>-nearest neighbours regression requires
that the training sample is available at all times.
It does not <em>synthesise</em> or <em>simplify</em> the inputs;
instead, it works as a kind  of a black-box.</p>
<p>In many contexts we might prefer creating a data <em>model</em> instead,
i.e., an easily interpretable mathematical function.
A simple yet still quite flexible choice
tackles regression problems via linear (affine) maps of the form:</p>
<div class="math notranslate nohighlight">
\[
y = f(x_1, x_2, \dots, x_m) = c_0 + c_1 x_1 + c_2 x_2 + \dots + c_m x_m,
\]</div>
<p>or, in matrix multiplication terms,</p>
<div class="math notranslate nohighlight">
\[
y = c_0 + \mathbf{c} \mathbf{x}^T,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{c}=[c_1\ c_2\ \cdots\ c_m]\)</span>
and <span class="math notranslate nohighlight">\(\mathbf{x}=[x_1\ x_2\ \cdots\ x_m]\)</span>.</p>
<p>For <span class="math notranslate nohighlight">\(m=1\)</span>, the above simply defines a straight line,
which we traditionally denote with</p>
<div class="math notranslate nohighlight">
\[
y = f(x) = ax+b
\]</div>
<p>i.e., where we mapped <span class="math notranslate nohighlight">\(x \mapsto x_1\)</span>, <span class="math notranslate nohighlight">\(c_0 \mapsto b\)</span> (intercept),
and <span class="math notranslate nohighlight">\(c_1 \mapsto a\)</span> (slope).</p>
<p>For <span class="math notranslate nohighlight">\(m&gt;1\)</span>, we obtain different hyperplanes (high-dimensional
generalisations of the notion of a plane).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As a separate intercept “<span class="math notranslate nohighlight">\(c_0 +\)</span>” term in the defining equation
can be quite inconvenient, notationwisely,
we usually restrict ourselves to linear maps like</p>
<div class="math notranslate nohighlight">
\[y = \mathbf{c} \mathbf{x}^T,\]</div>
<p>but where we can possibly have an explicit constant <span class="math notranslate nohighlight">\(1\)</span> component
<em>inside</em> <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, for instance,</p>
<div class="math notranslate nohighlight">
\[\mathbf{x} = [1\ x_1\ x_2\ \cdots\ x_m].\]</div>
<p>Together with
<span class="math notranslate nohighlight">\(\mathbf{c} = [c_0\ c_1\ c_2\ \cdots\ c_m]\)</span>,
as trivially <span class="math notranslate nohighlight">\(c_0\cdot 1=c_0\)</span>,
this new setting is equivalent to the original one.</p>
</div>
</div>
<div class="section" id="least-squares-method">
<span id="sec-least-squares"></span><h3><span class="section-number">9.2.3. </span>Least Squares Method<a class="headerlink" href="#least-squares-method" title="Permalink to this headline"></a></h3>
<p>A linear model is uniquely<a class="footnote-reference brackets" href="#footmodelserialise" id="id2">2</a> encoded using only
the coefficients <span class="math notranslate nohighlight">\(c_1,\dots,c_m\)</span>. In order to find them,
for each point <span class="math notranslate nohighlight">\(\mathbf{x}_{i,\cdot}\)</span> from the input (training) set,
we typically desire the <em>predicted</em> value</p>
<div class="math notranslate nohighlight">
\[\hat{y}_i = f(x_{i,1}, x_{i,2}, \dots, x_{i,m})= \mathbf{c} \mathbf{x}_{i,\cdot}^T\]</div>
<p>to be as <em>close</em> to the corresponding reference <span class="math notranslate nohighlight">\(y_i\)</span> as possible.</p>
<p>There can be many possible measures of <em>closeness</em>
but the most popular one<a class="footnote-reference brackets" href="#footwhyssr" id="id3">3</a> uses the notion of
the <em>sum of squared residuals</em> (true minus predicted outputs),</p>
<div class="math notranslate nohighlight">
\[
\mathrm{SSR}(\boldsymbol{c}|\mathbf{X},\mathbf{y})
= \sum_{i=1}^n \left( y_i - \hat{y}_i \right)^2
= \sum_{i=1}^n \left( y_i - (c_1 x_{i,1} + c_2 x_{i,2} + \dots + c_m x_{i,m}) \right)^2,
\]</div>
<p>which is a function of <span class="math notranslate nohighlight">\(\boldsymbol{c}=(c_1,\dots,c_m)\)</span>
(for fixed <span class="math notranslate nohighlight">\(\mathbf{X},\mathbf{y}\)</span>).</p>
<p>And thus the <em>least squares</em> solution to the stated linear
regression problem will be defined by the coefficient vector <span class="math notranslate nohighlight">\(\boldsymbol{c}\)</span>
that minimises the SSR.
Based on what we have said about matrix multiplication,
this is equivalent to solving the optimisation task</p>
<div class="math notranslate nohighlight">
\[
\text{minimise}\
\left(\mathbf{y}-\mathbf{c} \mathbf{X}^T\right) \left(\mathbf{y}-\mathbf{c} \mathbf{X}^T\right)^T
\qquad\text{w.r.t. }{(c_1,\dots,c_m)\in\mathbb{R}^m},
\]</div>
<p>because
<span class="math notranslate nohighlight">\(\hat{\mathbf{y}}=\mathbf{c} \mathbf{X}^T\)</span> gives the predicted values
as a row vector
(the kind reader is encouraged to check that on a piece of paper now),
<span class="math notranslate nohighlight">\(\mathbf{r}=\mathbf{y}-\hat{\mathbf{y}}\)</span> computes all the <span class="math notranslate nohighlight">\(n\)</span> residuals,
and <span class="math notranslate nohighlight">\(\mathbf{r}\, \mathbf{r}^T\)</span> gives their sum of squares.</p>
<p>The method of least squares is one of the
simplest and most natural approaches to regression analysis
(curve fitting). Its theoretical foundations (calculus…) were developed
more than 200 years ago by Gauss and then it was polished by Legendre.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>(*)
Had the points lain on a hyperplane exactly (the interpolation problem),
<span class="math notranslate nohighlight">\(\mathbf{y} = \mathbf{c} \mathbf{X}^T\)</span> would have an exact solution,
equivalent to solving the linear system of equations
<span class="math notranslate nohighlight">\(\mathbf{y}-\mathbf{c} \mathbf{X}^T =\mathbf{0}\)</span>.
However, in our setting we assume that there might be some
measurement errors or other discrepancies between the reality and the
theoretical model. Thus, to account for this,
we are trying to solve a more general problem
of finding a hyperplane for which
<span class="math notranslate nohighlight">\(\|\mathbf{y}-\mathbf{c} \mathbf{X}^T\|^2\)</span> is as small as possible.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>(*) The above can be solved analytically
(compute the partial derivatives of SSR with respect to
each <span class="math notranslate nohighlight">\(c_1,\dots,c_m\)</span>, equate them to 0, and solve a simple system
of linear equations), which results in
<span class="math notranslate nohighlight">\(\mathbf{c} = \mathbf{y} \mathbf{X} (\mathbf{X}^T \mathbf{X})^{-1}\)</span>,
where <span class="math notranslate nohighlight">\(\mathbf{A}^{-1}\)</span> is the inverse of a matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>,
i.e., the matrix such that
<span class="math notranslate nohighlight">\(\mathbf{A} \mathbf{A}^{-1}=\mathbf{A}^{-1} \mathbf{A} =\mathbf{I}\)</span>.
As inverting larger matrices directly is not too robust,
numerically speaking, we prefer relying upon some more
specialised algorithms to determine the solution.</p>
</div>
<p>The <strong class="command">scipy.linalg.lstsq</strong> function provides a quite numerically
stable procedure (which is based on the singular value decomposition
of the model matrix, see below for discussion and common pitfalls).</p>
<div style="margin-top: 1em"></div><p>Consider the above NHANES study excerpt one more time; let us
say we would like to express weight (the 1st column) as a now linear function
of hip circumference (the 6th column),</p>
<div class="math notranslate nohighlight">
\[
\text{weight}=a\cdot\text{hip circumference} + b  \qquad (+ \text{some error}).
\]</div>
<p>The error term corresponds to the residuals as by drawing
a scatterplot (see the figure below) of the involved variables we can see
that the data do not lie on a straight line perfectly:
each model is an idealisation/simplification of the described
reality.</p>
<p>The <em>design (model) matrix</em> <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>
and reference <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>s are thus:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_original</span> <span class="o">=</span> <span class="n">body</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">5</span><span class="p">]]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">x_original</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># 1s, hip circumference</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">body</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># weight</span>
</pre></div>
</div>
<p>Note that we have used the vectorised power operator
to convert each <span class="math notranslate nohighlight">\(x_i\)</span> (the <span class="math notranslate nohighlight">\(i\)</span>-th hip circumference)
to a pair <span class="math notranslate nohighlight">\((x_i^0, x_i^1) = (1, x_i)\)</span>, which is a nice trick
to prepended a column of 1s to <code class="docutils literal notranslate"><span class="pre">X</span></code> so that we can
include the intercept term in the model. Here is a preview:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">preview_indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">]</span>
<span class="n">X</span><span class="p">[</span><span class="n">preview_indices</span><span class="p">,</span> <span class="p">:]</span>
<span class="c1">## array([[  1. ,  92.5],</span>
<span class="c1">##        [  1. , 106.7],</span>
<span class="c1">##        [  1. ,  96.3],</span>
<span class="c1">##        [  1. , 102. ],</span>
<span class="c1">##        [  1. ,  94.8],</span>
<span class="c1">##        [  1. ,  97.5]])</span>
<span class="n">y_true</span><span class="p">[</span><span class="n">preview_indices</span><span class="p">]</span>
<span class="c1">## array([55.4, 62. , 66.2, 77.2, 64.2, 56.8])</span>
</pre></div>
</div>
<p>Let us determine the least squares solution to our regression problem:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.linalg</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)</span>
</pre></div>
</div>
<p>The optimal coefficients vector (one that minimises the SSR) is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">c</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">c</span>
<span class="c1">## array([-65.10087248,   1.3052463 ])</span>
</pre></div>
</div>
<p>Therefore, the estimated model is:</p>
<div class="math notranslate nohighlight">
\[
\text{weight}=1.305\cdot\text{hip circumference} -65.1 \qquad (+ \text{some error}).
\]</div>
<p>Let us contemplate the fact that the model is nicely interpretable.
For instance, with increasing hip circumference we expect the weights
be greater. It does not mean that there is some <em>casual</em> relationship
between the two (for instance, there can be some latent variables that
affect both of them), but rather that there is some general tendency of
how the data aligns in the sample space.
For instance, that the “best guess” (according to the current model –
there can be many, see below) weight for a person with
hip circumference of 100 cm is 65.4 kg.
Thanks to such models, we can understand certain phenomena better
or find some proxies for different variables (especially if measuring
them directly is tedious, costly, dangerous, etc.).</p>
<p>Let us determine the predicted weights and display them
for the first 6 persons:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">c</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>
<span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">y_pred</span><span class="p">[</span><span class="n">preview_indices</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1">## array([55.63, 74.17, 60.59, 68.03, 58.64, 62.16])</span>
</pre></div>
</div>
<p>The scatterplot and the fitted regression line in <a class="reference internal" href="#fig-hip-weight-lm"><span class="std std-numref">Figure 9.8</span></a>
indicates a quite good fit, but of course there is some natural variability.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x_original</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">x_original</span><span class="o">.</span><span class="n">max</span><span class="p">()])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">_y</span> <span class="o">=</span> <span class="n">c</span> <span class="o">@</span> <span class="p">(</span><span class="n">_x</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_original</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="s2">&quot;r-&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;hip circumference&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;weight&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id24">
<span id="fig-hip-weight-lm"></span><img alt="../_images/hip-weight-lm-15.png" src="../_images/hip-weight-lm-15.png" />
<p class="caption"><span class="caption-number">Figure 9.8 </span><span class="caption-text">The least squares line for weight vs hip circumference</span><a class="headerlink" href="#id24" title="Permalink to this image"></a></p>
</div>
<div class="proof proof-type-exercise" id="id25">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.7</span>
        
    </div><div class="proof-content">
<p>The <a class="reference external" href="https://github.com/gagolews/teaching_data/blob/master/r/anscombe.csv">Anscombe quartet</a> is a famous example dataset, where we have 4 pairs of variables
having very similar means, variances, linear correlation coefficients,
and which can be approximated by the same straight line, however, whose
scatter plots are very different. Contemplate upon this toy example yourself.</p>
</div></div></div>
<div class="section" id="analysis-of-residuals">
<h3><span class="section-number">9.2.4. </span>Analysis of Residuals<a class="headerlink" href="#analysis-of-residuals" title="Permalink to this headline"></a></h3>
<p>The residuals, i.e., the estimation errors – what we expected vs what we get,
for the chosen 6 observations are:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">r</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span>
<span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="n">preview_indices</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1">## array([ -0.23, -12.17,   5.61,   9.17,   5.56,  -5.36])</span>
</pre></div>
</div>
<p>These residuals are visualised in <a class="reference internal" href="#fig-residuals"><span class="std std-numref">Figure 9.9</span></a>.</p>
<div class="figure align-default" id="id26">
<span id="fig-residuals"></span><img alt="../_images/residuals-17.png" src="../_images/residuals-17.png" />
<p class="caption"><span class="caption-number">Figure 9.9 </span><span class="caption-text">Example residuals in a simple linear regression task</span><a class="headerlink" href="#id26" title="Permalink to this image"></a></p>
</div>
<p>We wanted the squared residuals (on average – across all the points)
be as small as possible, and the least squares method assured that
this is the case <em>relative to the chosen model</em>.
However, it still does not mean that what we have obtained necessarily
constitutes a good fit to the training data.
Thus, we need to perform the analysis of residuals.</p>
<p>Interestingly, the average of residuals is always zero:</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i) = 0.
\]</div>
<p>Therefore, if we want to summarise the residuals into a single number,
we should rather use, for example, the root mean squared error (RMSE):</p>
<div class="math notranslate nohighlight">
\[
 \mathrm{RMSE}(\boldsymbol{c}|\mathbf{X},\mathbf{y})
 = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i-\hat{y}_i)^2}.
\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
<span class="c1">## 6.948470091176111</span>
</pre></div>
</div>
<p>or the mean absolute error:</p>
<div class="math notranslate nohighlight">
\[
 \mathrm{MAE}(\boldsymbol{c}|\mathbf{X},\mathbf{y})
 = \frac{1}{n} \sum_{i=1}^n |y_i-\hat{y}_i|.
\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
<span class="c1">## 5.207073583769204</span>
</pre></div>
</div>
<p>which is nicely interpretable, because it measures by
how many kilograms do we err <em>on average</em>. Not bad.</p>
<div class="proof proof-type-exercise" id="id27">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.8</span>
        
    </div><div class="proof-content">
<p>Fit a regression line
explaining weight as a function of the waist circumference
and compute the corresponding RMSE and MAE.</p>
</div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Generally, fitting simple (involving one independent variable)
linear models can only make sense for highly linearly correlated variables.
Interestingly, if <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> are both standardised,
and <span class="math notranslate nohighlight">\(r\)</span> is their Pearson’s coefficient, then the least squares-fitted simple
linear model will be given by <span class="math notranslate nohighlight">\(y=rx\)</span>.</p>
</div>
<p>In order to verify whether a fitted model is not extremely wrong
(e.g., when we fit a liner model to data that clearly follows
a different functional relationship),
a plot of residuals against the fitted values
can be of help; see <a class="reference internal" href="#fig-resid-vs-fit"><span class="std std-numref">Figure 9.10</span></a>.
Ideally, the points should be aligned totally
at random (homoscedasticity) therein, without any dependence structure.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;fitted values&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;residuals&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id28">
<span id="fig-resid-vs-fit"></span><img alt="../_images/resid-vs-fit-19.png" src="../_images/resid-vs-fit-19.png" />
<p class="caption"><span class="caption-number">Figure 9.10 </span><span class="caption-text">Residuals vs fitted values for the linear model explaining weight as a function of hip circumference; the variance of residuals slightly increases as <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> increases, which is not ideal, but it could be much worse than this</span><a class="headerlink" href="#id28" title="Permalink to this image"></a></p>
</div>
<div class="proof proof-type-exercise" id="id29">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.9</span>
        
    </div><div class="proof-content">
<p>Compare<a class="footnote-reference brackets" href="#footknnas" id="id4">4</a> the RMSE and MAE for the <em>k</em>-nearest neighbour
regression curves depicted in the lefthandside of <a class="reference internal" href="#fig-knn-reg"><span class="std std-numref">Figure 9.7</span></a>.
Also, draw the residuals vs fitted plot.</p>
</div></div><div style="margin-top: 1em"></div><p>For linear models fitted using the least squares method,
it can be shown that</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{n} \sum_{i=1}^{n} \left(y_i-\bar{y}\right)^2 =
\frac{1}{n} \sum_{i=1}^{n} \left(\hat{y}_i-\bar{\hat{y}}\right)^2 +
\frac{1}{n} \sum_{i=1}^{n} \left(y_i-\hat{y}_i\right)^2
\]</div>
<p>that is, the variance of the dependent variable (left)
can be decomposed as the variance of the predicted variables
plus the averaged squared residuals.
Multiplying the above by <span class="math notranslate nohighlight">\(n\)</span>, we have that
the <em>total</em> sum of squares (which we want to be as close to the former
as possible) is equal to the
<em>explained</em> sum of squares plus the <em>residual</em> sum of squares
(which we want to be as small as possible):</p>
<div class="math notranslate nohighlight">
\[
\mathrm{TSS} = \mathrm{ESS} + \mathrm{RSS}.
\]</div>
<p>The <em>coefficient of determination</em> (unadjusted R-Squared,
sometimes referred to as simply the <em>score</em>)
is a popular normalised, unitless measure
that is quite easy to interpret with no domain-specific knowledge
of the modelled problem.
Namely, it is given by:</p>
<div class="math notranslate nohighlight">
\[
R^2(\boldsymbol{c}|\mathbf{X},\mathbf{y}) = \frac{\mathrm{ESS}}{\mathrm{TSS}}
= 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}}
= 1-\frac{s_r^2}{s_y^2},
\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y_true</span><span class="o">-</span><span class="n">y_pred</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
<span class="c1">## 0.8959634726270759</span>
</pre></div>
</div>
<p>The coefficient of determination in the current context<a class="footnote-reference brackets" href="#footscore" id="id5">5</a>
is thus the proportion of variance of the
dependent variable explained by the independent variables in the model –
the closer it is to 1, the better.
A dummy model that always returns the mean of <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>
gives R-squared of 0.</p>
<p>In our case, <span class="math notranslate nohighlight">\(R^2\simeq 0.9\)</span> is quite high, which indicates a
moderately good fit.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are certain statistical results that can be relied upon
provided that the residuals are independent random variables
with expectation zero and the same variance (e.g.,
the Gauss–Markov theorem). Further, if they are normally distributed,
then we have a number of hypothesis tests available (e.g.,
for the significance of coefficients). This is why various textbooks
verity such assumptions. However, we do not go that far in this
introductory course.</p>
</div>
</div>
<div class="section" id="multiple-regression">
<h3><span class="section-number">9.2.5. </span>Multiple Regression<a class="headerlink" href="#multiple-regression" title="Permalink to this headline"></a></h3>
<p>As another example, let us fit a model involving two
independent variables, arm and hip circumference:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">body</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]],</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># prepend a column of 1s</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1">## array([-63.38,   1.3 ,   0.9 ])</span>
</pre></div>
</div>
<p>Thus, we have fitted the plane</p>
<div class="math notranslate nohighlight">
\[\text{weight} = -63.38 + 1.3\, \text{arm circumference} + 0.9\, \text{hip circumference}.\]</div>
<p>We skip the visualisation part, because we do not expect it to result
in a readable plot – these are multidimensional data after all.
The coefficient of determination is</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">c</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span>
<span class="mi">1</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">r</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
<span class="c1">## 0.9243996585518783</span>
</pre></div>
</div>
<p>Root mean squared error:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
<span class="c1">## 5.923223870044694</span>
</pre></div>
</div>
<p>Mean absolute error:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
<span class="c1">## 4.431548244333894</span>
</pre></div>
</div>
<p>Thus, it is a slightly better model than the previous one —
we can predict the study participants’ weights with better precision,
at the cost of increased model complexity.</p>
</div>
<div class="section" id="variable-transformation-and-linearisable-models">
<h3><span class="section-number">9.2.6. </span>Variable Transformation and Linearisable Models (*)<a class="headerlink" href="#variable-transformation-and-linearisable-models" title="Permalink to this headline"></a></h3>
<p>We are of course not restricted merely to linear functions
of the input variables, because by applying arbitrary
transformations upon the design matrix columns, we can cover
many interesting scenarios.</p>
<p>For instance, a polynomial model involving two variables:</p>
<div class="math notranslate nohighlight">
\[
g(v_1, v_2)
= \beta_0 + \beta_1 v_1 + \beta_2 v_1^2 + \beta_3 v_1 v_2 + \beta_4 v_2 + \beta_5 v_2^2
\]</div>
<p>can be obtained by substituting
<span class="math notranslate nohighlight">\(x_1=1\)</span>, <span class="math notranslate nohighlight">\(x_2=v_1\)</span>, <span class="math notranslate nohighlight">\(x_3=v_1^2\)</span>, <span class="math notranslate nohighlight">\(x_4=v_1 v_2\)</span>, <span class="math notranslate nohighlight">\(x_5=v_2\)</span>, <span class="math notranslate nohighlight">\(x_6=v_2^2\)</span>,
and then fitting</p>
<div class="math notranslate nohighlight">
\[
f(x_1, x_2, \dots, x_6) = c_1 x_1 + c_2 x_2 + \dots + x_6 x_6.
\]</div>
<p>The design matrix is made of rubber, it can handle anything
(unless it carries highly correlated pairs of input variables).
It will still be a linear model, but with respect to transformed
data. The algorithm does not care. That is the beauty of the underlying
mathematics.</p>
<p>A creative modeller can also turn models such as <span class="math notranslate nohighlight">\(u=c e^{av}\)</span> into <span class="math notranslate nohighlight">\(y=ax+b\)</span>
by replacing <span class="math notranslate nohighlight">\(y=\log u\)</span>, <span class="math notranslate nohighlight">\(x=v\)</span>, and <span class="math notranslate nohighlight">\(b=\log c\)</span>. There are numerous
possibilities here – we call them <em>linearisable models</em>.</p>
<p>As an example, let us model the life expectancy at birth
in different countries as a function of their GDP per capita (PPP).</p>
<p>We will consider four different models:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(y=c_1+c_2 x\)</span> (linear),</p></li>
<li><p><span class="math notranslate nohighlight">\(y=c_1+c_2 x+c_3x^2\)</span> (quadratic),</p></li>
<li><p><span class="math notranslate nohighlight">\(y=c_1+c_2 x+c_3x^2+c_4x^4\)</span> (cubic),</p></li>
<li><p><span class="math notranslate nohighlight">\(y=c_1+c_2\log x\)</span> (logarithmic).</p></li>
</ol>
<p>Here are the helper functions that create the model matrices:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_model_matrix1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">make_model_matrix2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">make_model_matrix3</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">make_model_matrix4</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">make_model_matrix1</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="s2">&quot;linear model&quot;</span>
<span class="n">make_model_matrix2</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="s2">&quot;quadratic model&quot;</span>
<span class="n">make_model_matrix3</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="s2">&quot;cubic model&quot;</span>
<span class="n">make_model_matrix4</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="s2">&quot;logarithmic model&quot;</span>

<span class="n">model_matrix_makers</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">make_model_matrix1</span><span class="p">,</span>
    <span class="n">make_model_matrix2</span><span class="p">,</span>
    <span class="n">make_model_matrix3</span><span class="p">,</span>
    <span class="n">make_model_matrix4</span>
<span class="p">]</span>
<span class="n">Xs</span> <span class="o">=</span> <span class="p">[</span> <span class="n">make_model_matrix</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">make_model_matrix</span> <span class="ow">in</span> <span class="n">model_matrix_makers</span> <span class="p">]</span>
</pre></div>
</div>
<p>Fitting the models:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_true</span> <span class="o">=</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">cs</span> <span class="o">=</span> <span class="p">[</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">X</span> <span class="ow">in</span> <span class="n">Xs</span> <span class="p">]</span>
</pre></div>
</div>
<p>Their coefficients of determination are equal to:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Xs</span><span class="p">)):</span>
    <span class="n">R2</span> <span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">cs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">@</span> <span class="n">Xs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_matrix_makers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="vm">__name__</span><span class="si">:</span><span class="s2">20</span><span class="si">}</span><span class="s2"> R2=</span><span class="si">{</span><span class="n">R2</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1">## linear model         R2=0.431</span>
<span class="c1">## quadratic model      R2=0.567</span>
<span class="c1">## cubic model          R2=0.607</span>
<span class="c1">## logarithmic model    R2=0.651</span>
</pre></div>
</div>
<p>The logarithmic model is thus the best (out of the models we
have considered). The four models are depicted in
<a class="reference internal" href="#fig-gdp-life-four-models"><span class="std std-numref">Figure 9.11</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">101</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">model_matrix_makers</span><span class="p">)):</span>
    <span class="n">_y</span> <span class="o">=</span> <span class="n">cs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">@</span> <span class="n">model_matrix_makers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">_x</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">model_matrix_makers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;per capita GDP PPP&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;life expectancy (years)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id30">
<span id="fig-gdp-life-four-models"></span><img alt="../_images/gdp-life-four-models-21.png" src="../_images/gdp-life-four-models-21.png" />
<p class="caption"><span class="caption-number">Figure 9.11 </span><span class="caption-text">Different models for life expectancy vs GDP per-capita</span><a class="headerlink" href="#id30" title="Permalink to this image"></a></p>
</div>
<div class="proof proof-type-exercise" id="id31">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.10</span>
        
    </div><div class="proof-content">
<p>Draw box plots and histograms of residuals
for each model as well as the scatterplots of residuals vs fitted values.</p>
</div></div></div>
<div class="section" id="descriptive-vs-predictive-power">
<h3><span class="section-number">9.2.7. </span>Descriptive vs Predictive Power (*)<a class="headerlink" href="#descriptive-vs-predictive-power" title="Permalink to this headline"></a></h3>
<p>We have <em>approximated</em> the life vs GDP relationship
using a few different functions. Nevertheless, we see that the above
quadratic and cubic models probably do not make
much sense, semantically speaking. Sure, they do fit the data better,
as far as individual points in the training set are concerned:
after all, they have smaller mean squared errors (again: at these
given points).
However, looking at the way they behave, one does not need a
university degree in economics/social policy to conclude that they probably
are not the best <em>description</em> of how the reality behaves (on average).</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Naturally, a model’s fit to observed data improves as the
model’s complexity increases. The Razor principle
(by William of Ockham et al.) advises that if some phenomenon can be explained
in many different ways, the simplest explanation should be chosen
(<em>do not multiply entities</em> [here: introduce independent variables]
<em>without necessity</em>).</p>
</div>
<p>In particular, the more independent variables we have in the model,
the greater the <span class="math notranslate nohighlight">\(R^2\)</span> coefficient will be. Thus,
we can try correcting for this phenomenon
by considering the <em>adjusted <span class="math notranslate nohighlight">\(R^2\)</span></em></p>
<div class="math notranslate nohighlight">
\[
\bar{R}^2(\boldsymbol{c}|\mathbf{X},\mathbf{y}) = 1 - (1-{R}^2(\boldsymbol{c}|\mathbf{X},\mathbf{y}))\frac{n-1}{n-m-1},
\]</div>
<p>which, to some extent, penalises more complex models.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>(**) Model quality measures adjusted for the number of model
parameters, <span class="math notranslate nohighlight">\(m\)</span>, can also be useful in automated variable selection.
For example, the Akaike Information Criterion is given by
<span class="math notranslate nohighlight">\(\mathrm{AIC}(\boldsymbol{c}|\mathbf{X},\mathbf{y}) =
2m+n\log(\mathrm{SSR}(\boldsymbol{c}|\mathbf{X},\mathbf{y}))-n\log n\)</span>
or the Bayes Information Criterion is defined via
<span class="math notranslate nohighlight">\(\mathrm{BIC}(\boldsymbol{c}|\mathbf{X},\mathbf{y}) =
m\log n+n\log(\mathrm{SSR}(\boldsymbol{c}|\mathbf{X},\mathbf{y}))-n\log n.\)</span>
However, they are dependent on the scale of <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>.</p>
</div>
<p>Further, we should also be interested in a model’s <em>predictive</em> power –
how well does it generalise to data points that we do not have now
(or pretend we do not have), but might face in the future.
After all, we observe the modelled reality only at a few
different points – the question is how does the model perform
when filling the gaps between them.</p>
<p>In particular, we should definitely be careful when <em>extrapolating</em>
the model, i.e., making predictions outside of its usual domain.
For example, the linear model predicts the following life expectancy
for an imaginary country with $500,000 per capita GDP PPP:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">@</span> <span class="n">model_matrix_makers</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">500000</span><span class="p">]))</span><span class="o">.</span><span class="n">T</span>
<span class="c1">## array([164.3593753])</span>
</pre></div>
</div>
<p>and the quadratic one gives:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">@</span> <span class="n">model_matrix_makers</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">500000</span><span class="p">]))</span><span class="o">.</span><span class="n">T</span>
<span class="c1">## array([-364.10630778])</span>
</pre></div>
</div>
<p>Nonsense.</p>
<div style="margin-top: 1em"></div><p>Consider the following theoretical example.
Let us assume that our true model is <span class="math notranslate nohighlight">\(y=5+3x^3\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">true_model</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">5</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>Next, we generate a sample from this model but which is – and
such is life – subject to some measurement error:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">true_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
<p>Least-squares fitting of <span class="math notranslate nohighlight">\(y=c_1+c_2 x^3\)</span> to the above gives:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">c03</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ssr03</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">c03</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">c03</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1">## array([5.01, 3.13])</span>
</pre></div>
</div>
<p>which is not very far, but still somewhat distant from the true
coefficients, 5 and 3.</p>
<p>However, if we decided to fit a more flexible cubic polynomial,
<span class="math notranslate nohighlight">\(y=c_1+c_2 x+c_3 x^2 + c_4 x_3\)</span></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">c0123</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ssr0123</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">c0123</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">c0123</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1">## array([4.89, 0.32, 0.57, 2.23])</span>
</pre></div>
</div>
<p>in terms of the SSR, the more complex model is of course better:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ssr03</span><span class="p">,</span> <span class="n">ssr0123</span>
<span class="c1">## (1.0612111154029558, 0.9619488226837543)</span>
</pre></div>
</div>
<p>but it is farther away from the <em>truth</em>
(which, when we perform the fitting task
based only on given <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>,
is unknown).
We may thus say that the first model generalises
better on yet-to-be-observed data,
see <a class="reference internal" href="#fig-true-model-vs-guess"><span class="std std-numref">Figure 9.12</span></a> for an illustration.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">true_model</span><span class="p">(</span><span class="n">_x</span><span class="p">),</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;true model&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">c0123</span> <span class="o">@</span> <span class="p">(</span><span class="n">_x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;fitted model y=x**[0, 1, 2, 3]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">c03</span> <span class="o">@</span> <span class="p">(</span><span class="n">_x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;fitted model y=x**[0, 3]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id32">
<span id="fig-true-model-vs-guess"></span><img alt="../_images/true-model-vs-guess-23.png" src="../_images/true-model-vs-guess-23.png" />
<p class="caption"><span class="caption-number">Figure 9.12 </span><span class="caption-text">The true (theoretical) model vs some guesstimates (fitted from noisy data); more degrees of freedom does not necessarily reflect the truth better</span><a class="headerlink" href="#id32" title="Permalink to this image"></a></p>
</div>
<p>Note that we have defined the sum of squared residuals
(and its function, the root mean squared error) by means
of the averaged deviation from the reference values,
which in fact are themselves subject to error.
They are our best-shot approximation of the truth, however,
they should be taken with a degree of scepticism.</p>
<p>In our case, given the true (reference) model <span class="math notranslate nohighlight">\(f\)</span> defined over the domain
<span class="math notranslate nohighlight">\(D\)</span> (in our case, <span class="math notranslate nohighlight">\(f(x)=3x^3+5\)</span> and <span class="math notranslate nohighlight">\(D=[0,1]\)</span>)
and an empirically fitted model <span class="math notranslate nohighlight">\(\hat{f}\)</span>, we can replace the
sample RMSE with the square root of the integrated squared error:</p>
<div class="math notranslate nohighlight">
\[
\mathrm{RMSE}(\hat{f}|f) = \sqrt{
    \int_D (f(x)-\hat{f}(x))^2\, dx
}
\]</div>
<p>which we can approximate numerically by sampling the above
at sufficiently many points and applying the trapezoidal rule.</p>
<p>Let us fit a range of polynomial models of different degrees.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">cs</span><span class="p">,</span> <span class="n">rmse_train</span><span class="p">,</span> <span class="n">rmse_test</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">ps</span><span class="p">:</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">cs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">c</span> <span class="o">@</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
    <span class="n">rmse_train</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">y_pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span>

    <span class="n">_y</span> <span class="o">=</span> <span class="n">c</span> <span class="o">@</span> <span class="p">(</span><span class="n">_x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
    <span class="n">_r</span> <span class="o">=</span> <span class="p">(</span><span class="n">true_model</span><span class="p">(</span><span class="n">_x</span><span class="p">)</span> <span class="o">-</span> <span class="n">_y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">rmse_test</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">_x</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">_r</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">+</span><span class="n">_r</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="p">)))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ps</span><span class="p">,</span> <span class="n">rmse_train</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;RMSE (training set)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ps</span><span class="p">,</span> <span class="n">rmse_test</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;RMSE (theoretical)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;model complexity (polynomial degree)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id33">
<span id="fig-true-model-vs-polynomials-rmse"></span><img alt="../_images/true-model-vs-polynomials-rmse-25.png" src="../_images/true-model-vs-polynomials-rmse-25.png" />
<p class="caption"><span class="caption-number">Figure 9.13 </span><span class="caption-text">Good RMSE on training data does not necessarily imply good generalisation abilities</span><a class="headerlink" href="#id33" title="Permalink to this image"></a></p>
</div>
<p>From <a class="reference internal" href="#fig-true-model-vs-polynomials-rmse"><span class="std std-numref">Figure 9.13</span></a> we see that a model’s
ability to make correct generalisations to unseen data, with the increased
complexity initially improves, but then becomes worse. It is quite
a typical behaviour. In fact, the model with the smallest RMSE on the
training set, actually <em>overfits</em> to the input sample,
see <a class="reference internal" href="#fig-true-model-vs-polynomials"><span class="std std-numref">Figure 9.14</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">true_model</span><span class="p">(</span><span class="n">_x</span><span class="p">),</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;true model&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">]:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">cs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">@</span> <span class="p">(</span><span class="n">_x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">ps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;fitted degree-</span><span class="si">{</span><span class="n">ps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2"> polynomial&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id34">
<span id="fig-true-model-vs-polynomials"></span><img alt="../_images/true-model-vs-polynomials-27.png" src="../_images/true-model-vs-polynomials-27.png" />
<p class="caption"><span class="caption-number">Figure 9.14 </span><span class="caption-text">Under- and overfitting to training data</span><a class="headerlink" href="#id34" title="Permalink to this image"></a></p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>When evaluating a model’s quality in terms of predictive power on unseen
data, we should go beyond inspecting its behaviour merely on the points
from the training sample. As the <em>truth</em> is usually not known (if it were,
we would not need any guessing), a common approach in case where we have
a dataset of a considerable size is to divide it (randomly) into three
parts:</p>
<ul class="simple">
<li><p>training sample (say, 60%) – used to fit a model,</p></li>
<li><p>test sample (the remaining 40%) – used to assess its quality
(e.g., by means of RMSE).</p></li>
</ul>
<p>This might <em>emulate</em> an environment where some new data arrives later,
see <a class="reference internal" href="430-group-by.html#sec-train-test-split"><span class="std std-numref">Section 12.3.3</span></a> for more details.</p>
<p>Furthermore, if model selection is required, we may apply a
training-validation-test split (say, 60/20/20%;
see <a class="reference internal" href="430-group-by.html#sec-model-validation"><span class="std std-numref">Section 12.3.4</span></a>) where many models are constructed
on the training set, the validation set is used to compute the metrics
and choose the best model, and then the test set is employed
to come up with the final valuation (because we do not want
the model to overfit to the test set).</p>
</div>
<p>Overall, models should never be blindly trusted – common sense must always
be applied. The fact that we have fitted something using a sophisticated
procedure on a dataset that was hard to obtain does not mean we must
glorify it. Bad models must be discarded and we should move on.
Let us not think about them too much.</p>
</div>
<div class="section" id="fitting-regression-models-with-scikit-learn">
<h3><span class="section-number">9.2.8. </span>Fitting Regression Models with <strong class="program">scikit-learn</strong> (*)<a class="headerlink" href="#fitting-regression-models-with-scikit-learn" title="Permalink to this headline"></a></h3>
<p><a class="reference external" href="https://scikit-learn.org/stable/index.html"><strong class="program">scikit-learn</strong></a>
(<strong class="program">sklearn</strong>; <span id="id6">[<a class="reference internal" href="999-bibliography.html#id3">PVG+11</a>]</span>)
is a huge Python package for machine learning
built on top of <strong class="program">numpy</strong>, <strong class="program">scipy</strong>, and <strong class="program">matplotlib</strong>.
It has a nicely consistent API and implements or provides
wrappers for many regression, classification, clustering, and
dimensionality reduction algorithms (amongst others).</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong class="program">sklearn</strong> is very convenient but
allows for fitting models even if we do not understand
the mathematics behind them. This is dangerous – it is
like driving a manual without a driver’s license only on an autopilot.
Advanced students and practitioners will appreciate it, but
if used by beginners, it needs to be handled with care;
we should not mistake something’s being easily accessible
with its being safe to use.
Too many bad models go into production and make our daily lives harder.
Hence, as a rule, if given a function implementing some
procedure and we are not able to provide its definition/mathematical
properties/explain its idealised version using pseudocode,
we should refrain from using it.</p>
</div>
<p>Because of the above, we shall only demo the package’s API.
Let us do that by fitting a multiple linear regression model
for, again, weight as a function of arm  and hip circumference:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">body</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]]</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">body</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="kn">import</span> <span class="nn">sklearn.linear_model</span>
<span class="n">lm</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">lm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)</span>
<span class="n">lm</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">coef_</span>
<span class="c1">## LinearRegression()</span>
<span class="c1">## (-63.383425410947524, array([1.30457807, 0.8986582 ]))</span>
</pre></div>
</div>
<p>In <strong class="program">scikit-learn</strong>, once we construct an object representing
the model to be fitted, the <strong class="command">fit</strong> method will determine the optimal
parameters. We have obtained exactly the same solution – it is the
same method, after all.</p>
<p>Computing the predicted values can be done by means
of the <strong class="command">predict</strong> method.
For example, we can calculate the coefficient of determination:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span>
<span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="c1">## 0.9243996585518783</span>
</pre></div>
</div>
<p>This function is convenient, but do we remember the formula for
the score? We should always do.</p>
</div>
<div class="section" id="ill-conditioned-model-matrices">
<span id="sec-ill-condition"></span><h3><span class="section-number">9.2.9. </span>Ill-Conditioned Model Matrices (*)<a class="headerlink" href="#ill-conditioned-model-matrices" title="Permalink to this headline"></a></h3>
<p>Our approach to regression analysis relies on solving
an optimisation problem (the method least squares).
However, sometimes the “optimal” solution that the algorithm returns
might have nothing to do with the <em>true</em> minimum.
And this is despite the fact that we have the theoretical
results (the objective is convex) stating that the solution
is unique (there are methods in statistical learning
where there might be multiple local minima – this is even more difficult;
see <a class="reference internal" href="430-group-by.html#sec-local-minima"><span class="std std-numref">Section 12.4.4</span></a>).
The problem stems from our using the computer’s finite-precision
floating point arithmetic.</p>
<p>Let us fit a degree-4 polynomial to the life expectancy vs per capita GDP
dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">world</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">0</span><span class="p">]]</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">cs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</pre></div>
</div>
<p>We store the estimated model coefficients in a dictionary,
because many methods will follow next.
First, <strong class="command">scipy</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">res</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)</span>
<span class="n">cs</span><span class="p">[</span><span class="s2">&quot;scipy_X&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">cs</span><span class="p">[</span><span class="s2">&quot;scipy_X&quot;</span><span class="p">]</span>
<span class="c1">## array([ 2.33103950e-16,  6.31514064e-12,  1.34162021e-07,</span>
<span class="c1">##        -2.33980973e-12,  1.03490968e-17])</span>
</pre></div>
</div>
<p>If we drew the fitted polynomial (see the figure below),
we would see that the fit is actually very very bad.
The result returned by <strong class="command">lstsq</strong> in this case is not at all optimal.
It turns out that the fitting problem is very <em>ill-conditioned</em>
(and it is not the algorithm’s fault): GDPs range from very small
to very large ones, and taking the powers of 4 thereof
results in numbers of ever greater range.
Finding the least squares solution involves some matrix inverse
(not necessarily directly) and such a matrix might be close to singular.</p>
<p>As a measure of the model matrix’s ill-conditioning,
we often use the so-called condition number,
being the ratio of the largest to the smallest so-called <em>singular values</em><a class="footnote-reference brackets" href="#footsingularvalue" id="id7">6</a>
of <span class="math notranslate nohighlight">\(\mathbf{X}^T\)</span>, which are in fact returned by the <strong class="command">lstsq</strong> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">s</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="c1">## 9.101246653543121e+19</span>
</pre></div>
</div>
<p>As a rule of thumb, if the condition number is <span class="math notranslate nohighlight">\(10^k\)</span>,
we are losing <span class="math notranslate nohighlight">\(k\)</span> digits of numerical precision when performing
the underlying computations. This is thus a very ill-conditioned problem,
because the above number is very large.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>(**) The least squares regression problem can be solved by means
of the singular value decomposition of the model matrix,
see <a class="reference internal" href="#sec-svd"><span class="std std-numref">Section 9.3.4</span></a>.
Let <span class="math notranslate nohighlight">\(\mathbf{U}\mathbf{S}\mathbf{Q}\)</span> be the SVD of
<span class="math notranslate nohighlight">\(\mathbf{X}^T\)</span>.
Then <span class="math notranslate nohighlight">\(\mathbf{c} = \mathbf{U} (1/\mathbf{S}) \mathbf{Q} \mathbf{y}\)</span>,
with <span class="math notranslate nohighlight">\((1/\mathbf{S})=\mathrm{diag}(1/s_{1,1},\dots,1/s_{m,m})\)</span>.
Interestingly, as <span class="math notranslate nohighlight">\(s_{1,1}\ge\dots\ge s_{m,m}\)</span> gives the singular
values of <span class="math notranslate nohighlight">\(\mathbf{X}^T\)</span> (square roots of the eigenvalue of
<span class="math notranslate nohighlight">\(\mathbf{X}^T \mathbf{X}\)</span>), the aforementioned
condition number can simply be computed as <span class="math notranslate nohighlight">\(s_{1,1}/s_{m,m}\)</span>.</p>
</div>
<p>Let us verify the approach used by <strong class="program">scikit-learn</strong>, which
fits the intercept separately (but still, it is merely a
wrapper around <strong class="command">lstsq</strong> with a different API),
and so should be slightly better-behaving:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sklearn.linear_model</span>
<span class="n">lm</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">lm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">y_true</span><span class="p">)</span>
<span class="n">cs</span><span class="p">[</span><span class="s2">&quot;sklearn&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">lm</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">coef_</span><span class="p">]</span>
<span class="n">cs</span><span class="p">[</span><span class="s2">&quot;sklearn&quot;</span><span class="p">]</span>
<span class="c1">## LinearRegression()</span>
<span class="c1">## array([ 6.92257641e+01,  5.05754568e-13,  1.38835855e-08,</span>
<span class="c1">##        -2.18869526e-13,  9.09347414e-19])</span>
</pre></div>
</div>
<p>Here is the condition number:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lm</span><span class="o">.</span><span class="n">singular_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">lm</span><span class="o">.</span><span class="n">singular_</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="c1">## 1.4025984282521556e+16</span>
</pre></div>
</div>
<p>The condition number is also huge – and <strong class="program">scikit-learn</strong>
did not warn us about
it. Had we trusted the solution returned by it, we would end up
with conclusions from our data analysis built on sand.</p>
<p>If the model matrix is almost singular, the computation of its inverse
is prone to enormous numerical errors.
One way of dealing with this is to remove highly correlated variables
(the multicollinearity problem).</p>
<p>Interestingly, standardisation can <em>sometimes</em> make the fitting more
numerically
stable. Let <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> be a standardised version of the model matrix
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span> with the intercept part (the column of 1s) not included,
i.e., with <span class="math notranslate nohighlight">\(\mathbf{z}_{\cdot,j} = (\mathbf{x}_{\cdot,j}-\bar{x}_j)/s_j\)</span>
with <span class="math notranslate nohighlight">\(\bar{x}_j\)</span> and <span class="math notranslate nohighlight">\(s_j\)</span> denoting the arithmetic mean
and standard deviation of the <em>j</em>-th column in <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.
If <span class="math notranslate nohighlight">\((d_1,\dots,d_{m-1})\)</span> is the least squares solution
for <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>, then the least squares solution to the underlying
original regression problem is</p>
<div class="math notranslate nohighlight">
\[
\left(
    \bar{y}-\sum_{j=1}^{m-1} \frac{d_j}{s_j} \bar{x}_j,
    \frac{d_1}{s_1},
    \frac{d_2}{s_2},
    \dots,
    \frac{d_{m-1}}{s_{m-1}}
\right)
\]</div>
<p>with the first term corresponding to the intercept.</p>
<p>Let us test this approach with <strong class="command">scipy.linalg.lstsq</strong>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">stds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">-</span><span class="n">means</span><span class="p">)</span><span class="o">/</span><span class="n">stds</span>
<span class="n">resZ</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">y_true</span><span class="p">)</span>
<span class="n">c_scipyZ</span> <span class="o">=</span> <span class="n">resZ</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">stds</span>
<span class="n">cs</span><span class="p">[</span><span class="s2">&quot;scipy_Z&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">c_scipyZ</span> <span class="o">@</span> <span class="n">means</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">c_scipyZ</span><span class="p">]</span>
<span class="n">cs</span><span class="p">[</span><span class="s2">&quot;scipy_Z&quot;</span><span class="p">]</span>
<span class="c1">## array([ 6.35946784e+01,  1.04541932e-03, -2.41992445e-08,</span>
<span class="c1">##         2.39133533e-13, -8.13307828e-19])</span>
</pre></div>
</div>
<p>The condition number is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="n">resZ</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">s</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="c1">## 139.4279225737235</span>
</pre></div>
</div>
<p>This is still not too good (we would prefer a value close to 1)
but nevertheless way better.</p>
<p><a class="reference internal" href="#fig-ill-cond"><span class="std std-numref">Figure 9.15</span></a> depicts the three fitted models,
each claiming to be <em>the</em> solution to the
original regression problem.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">101</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">_X</span> <span class="o">=</span> <span class="n">_x</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span>
<span class="k">for</span> <span class="n">lab</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">cs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">ssr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_true</span><span class="o">-</span><span class="n">c</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">c</span> <span class="o">@</span> <span class="n">_X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">lab</span><span class="si">:</span><span class="s2">10</span><span class="si">}</span><span class="s2"> SSR=</span><span class="si">{</span><span class="n">ssr</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;per capita GDP PPP&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;life expectancy (years)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id35">
<span id="fig-ill-cond"></span><img alt="../_images/ill-cond-29.png" src="../_images/ill-cond-29.png" />
<p class="caption"><span class="caption-number">Figure 9.15 </span><span class="caption-text">Ill-conditioned model matrix can result in the resulting models be very wrong</span><a class="headerlink" href="#id35" title="Permalink to this image"></a></p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Always check the model matrix’s condition number.</p>
</div>
<p>To be strict, if we read a paper in, say, social or medical sciences
(amongst others) where the researchers fit a regression model but
do not provide the model matrix’s condition number,
we should doubt the conclusions they make.</p>
<p>On a final note, we might wonder why the standardisation
is not done automatically by the least squares solver.
As usual with most numerical methods, there is no one-fits-all solution:
e.g., when there are columns of very small variance
or there are outliers in data.
This is why we need to study all the topics deeply:
to be able to respond flexibly
to many different scenarios ourselves.</p>
</div>
</div>
<div class="section" id="finding-interesting-combinations-of-variables">
<h2><span class="section-number">9.3. </span>Finding Interesting Combinations of Variables (*)<a class="headerlink" href="#finding-interesting-combinations-of-variables" title="Permalink to this headline"></a></h2>
<div class="section" id="dot-products-angles-collinearity-and-orthogonality">
<span id="sec-dot-cosine"></span><h3><span class="section-number">9.3.1. </span>Dot Products, Angles, Collinearity, and Orthogonality<a class="headerlink" href="#dot-products-angles-collinearity-and-orthogonality" title="Permalink to this headline"></a></h3>
<p>It turns out that the dot product (<a class="reference internal" href="320-transform-matrix.html#sec-matrix-multiply"><span class="std std-numref">Section 8.3</span></a>)
has a nice geometrical interpretation:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{x}\cdot\boldsymbol{y} = \|\boldsymbol{x}\|\, \|\boldsymbol{y}\|\,
\cos\alpha,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is the angle between the two vectors.
In other words, it is the product of the lengths of the two vectors
and the cosine of the angle between them.</p>
<p>Note that we can get the cosine part by computing the dot product
of the <em>normalised</em> vectors, i.e., such that their lengths are equal to 1:</p>
<div class="math notranslate nohighlight">
\[
\cos\alpha = \frac{\boldsymbol{x}}{\|\boldsymbol{x}\|}\cdot\frac{\boldsymbol{y}}{\|\boldsymbol{y}\|}.
\]</div>
<p>For example, consider two vectors in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>,
<span class="math notranslate nohighlight">\((1/2, 0)\)</span> and <span class="math notranslate nohighlight">\((\sqrt{2}/2, \sqrt{2}/2)\)</span>,
which are depicted in <a class="reference internal" href="#fig-two-vectors-angle"><span class="std std-numref">Figure 9.16</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
<p>Their dot product is equal to:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">u</span><span class="o">*</span><span class="n">v</span><span class="p">)</span>
<span class="c1">## 0.3535533905932738</span>
</pre></div>
</div>
<p>The dot product of their normalised versions, i.e., the cosine
of the angle between them is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">u_norm</span> <span class="o">=</span> <span class="n">u</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">u</span><span class="o">*</span><span class="n">u</span><span class="p">))</span>
<span class="n">v_norm</span> <span class="o">=</span> <span class="n">v</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">v</span><span class="o">*</span><span class="n">v</span><span class="p">))</span>  <span class="c1"># BTW: this vector was already normalised</span>
<span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">u_norm</span><span class="o">*</span><span class="n">v_norm</span><span class="p">)</span>
<span class="c1">## 0.7071067811865476</span>
</pre></div>
</div>
<p>The angle itself can be determined by referring to the
inverse of the cosine function, i.e., arccosine.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">arccos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">u_norm</span><span class="o">*</span><span class="n">v_norm</span><span class="p">))</span> <span class="o">*</span> <span class="mi">180</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span>
<span class="c1">## 45.0</span>
</pre></div>
</div>
<p>Note that we have converted the angle from radians to degrees.</p>
<div class="figure align-default" id="id36">
<span id="fig-two-vectors-angle"></span><img alt="../_images/two-vectors-angle-31.png" src="../_images/two-vectors-angle-31.png" />
<p class="caption"><span class="caption-number">Figure 9.16 </span><span class="caption-text">Example vectors and the angle between them</span><a class="headerlink" href="#id36" title="Permalink to this image"></a></p>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If two vectors are collinear
(<em>codirectional</em>, one is a scaled version of another, angle 0),
then <span class="math notranslate nohighlight">\(\cos 0 = 1\)</span>. If they point to opposite directions
(-180 degrees angle), then <span class="math notranslate nohighlight">\(\cos \pi=-1\)</span>.
For <em>orthogonal</em> (perpendicular, 90 or -90 degree angle) vectors,
we get <span class="math notranslate nohighlight">\(\cos(\pi/2)=\cos(-\pi/2)=0\)</span>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>(**)
Note that the standard deviation <span class="math notranslate nohighlight">\(s\)</span> of a vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>
that has been centred (whose mean is 0) is a scaled version
of the Euclidean norm (divided by the square root
of the number of elements, <span class="math notranslate nohighlight">\(n\)</span>), i.e., <span class="math notranslate nohighlight">\(s = \|\boldsymbol{x}\|/\sqrt{n}\)</span>.
Looking at the definition of the Pearson linear correlation coefficient,
we see that it is the dot product of the standardised versions
of two vectors <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> divided by
the number of elements therein. However, if the vectors are
centred, we can rewrite the formula equivalently as
<span class="math notranslate nohighlight">\(r(\boldsymbol{x}, \boldsymbol{y})=
\frac{\boldsymbol{x}}{\|\boldsymbol{x}\|}\cdot\frac{\boldsymbol{y}}{\|\boldsymbol{y}\|}= \cos\alpha\)</span>. It is not easy to imagine
vectors in very high dimensional spaces, but at least the fact
that <span class="math notranslate nohighlight">\(r\)</span> is bounded by -1 and 1 is implied from this observation.</p>
</div>
</div>
<div class="section" id="geometric-transformations-of-points">
<span id="sec-geometric-transform"></span><h3><span class="section-number">9.3.2. </span>Geometric Transformations of Points<a class="headerlink" href="#geometric-transformations-of-points" title="Permalink to this headline"></a></h3>
<p>For certain square matrices of size <em>m</em> by <em>m</em>,
matrix multiplication can be thought of as an application
of the corresponding geometrical transformation.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> be a matrix of shape <em>n</em> by <em>m</em>,
which we treat as representing the coordinates of
<em>n</em> points in an <em>m</em>-dimensional space.
For instance, if we are given a diagonal matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{S}=\mathrm{diag}(s_1, s_2,\dots, s_m)=
\left[
\begin{array}{cccc}
s_1 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; s_2 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; s_m \\
\end{array}
\right],
\end{split}\]</div>
<p>then <span class="math notranslate nohighlight">\(\mathbf{X} \mathbf{S}\)</span> represents <em>scaling</em> (stretching) with respect
to the axes of the coordinate system in use, because:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X} \mathbf{S} =
\left[
\begin{array}{cccc}
s_1 x_{1,1} &amp; s_2 x_{1,2} &amp; \dots &amp; s_m x_{1,m} \\
s_1 x_{2,1} &amp; s_2 x_{2,2} &amp; \dots &amp; s_m x_{2,m} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
s_1 x_{n-1,1} &amp; s_2 x_{n-1,2} &amp; \dots &amp; s_m x_{n-1,m} \\
s_1 x_{n,1} &amp; s_2 x_{n,2} &amp; \dots &amp; s_m x_{n,m} \\
\end{array}
\right].
\end{split}\]</div>
<p>Note that in <strong class="program">numpy</strong> this can be implemented without
actually referring to matrix multiplication;
a notation like <code class="docutils literal notranslate"><span class="pre">X</span> <span class="pre">*</span> <span class="pre">np.array([s1,</span> <span class="pre">s2,</span> <span class="pre">...,</span> <span class="pre">sm]).reshape(1,</span> <span class="pre">-1)</span></code>
will suffice (elementwise multiplication and proper shape broadcasting).</p>
<p>Furthermore, if <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> is an <em>orthonormal</em> matrix, i.e.,
a square matrix whose columns and rows
are unit vectors (normalised), all orthogonal to each other,
then <span class="math notranslate nohighlight">\(\mathbf{X}\mathbf{Q}\)</span> represents a combination
of rotations and reflections.</p>
<p>Orthonormal matrices are sometimes simply referred to as
orthogonal ones.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>By definition, a matrix <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span>
is <em>orthonormal</em> if and only if
<span class="math notranslate nohighlight">\(\mathbf{Q}^T \mathbf{Q} = \mathbf{Q} \mathbf{Q}^T = \mathbf{I}\)</span>.
It is due to the <span class="math notranslate nohighlight">\(\cos(\pi/2) = \cos(-\pi/2)=0\)</span> interpretation
of the dot products of normalised orthogonal vectors.</p>
</div>
<p>In particular, for any angle <span class="math notranslate nohighlight">\(\alpha\)</span>, the matrix representing
rotations in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{R}(\alpha) = \left[
\begin{array}{cc}
 \cos \alpha &amp; \sin\alpha \\
-\sin \alpha &amp; \cos\alpha \\
\end{array}
\right],
\end{split}\]</div>
<p>is orthonormal (which can be easily verified using the basic
trigonometric equalities).</p>
<p>Furthermore,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left[
\begin{array}{cc}
 1 &amp; 0 \\
 0 &amp; -1 \\
\end{array}
\right]
\quad\text{ and }\quad
\left[
\begin{array}{cc}
 -1 &amp; 0 \\
 0 &amp; 1 \\
\end{array}
\right]
\end{split}\]</div>
<p>represent the two reflections, one
against the x- and the other against the y-axis, respectively.
Both are orthonormal matrices as well.</p>
<div style="margin-top: 1em"></div><p>Consider a dataset <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">12345</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.25</span>
</pre></div>
</div>
<p>and its scaled, rotated, and translated (shifted) version</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{Y} =
\mathbf{X}\ \left[
\begin{array}{cc}
 2 &amp; 0 \\
0 &amp; 0.5 \\
\end{array}
\right]\ \left[
\begin{array}{cc}
 \cos \frac{\pi}{6} &amp; \sin\frac{\pi}{6} \\
-\sin \frac{\pi}{6} &amp; \cos\frac{\pi}{6} \\
\end{array}
\right] + \left[
\begin{array}{cc}
3 &amp; 2 \\
\end{array}
\right].
\end{split}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="n">S</span>
<span class="c1">## array([[2. , 0. ],</span>
<span class="c1">##        [0. , 0.5]])</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">6</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">alpha</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">alpha</span><span class="p">)],</span>
    <span class="p">[</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">alpha</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">alpha</span><span class="p">)]</span>
<span class="p">])</span>
<span class="n">Q</span>
<span class="c1">## array([[ 0.8660254,  0.5      ],</span>
<span class="c1">##        [-0.5      ,  0.8660254]])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">S</span> <span class="o">@</span> <span class="n">Q</span> <span class="o">+</span> <span class="n">t</span>
</pre></div>
</div>
<div class="figure align-default" id="id37">
<span id="fig-geom-trans"></span><img alt="../_images/geom-trans-33.png" src="../_images/geom-trans-33.png" />
<p class="caption"><span class="caption-number">Figure 9.17 </span><span class="caption-text">A dataset and its scaled, rotated, and shifted version</span><a class="headerlink" href="#id37" title="Permalink to this image"></a></p>
</div>
<p>We can consider <span class="math notranslate nohighlight">\(\mathbf{Y}=\mathbf{X} \mathbf{S} \mathbf{Q} + \mathbf{t}\)</span>
a version of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> in a new coordinate system (basis),
see <a class="reference internal" href="#fig-geom-trans"><span class="std std-numref">Figure 9.17</span></a>.
Each column in the transformed matrix is a
shifted linear combination of the columns in the original matrix:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{y}_{\cdot,j} = t_j + \sum_{k=1}^m (s_{k, k} q_{k, j}) \mathbf{x}_{\cdot, k}.
\]</div>
<p>Computing such linear combinations of columns is natural during
a dataset’s preprocessing step, especially if they are on the
same scale or they are unitless. Actually, we can note
that standardisation itself is a form of scaling and translation.</p>
<div class="proof proof-type-exercise" id="id38">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.11</span>
        
    </div><div class="proof-content">
<p>Assume that we have a dataset with two columns,
giving the number of apples and the number of oranges in a fresh veggie market
clients’ baskets. What kind of orthonormal
and scaling transforms should be applied to obtain a matrix
bearing the total number of fruits and surplus apples
(e.g., a row <span class="math notranslate nohighlight">\((4, 7)\)</span> should be converted to <span class="math notranslate nohighlight">\((11, -3)\)</span>)?</p>
</div></div></div>
<div class="section" id="matrix-inverse">
<span id="sec-matrix-inverse"></span><h3><span class="section-number">9.3.3. </span>Matrix Inverse<a class="headerlink" href="#matrix-inverse" title="Permalink to this headline"></a></h3>
<p>The <em>inverse</em> of a square matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>
(if it exists) is denoted with <span class="math notranslate nohighlight">\(\mathbf{A}^{-1}\)</span> – it is the matrix
fulfilling the identity</p>
<div class="math notranslate nohighlight">
\[\mathbf{A}^{-1} \mathbf{A} = \mathbf{A} \mathbf{A}^{-1} = \mathbf{I}.\]</div>
<p>Noting that the identity matrix <span class="math notranslate nohighlight">\(\mathbf{I}\)</span> is the neutral element
of the matrix multiplication, the above is thus the analogue
of the inverse of a scalar: something like
<span class="math notranslate nohighlight">\(3 \cdot 3^{-1} = 3\cdot \frac{1}{3} = \frac{1}{3} \cdot 3 = 1\)</span>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>For any invertible matrices of admissible shapes,
it might be shown that the following noteworthy properties hold:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((\mathbf{A}^{-1})^T = (\mathbf{A}^T)^{-1}\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\((\mathbf{A}\mathbf{B})^{-1} = \mathbf{B}^{-1} \mathbf{A}^{-1}\)</span>,</p></li>
<li><p>a matrix equality <span class="math notranslate nohighlight">\(\mathbf{A}=\mathbf{B}\mathbf{C}\)</span> holds if and only if
<span class="math notranslate nohighlight">\(\mathbf{A} \mathbf{C}^{-1}=\mathbf{B}\mathbf{C}\mathbf{C}^{-1}=\mathbf{B}\)</span>;
this is also equivalent to
<span class="math notranslate nohighlight">\(\mathbf{B}^{-1} \mathbf{A}=\mathbf{B}^{-1} \mathbf{B}\mathbf{C}=\mathbf{C}\)</span>.</p></li>
</ul>
</div>
<p>Matrix inverse allows us to identify the inverses of geometrical
transformations.
Knowing that <span class="math notranslate nohighlight">\(\mathbf{Y} = \mathbf{X}\mathbf{S}\mathbf{Q}+\mathbf{t}\)</span>,
we can recreate the original matrix by applying:</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X} = (\mathbf{Y}-\mathbf{t}) (\mathbf{S}\mathbf{Q})^{-1}
= (\mathbf{Y}-\mathbf{t}) \mathbf{Q}^{-1} \mathbf{S}^{-1}.
\]</div>
<p>It is worth knowing that if <span class="math notranslate nohighlight">\(\mathbf{S}=\mathrm{diag}(s_1,s_2,\dots,s_m)\)</span>
is a diagonal matrix, then its inverse is
<span class="math notranslate nohighlight">\(\mathbf{S}^{-1} = \mathrm{diag}(1/s_1,1/s_2,\dots,1/s_m)\)</span>.</p>
<p>Furthermore, the inverse of an orthonormal matrix <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span>
is always equal to its transpose,
<span class="math notranslate nohighlight">\(\mathbf{Q}^{-1}=\mathbf{Q}^T\)</span>.</p>
<p>Luckilly, we will not be inverting other matrices in this introductory course.</p>
<p>Let us verify this numerically
(testing equality up to some inherent round-off error):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">(</span><span class="n">Y</span><span class="o">-</span><span class="n">t</span><span class="p">)</span> <span class="o">@</span> <span class="n">Q</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S</span><span class="p">)))</span>
<span class="c1">## True</span>
</pre></div>
</div>
</div>
<div class="section" id="singular-value-decomposition">
<span id="sec-svd"></span><h3><span class="section-number">9.3.4. </span>Singular Value Decomposition<a class="headerlink" href="#singular-value-decomposition" title="Permalink to this headline"></a></h3>
<p>It turns out that given any real <em>n</em>-by-<em>m</em> matrix <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>
with <span class="math notranslate nohighlight">\(n\ge m\)</span>, we can find a very interesting scaling and orthonormal
transforms that, when applied on a dataset whose columns are already
normalised, yield exactly <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>.</p>
<p>Namely, the singular value decomposition (SVD in the so-called compact form)
is a factorisation</p>
<div class="math notranslate nohighlight">
\[\mathbf{Y} = \mathbf{U}\mathbf{S}\mathbf{Q},\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{U}\)</span> is an <em>n</em>-by-<em>m</em> semi-orthonormal matrix
(its columns are orthonormal vectors;
it holds <span class="math notranslate nohighlight">\(\mathbf{U}^T \mathbf{U} = \mathbf{I}\)</span>);</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{S}\)</span> is an <em>m</em>-by-<em>m</em> diagonal matrix such
that <span class="math notranslate nohighlight">\(s_{1,1}\ge s_{2,2}\ge\dots\ge s_{m,m}\ge 0\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> is an <em>m</em>-by-<em>m</em> orthonormal matrix.</p></li>
</ul>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>In data analysis, we usually apply the SVD on matrices that
have already been centred (so that their column means are all 0).</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.linalg</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">Y_centred</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">Y_centred</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>And now:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">U</span><span class="p">[:</span><span class="mi">6</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># preview first few rows</span>
<span class="c1">## array([[-0.00195072,  0.00474569],</span>
<span class="c1">##        [-0.00510625, -0.00563582],</span>
<span class="c1">##        [ 0.01986719,  0.01419324],</span>
<span class="c1">##        [ 0.00104386,  0.00281853],</span>
<span class="c1">##        [ 0.00783406,  0.01255288],</span>
<span class="c1">##        [ 0.01025205, -0.0128136 ]])</span>
</pre></div>
</div>
<p>The norms of all the columns in <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> are all equal to 1
(and hence standard deviations are <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span>),
thus they are on the same scale:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>  <span class="c1"># compare</span>
<span class="c1">## (array([0.01, 0.01]), 0.01)</span>
</pre></div>
</div>
<p>Furthermore, they are orthogonal: their dot products
are all equal to <span class="math notranslate nohighlight">\(0\)</span>. Regarding what we have said about Pearson’s
linear correlation coefficients and its relation to dot products of normalised vectors, we imply that the columns in <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> are not
linearly correlated – they form <em>independent</em> dimensions.</p>
<p>Now <span class="math notranslate nohighlight">\(\mathbf{S} = \mathbf{diag}(s_1,\dots,s_m)\)</span>,
with the elements on the diagonal being</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">s</span>
<span class="c1">## array([49.72180455, 12.5126241 ])</span>
</pre></div>
</div>
<p>is used to scale each column in <span class="math notranslate nohighlight">\(\mathbf{U}\)</span>.
The fact that the elements on the diagonal are ordered decreasingly
means that the first column in <span class="math notranslate nohighlight">\(\mathbf{U}\mathbf{S}\)</span>
has the greatest standard deviation,
the second column has the second greatest variability,
and so forth.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="n">US</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">S</span>
<span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">US</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># equal to s/np.sqrt(n)</span>
<span class="c1">## array([0.49721805, 0.12512624])</span>
</pre></div>
</div>
<p>Multiplying <span class="math notranslate nohighlight">\(\mathbf{U}\mathbf{S}\)</span> by <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> simply
rotates and/or reflects the dataset.
This brings  <span class="math notranslate nohighlight">\(\mathbf{U}\mathbf{S}\)</span> to a new coordinate system
where, by construction, the dataset projected onto the direction
determined by the first row in <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span>, i.e., <span class="math notranslate nohighlight">\(\mathbf{q}_{1,\cdot}\)</span>
has the largest variance,
projection onto <span class="math notranslate nohighlight">\(\mathbf{q}_{2,\cdot}\)</span> has the second largest variance,
and so on.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Q</span>
<span class="c1">## array([[ 0.86781968,  0.49687926],</span>
<span class="c1">##        [-0.49687926,  0.86781968]])</span>
</pre></div>
</div>
<p>This is why we refer to the rows in <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span>
as <em>principal directions</em> or <em>components</em>. Their scaled versions (proportional
to the standard deviations along them) are depicted in
<a class="reference internal" href="#fig-principal-directions"><span class="std std-numref">Figure 9.18</span></a>.
Note that this way we have more or less recreated the steps needed
to construct <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> from <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> above
(by the way we generated <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, we expect it to
have linearly uncorrelated columns;
yet note that <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> have different
column variances).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Y_centred</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Y_centred</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span>
    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Q</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">Q</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">),</span>
    <span class="n">width</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">length_includes_head</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span>
    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Q</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">Q</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">),</span>
    <span class="n">width</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">length_includes_head</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id39">
<span id="fig-principal-directions"></span><img alt="../_images/principal-directions-35.png" src="../_images/principal-directions-35.png" />
<p class="caption"><span class="caption-number">Figure 9.18 </span><span class="caption-text">Principal directions of an example dataset (scaled so that they are proportional to the standard deviations along them)</span><a class="headerlink" href="#id39" title="Permalink to this image"></a></p>
</div>
</div>
<div class="section" id="dimensionality-reduction-with-svd">
<h3><span class="section-number">9.3.5. </span>Dimensionality Reduction with SVD<a class="headerlink" href="#dimensionality-reduction-with-svd" title="Permalink to this headline"></a></h3>
<p>Let us consider the following example three dimensional dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">chainlink</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching_data/master/clustering/fcps_chainlink.csv&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>As we have told in <a class="reference internal" href="310-matrix.html#sec-visual-multidim"><span class="std std-numref">Section 7.4</span></a>,
plotting is always done on a 2-dimensional surface (be it the computer
screen or book page), therefore we can look at the dataset
only from one <em>angle</em> at a time.</p>
<p>In particular, a scatterplot matrix only depicts the dataset
from the perspective of the axes of the Cartesian coordinate system
(standard basis), see <a class="reference internal" href="#fig-chainlink-pairplot"><span class="std std-numref">Figure 9.19</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">chainlink</span><span class="p">))</span>
<span class="c1"># plt.show()  # not needed :/</span>
</pre></div>
</div>
<div class="figure align-default" id="id40">
<span id="fig-chainlink-pairplot"></span><img alt="../_images/chainlink-pairplot-37.png" src="../_images/chainlink-pairplot-37.png" />
<p class="caption"><span class="caption-number">Figure 9.19 </span><span class="caption-text">Views from the perspective of the main axes</span><a class="headerlink" href="#id40" title="Permalink to this image"></a></p>
</div>
<p>These viewpoints by no means must reveal the true geometric
structure of the dataset. We know that we can actually rotate the virtual
camera and find some more <em>interesting</em> angle.
It turns out that our dataset represents two nonintersecting rings,
hopefully visible <a class="reference internal" href="#fig-chainlink-rotated"><span class="std std-numref">Figure 9.20</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s2">&quot;3d&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">chainlink</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">chainlink</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">chainlink</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">azim</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">vertical_axis</span><span class="o">=</span><span class="s2">&quot;z&quot;</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s2">&quot;3d&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">chainlink</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">chainlink</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">chainlink</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mi">37</span><span class="p">,</span> <span class="n">azim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vertical_axis</span><span class="o">=</span><span class="s2">&quot;z&quot;</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s2">&quot;3d&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">chainlink</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">chainlink</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">chainlink</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">azim</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">vertical_axis</span><span class="o">=</span><span class="s2">&quot;z&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id41">
<span id="fig-chainlink-rotated"></span><img alt="../_images/chainlink-rotated-39.png" src="../_images/chainlink-rotated-39.png" />
<p class="caption"><span class="caption-number">Figure 9.20 </span><span class="caption-text">Different views</span><a class="headerlink" href="#id41" title="Permalink to this image"></a></p>
</div>
<p>It turns out that we may find (one of the many) such an noteworthy
viewpoint using the SVD. Namely, we can perform the decomposition
of a centred dataset which we denote with <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbf{Y} = \mathbf{U}\mathbf{S}\mathbf{Q}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.linalg</span>
<span class="n">Y_centered</span> <span class="o">=</span> <span class="n">chainlink</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">chainlink</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">Y_centered</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Then, considering its rotated/reflected version</p>
<div class="math notranslate nohighlight">
\[\mathbf{T}=\mathbf{Y} \mathbf{Q}^{-1} = \mathbf{U}\mathbf{S}\]</div>
<p>we know that its first column has the highest variance.
Furthermore, the second column has the second highest variability,
and so on.
It might indeed be worth to look at that dataset from that
<em>most informative</em> perspective.</p>
<p><a class="reference internal" href="#fig-chainlink-svd"><span class="std std-numref">Figure 9.21</span></a> gives the scatter plot for <span class="math notranslate nohighlight">\(\mathbf{t}_{\cdot,1}\)</span>
and <span class="math notranslate nohighlight">\(\mathbf{t}_{\cdot,2}\)</span>. Maybe this does not reveal the true
geometric structure of the dataset (no single two-dimensional projection
would do that here), but at least it is better than the initial
ones (from the pairplot).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">T2</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># the same as (U@np.diag(s))[:, :2]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">T2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">T2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id42">
<span id="fig-chainlink-svd"></span><img alt="../_images/chainlink-svd-41.png" src="../_images/chainlink-svd-41.png" />
<p class="caption"><span class="caption-number">Figure 9.21 </span><span class="caption-text">The view from the two principal axes</span><a class="headerlink" href="#id42" title="Permalink to this image"></a></p>
</div>
<p>What we have thus done was a kind of <em>dimensionality reduction</em> –
we have found a viewpoint (in the form of an orthonormal matrix, being
a mixture of rotations and reflections) on <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>
such that its orthonormal projection onto the
first two axes of the Cartesian coordinate system
is the most informative (in terms of having the highest variance,
axiswisely).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>(**) The Eckart–Young–Mirsky theorem states that
<span class="math notranslate nohighlight">\(\mathbf{U}_{\cdot, :k} \mathbf{S}_{:k, :k} \mathbf{Q}_{:k, \cdot}\)</span>
(where “<em>:k</em>” denotes “first <em>k</em> rows or columns”)
is actually the best rank-<span class="math notranslate nohighlight">\(k\)</span> approximation of <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>
with respect to both the Frobenius and spectral norms.</p>
</div>
</div>
<div class="section" id="principal-component-analysis">
<h3><span class="section-number">9.3.6. </span>Principal Component Analysis<a class="headerlink" href="#principal-component-analysis" title="Permalink to this headline"></a></h3>
<p><em>Principal component analysis</em> (PCA) is a fancy name for the whole process of
reflecting upon what happens along the projections onto the most
variable dimensions.
It can be used not only for data visualisation and deduplication,
but also feature engineering (as in fact it creates new columns
that are linear combinations of existing ones).</p>
<p>Let us consider a few chosen countrywise 2016
<a class="reference external" href="https://ssi.wi.th-koeln.de/">Sustainable Society Indices</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ssi</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching_data/master/marek/ssi_2016_indicators.csv&quot;</span><span class="p">,</span>
    <span class="n">comment</span><span class="o">=</span><span class="s2">&quot;#&quot;</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">ssi</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">19</span><span class="p">]</span> <span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">Y</span><span class="p">[:</span><span class="mi">6</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># preview</span>
<span class="c1">## array([[ 9.32      ,  8.13333333,  8.386     ,  8.5757    ,  5.46249573],</span>
<span class="c1">##        [ 8.74      ,  7.71666667,  7.346     ,  6.8426    ,  6.2929302 ],</span>
<span class="c1">##        [ 5.11      ,  4.31666667,  8.788     ,  9.2035    ,  3.91062849],</span>
<span class="c1">##        [ 9.61      ,  7.93333333,  5.97      ,  5.5232    ,  7.75361284],</span>
<span class="c1">##        [ 8.95      ,  7.81666667,  8.032     ,  8.2639    ,  4.42350654],</span>
<span class="c1">##        [10.        ,  8.65      ,  1.        ,  1.        ,  9.66401848]])</span>
</pre></div>
</div>
<p>Each index is on the scale from 0 to 10.
These are, in this order:</p>
<ol class="simple">
<li><p>Safe Sanitation,</p></li>
<li><p>Healthy Life,</p></li>
<li><p>Energy Use,</p></li>
<li><p>Greenhouse Gases,</p></li>
<li><p>Gross Domestic Product.</p></li>
</ol>
<p>Note that above we have displayed the data corresponding to the
6 following countries:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">countries</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">ssi</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">countries</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>  <span class="c1"># preview</span>
<span class="c1">## [&#39;Albania&#39;, &#39;Algeria&#39;, &#39;Angola&#39;, &#39;Argentina&#39;, &#39;Armenia&#39;, &#39;Australia&#39;]</span>
</pre></div>
</div>
<p>This is a 5-dimensional dataset, thus we cannot easily visualise it.
That the pairplot does not reveal too much is left as an exercise.</p>
<p>Let us thus perform the SVD decomposition of a standardised
version of this dataset (recall that centring is necessary,
at the very least).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Y_std</span> <span class="o">=</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">Y_std</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>The standard deviations of the data projected onto
the consecutive principal components (columns in <span class="math notranslate nohighlight">\(\mathbf{U}\mathbf{S}\)</span>) are:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">s</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="c1">## array([2.02953531, 0.7529221 , 0.3943008 , 0.31897889, 0.23848286])</span>
</pre></div>
</div>
<p>It is customary to check the ratios of the variances
explained by the consecutive principal components, which
is a normalised measure of their importances.
We can compute them by calling:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">s</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">s</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="c1">## array([0.82380272, 0.93718105, 0.96827568, 0.98862519, 1.        ])</span>
</pre></div>
</div>
<p>Thus, we may say that the variability within the first two components
covers 94% of the variability of the whole dataset.
It might thus be a good idea to consider only a 2-dimensional projection
of this dataset (actually, we are quite lucky here – or someone
has selected these countrywise indices for us in a very
clever fashion).</p>
<p>The rows in <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> give us the so-called <em>loadings</em>.
They give the coefficients defining the linear combinations of the
rows in <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> that correspond to the principal components.</p>
<p>Let us try to interpret them.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1">## array([-0.43, -0.43,  0.44,  0.45, -0.47])</span>
</pre></div>
</div>
<p>The first row in <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> consists of similar values,
but with different signs.
We can consider them a scaled version of the average of
Energy Use (column 3), Greenhouse Gases (4), and
MINUS Safe Sanitation (1), MINUS Healthy Life (2),
MINUS Gross Domestic Product (5).
Possibly some kind of a measure of a country’s overall eco-unfriendliness?</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1">## array([ 0.52,  0.5 ,  0.52,  0.45, -0.02])</span>
</pre></div>
</div>
<p>The second row in <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> defines a scaled version of the average
of Safe Sanitation (1), Healthy Life (2), Energy Use (3),
and Greenhouse Gases (4), almost totally ignoring the GDP (5).
Should we call it a measure of industrialisation? Something like this.
But this naming is just for fun; mathematics – unlike the human brain –
does not need our imperfect interpretations/fairy tales to function properly.</p>
<p>In <a class="reference internal" href="#fig-pca-country"><span class="std std-numref">Figure 9.22</span></a> is a scatter plot of the countries projected onto the
said 2 principal directions. For readability, we only display a few
chosen labels.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">T2</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># == Y @ Q[:2, :].T</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">T2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">T2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">which</span> <span class="o">=</span> <span class="p">[</span>   <span class="c1"># hand-crafted/artisan</span>
    <span class="mi">141</span><span class="p">,</span> <span class="mi">117</span><span class="p">,</span> <span class="mi">69</span><span class="p">,</span> <span class="mi">123</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">93</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span>
    <span class="mi">104</span><span class="p">,</span> <span class="mi">122</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">134</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">94</span><span class="p">,</span> <span class="mi">114</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">34</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">77</span><span class="p">,</span>
    <span class="mi">64</span><span class="p">,</span> <span class="mi">67</span><span class="p">,</span> <span class="mi">152</span><span class="p">,</span> <span class="mi">135</span><span class="p">,</span> <span class="mi">148</span><span class="p">,</span> <span class="mi">99</span><span class="p">,</span> <span class="mi">149</span><span class="p">,</span> <span class="mi">126</span><span class="p">,</span> <span class="mi">111</span><span class="p">,</span> <span class="mi">57</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">63</span>
<span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">which</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">T2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">T2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">countries</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">)</span> <span class="c1">#countries[i])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;1st principal component (eco-unfriendliness?)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;2nd principal component (industrialisation?)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure align-default" id="id43">
<span id="fig-pca-country"></span><img alt="../_images/pca-country-43.png" src="../_images/pca-country-43.png" />
<p class="caption"><span class="caption-number">Figure 9.22 </span><span class="caption-text">An example principal component analysis of countries</span><a class="headerlink" href="#id43" title="Permalink to this image"></a></p>
</div>
<p>This is merely a projection, but it might be an interesting one
for some practitioners.</p>
</div>
</div>
<div class="section" id="further-reading">
<h2><span class="section-number">9.4. </span>Further Reading<a class="headerlink" href="#further-reading" title="Permalink to this headline"></a></h2>
<p>Some now-classic introductory text in statistical learning
is <span id="id8">[<a class="reference internal" href="999-bibliography.html#id49">HTF17</a>]</span>.
We recommend <span id="id9">[<a class="reference internal" href="999-bibliography.html#id50">Bis06</a>, <a class="reference internal" href="999-bibliography.html#id112">BHK20</a>, <a class="reference internal" href="999-bibliography.html#id73">DGL96</a>]</span> for more advanced students.</p>
</div>
<div class="section" id="exercises">
<h2><span class="section-number">9.5. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this headline"></a></h2>
<div class="proof proof-type-exercise" id="id44">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.12</span>
        
    </div><div class="proof-content">
<p>What does “correlation is not causation” mean?</p>
</div></div><div class="proof proof-type-exercise" id="id45">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.13</span>
        
    </div><div class="proof-content">
<p>What does linear correlation of 0.9 mean?
How about rank correlation of 0.9?
And linear correlation of 0.0?</p>
</div></div><div class="proof proof-type-exercise" id="id46">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.14</span>
        
    </div><div class="proof-content">
<p>How is the Spearman’s coefficient related to the Pearson’s one?</p>
</div></div><div class="proof proof-type-exercise" id="id47">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.15</span>
        
    </div><div class="proof-content">
<p>How to write a linear model equation using the notion of the dot
product?</p>
</div></div><div class="proof proof-type-exercise" id="id48">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.16</span>
        
    </div><div class="proof-content">
<p>State the optimisation problem behind the least squares fitting
of linear models.</p>
</div></div><div class="proof proof-type-exercise" id="id49">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.17</span>
        
    </div><div class="proof-content">
<p>What are the different ways of the numerical summarising of residuals?</p>
</div></div><div class="proof proof-type-exercise" id="id50">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.18</span>
        
    </div><div class="proof-content">
<p>Why is it important for the residuals to be homoscedastic?</p>
</div></div><div class="proof proof-type-exercise" id="id51">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.19</span>
        
    </div><div class="proof-content">
<p>Is a more complex model always better?</p>
</div></div><div class="proof proof-type-exercise" id="id52">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.20</span>
        
    </div><div class="proof-content">
<p>Why should extrapolation be handled with care?</p>
</div></div><div class="proof proof-type-exercise" id="id53">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.21</span>
        
    </div><div class="proof-content">
<p>What is the difference between a train-test and a train-validate-test
split?</p>
</div></div><div class="proof proof-type-exercise" id="id54">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.22</span>
        
    </div><div class="proof-content">
<p>Why did we say that <strong class="program">scikit-learn</strong> should be used carefully
by novice users?</p>
</div></div><div class="proof proof-type-exercise" id="id55">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.23</span>
        
    </div><div class="proof-content">
<p>What is a condition number of the model matrix and why it should
always be checked?</p>
</div></div><div class="proof proof-type-exercise" id="id56">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.24</span>
        
    </div><div class="proof-content">
<p>What is the geometrical interpretation of the dot product of
two normalised vectors?</p>
</div></div><div class="proof proof-type-exercise" id="id57">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.25</span>
        
    </div><div class="proof-content">
<p>How to verify if two vectors are orthogonal?</p>
</div></div><div class="proof proof-type-exercise" id="id58">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.26</span>
        
    </div><div class="proof-content">
<p>What is an orthonormal projection? What is the inverse of an
orthonormal matrix?</p>
</div></div><div class="proof proof-type-exercise" id="id59">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.27</span>
        
    </div><div class="proof-content">
<p>What is the inverse of a diagonal matrix?</p>
</div></div><div class="proof proof-type-exercise" id="id60">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.28</span>
        
    </div><div class="proof-content">
<p>Characterise the general properties of the three matrices obtained
by performing the singular value decomposition of a given matrix
of shape <em>n</em>-by-<em>m</em>.</p>
</div></div><div class="proof proof-type-exercise" id="id61">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.29</span>
        
    </div><div class="proof-content">
<p>How to obtain the first principal component of a given centred matrix?</p>
</div></div><div class="proof proof-type-exercise" id="id62">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.30</span>
        
    </div><div class="proof-content">
<p>How to compute the ratios of the variances explained by the
consecutive principal components?</p>
</div></div><hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="footspearman"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>If a method Y is nothing else than X on transformed
data, we should not consider it a totally new method.</p>
</dd>
<dt class="label" id="footmodelserialise"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>And thus in order to memorise the model
for a further reference, we only need to serialise
its <em>m</em> coefficients, e.g., in a JSON or CSV file.</p>
</dd>
<dt class="label" id="footwhyssr"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Due to computability
and mathematical analysability, which we usually explore
in more advanced courses on statistical data analysis, which
this one is not.</p>
</dd>
<dt class="label" id="footknnas"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>Note that in such an approach to regression we are not
aiming to minimise anything in particular. If the model
is good with respect to some metrics such as RMSE or MAE,
we can consider ourselves
lucky. However, there are some asymptotic results that guarantee
the optimality of the results generated (e.g., consistency)
for large sample sizes, see, e.g., <span id="id10">[<a class="reference internal" href="999-bibliography.html#id73">DGL96</a>]</span>.</p>
</dd>
<dt class="label" id="footscore"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p>Note that for models that are not generated via least squares,
R-squared can also be negative, if they are extremely bad.
Also note that this measure is dataset-dependent,
therefore it should not be used for comparing
between models explaining different dependent variables.</p>
</dd>
<dt class="label" id="footsingularvalue"><span class="brackets"><a class="fn-backref" href="#id7">6</a></span></dt>
<dd><p>Being themselves the square roots of eigenvalues
of <span class="math notranslate nohighlight">\(\mathbf{X}^T \mathbf{X}\)</span>. Seriously, we really need linear algebra
when we even remotely think about practising data science.</p>
</dd>
</dl>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="410-data-frame.html" class="btn btn-neutral float-right" title="10. Introducing Data Frames" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="320-transform-matrix.html" class="btn btn-neutral float-left" title="8. Processing Multidimensional Data" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        Copyright &#169; 2022 by <a href="https://www.gagolewski.com">Marek Gagolewski</a>. Some rights reserved. Licensed under <a href='https://creativecommons.org/licenses/by-nc-nd/4.0/'>CC BY-NC-ND 4.0</a>.
      <span class="lastupdated">
        Last updated on 2022-06-22T15:44:17+1000.
      </span>
    Built with <a href="https://sphinx-doc.org/">Sphinx</a>
    and a customised <a href="https://github.com/rtfd/sphinx_rtd_theme">rtd</a> theme.
    </p>
  </div> 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>