<!doctype html>
<html class="no-js" lang="en">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />
<meta content="Minimalist Data Wrangling with Python" name="citation_title" />
<meta content="Marek Gagolewski" name="citation_author" />
<meta content="2023" name="citation_date" />
<meta content="2023" name="citation_publication_date" />
<meta content="https://datawranglingpy.gagolewski.com/datawranglingpy.pdf" name="citation_pdf_url" />
<meta content="https://datawranglingpy.gagolewski.com/" name="citation_public_url" />
<meta content="10.5281/zenodo.6451068" name="citation_doi" />
<meta content="Minimalist Data Wrangling with Python is envisaged as a student's first introduction to data science, providing a high-level overview as well as discussing key concepts in detail. We explore methods for cleaning data gathered from different sources, transforming, selecting, and extracting features, performing exploratory data analysis and dimensionality reduction, identifying naturally occurring data clusters, modelling patterns in data, comparing data between groups, and reporting the results. This textbook is a non-profit project. Its online and PDF versions are freely available at https://datawranglingpy.gagolewski.com/." name="citation_abstract" />
<meta content="summary" name="twitter:card" />
<meta content="Minimalist Data Wrangling with Python" name="twitter:title" />
<meta content="Minimalist Data Wrangling with Python" name="og:title" />
<meta content="Minimalist Data Wrangling with Python is envisaged as a student's first introduction to data science, providing a high-level overview as well as discussing key concepts in detail. We explore methods for cleaning data gathered from different sources, transforming, selecting, and extracting features, performing exploratory data analysis and dimensionality reduction, identifying naturally occurring data clusters, modelling patterns in data, comparing data between groups, and reporting the results. This textbook is a non-profit project. Its online and PDF versions are freely available at https://datawranglingpy.gagolewski.com/." name="twitter:description" />
<meta content="Minimalist Data Wrangling with Python is envisaged as a student's first introduction to data science, providing a high-level overview as well as discussing key concepts in detail. We explore methods for cleaning data gathered from different sources, transforming, selecting, and extracting features, performing exploratory data analysis and dimensionality reduction, identifying naturally occurring data clusters, modelling patterns in data, comparing data between groups, and reporting the results. This textbook is a non-profit project. Its online and PDF versions are freely available at https://datawranglingpy.gagolewski.com/." name="og:description" />
<meta content="gagolews/datawranglingpy" name="og:site_name" />
<meta content="https://datawranglingpy.gagolewski.com/" name="og:url" />
<meta content="https://datawranglingpy.gagolewski.com/_images/cover.png" name="twitter:image" />
<meta content="https://datawranglingpy.gagolewski.com/_images/cover.png" name="og:image" />
<meta content="https://datawranglingpy.gagolewski.com/" name="DC.identifier" />
<meta content="Marek Gagolewski" name="DC.publisher" />
<meta content="INDEX,FOLLOW" name="robots" />
<meta content="book" name="og:type" />
<meta content="9780645571912" name="og:book:isbn" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="10. Introducing data frames" href="410-data-frame.html" /><link rel="prev" title="8. Processing multidimensional data" href="320-transform-matrix.html" />
        <link rel="canonical" href="https://datawranglingpy.gagolewski.com/chapter/330-relationship.html" />

    <link rel="shortcut icon" href="../_static/favicon.png"/><!-- Generated with Sphinx 6.2.1 and Furo 2023.03.27 -->
        <title>9. Exploring relationships between variables - Minimalist Data Wrangling with Python</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?digest=fad236701ea90a88636c2a8c73b44ae642ed2a53" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?digest=30d1aed668e5c3a91c3e3bf6a60b675221979f0e" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  --admonition-font-size: 95%;
  --admonition-title-font-size: 95%;
  --color-brand-primary: red;
  --color-brand-content: #CC3333;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --admonition-font-size: 95%;
  --admonition-title-font-size: 95%;
  --color-brand-primary: #ff2b53;
  --color-brand-content: #dd3333;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --admonition-font-size: 95%;
  --admonition-title-font-size: 95%;
  --color-brand-primary: #ff2b53;
  --color-brand-content: #dd3333;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">Minimalist Data Wrangling with Python</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto colour theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky">

<span class="sidebar-brand-text">
<a class="sidebar-brand" href="../index.html">Minimalist Data Wrangling with Python</a>
</span>
<div class="sidebar-brand">
An open-access textbook<br />
by <a href='https://www.gagolewski.com' style="display: contents">Marek Gagolewski</a><br />
v1.0.3.9004
</div>
<form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">About</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.gagolewski.com/">Author</a></li>
<li class="toctree-l1"><a class="reference external" href="https://datawranglingpy.gagolewski.com/datawranglingpy.pdf">This book in PDF</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.amazon.com/dp/0645571911">Printed version</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/gagolews/datawranglingpy">Report bugs or typos (GitHub)</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/gagolews/teaching-data">Datasets</a></li>
<li class="toctree-l1"><a class="reference external" href="https://deepr.gagolewski.com/">Deep R programming</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Start here</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="000-preface.html">Preface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introducing Python</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="110-setup.html">1. Getting started with Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="120-scalar.html">2. Scalar types and control structures in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="130-sequential.html">3. Sequential and other types in Python</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Unidimensional data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="210-vector.html">4. Unidimensional numeric data and their empirical distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="220-transform-vector.html">5. Processing unidimensional data</a></li>
<li class="toctree-l1"><a class="reference internal" href="230-distribution.html">6. Continuous probability distributions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multidimensional data</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="310-matrix.html">7. Multidimensional numeric data at a glance</a></li>
<li class="toctree-l1"><a class="reference internal" href="320-transform-matrix.html">8. Processing multidimensional data</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">9. Exploring relationships between variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Heterogeneous data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="410-data-frame.html">10. Introducing data frames</a></li>
<li class="toctree-l1"><a class="reference internal" href="420-categorical.html">11. Handling categorical data</a></li>
<li class="toctree-l1"><a class="reference internal" href="430-group-by.html">12. Processing data in groups</a></li>
<li class="toctree-l1"><a class="reference internal" href="440-sql.html">13. Accessing databases</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other data types</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="510-text.html">14. Text data</a></li>
<li class="toctree-l1"><a class="reference internal" href="520-missingness.html">15. Missing, censored, and questionable data</a></li>
<li class="toctree-l1"><a class="reference internal" href="530-time-series.html">16. Time series</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="998-changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="999-bibliography.html">References</a></li>
</ul>

</div></div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="edit-this-page">
  <a class="muted-link" href="https://github.com/gagolews/datawranglingpy/issues/" title="Edit this page">
    <svg aria-hidden="true" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <path d="M4 20h4l10.5 -10.5a1.5 1.5 0 0 0 -4 -4l-10.5 10.5v4" />
      <line x1="13.5" y1="6.5" x2="17.5" y2="10.5" />
    </svg>
    <span class="visually-hidden">Edit this page</span>
  </a>
</div><div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto colour theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section class="tex2jax_ignore mathjax_ignore" id="exploring-relationships-between-variables">
<span id="chap-relationship"></span><h1><span class="section-number">9. </span>Exploring relationships between variables<a class="headerlink" href="#exploring-relationships-between-variables" title="Permalink to this heading">#</a></h1>
<blockquote>
<div><p><em>The open-access textbook</em> Minimalist Data Wrangling with Python <em>by
<a class="reference external" href="https://www.gagolewski.com/">Marek Gagolewski</a>
is, and will remain, freely available for everyone’s enjoyment
(also in <a class="reference external" href="https://datawranglingpy.gagolewski.com/datawranglingpy.pdf">PDF</a>;
a printed version can be ordered from
<a class="reference external" href="https://www.amazon.com/dp/0645571911">Amazon</a>:
<a class="reference external" href="https://amazon.com.au/dp/0645571911">AU</a>
<a class="reference external" href="https://amazon.ca/dp/0645571911">CA</a>
<a class="reference external" href="https://amazon.de/dp/0645571911">DE</a>
<a class="reference external" href="https://amazon.es/dp/0645571911">ES</a>
<a class="reference external" href="https://amazon.fr/dp/0645571911">FR</a>
<a class="reference external" href="https://amazon.it/dp/0645571911">IT</a>
<a class="reference external" href="https://amazon.co.jp/dp/0645571911">JP</a>
<a class="reference external" href="https://amazon.nl/dp/0645571911">NL</a>
<a class="reference external" href="https://amazon.pl/dp/0645571911">PL</a>
<a class="reference external" href="https://amazon.se/dp/0645571911">SE</a>
<a class="reference external" href="https://amazon.co.uk/dp/0645571911">UK</a>
<a class="reference external" href="https://amazon.com/dp/0645571911">US</a>).
It is a non-profit project. Although available online, it is a whole course,
and should be read from the beginning to the end.
Refer to the <a class="reference internal" href="000-preface.html#chap-preface"><span class="std std-ref">Preface</span></a> for general introductory remarks. Any
<a class="reference external" href="https://github.com/gagolews/datawranglingpy/issues">bug/typo reports/fixes</a>
are appreciated. Make sure to check out the author’s other book,
<a class="reference external" href="https://deepr.gagolewski.com/"><em>Deep R Programming</em></a>
<span id="id1">[<a class="reference internal" href="999-bibliography.html#id2" title="Gagolewski, M. (2023).  Deep R Programming. Zenodo. URL: https://deepr.gagolewski.com/, DOI: 10.5281/zenodo.7490464.">34</a>]</span>.</em></p>
</div></blockquote>
<p>Let us go back to National Health and Nutrition Examination Survey
(NHANES study) excerpt that we were playing with in
<a class="reference internal" href="310-matrix.html#sec-visual-multidim"><span class="std std-numref">Section 7.4</span></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">body</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching-data/master/marek/nhanes_adult_female_bmx_2020.csv&quot;</span><span class="p">,</span>
    <span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span>  <span class="c1"># skip first row (column names)</span>
<span class="n">body</span><span class="p">[:</span><span class="mi">6</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># preview: 6 first rows, all columns</span>
<span class="c1">## array([[ 97.1, 160.2,  34.7,  40.8,  35.8, 126.1, 117.9],</span>
<span class="c1">##        [ 91.1, 152.7,  33.5,  33. ,  38.5, 125.5, 103.1],</span>
<span class="c1">##        [ 73. , 161.2,  37.4,  38. ,  31.8, 106.2,  92. ],</span>
<span class="c1">##        [ 61.7, 157.4,  38. ,  34.7,  29. , 101. ,  90.5],</span>
<span class="c1">##        [ 55.4, 154.6,  34.6,  34. ,  28.3,  92.5,  73.2],</span>
<span class="c1">##        [ 62. , 144.7,  32.5,  34.2,  29.8, 106.7,  84.8]])</span>
<span class="n">body</span><span class="o">.</span><span class="n">shape</span>
<span class="c1">## (4221, 7)</span>
</pre></div>
</div>
<p>We thus have <span class="math notranslate nohighlight">\(n=4221\)</span> participants and seven different
features describing them, in this order:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">body_columns</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="s2">&quot;weight&quot;</span><span class="p">,</span>
    <span class="s2">&quot;height&quot;</span><span class="p">,</span>
    <span class="s2">&quot;arm len.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;leg len.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;arm circ.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;hip circ.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;waist circ.&quot;</span>
<span class="p">])</span>
</pre></div>
</div>
<p>We expect the data in different columns to be <em>related</em> to each other
(e.g., a taller person <em>usually tends to</em> weight more).
This is why we will now be interested in quantifying the degree of
association between the variables, modelling the possible functional relationships, and finding new interesting combinations of columns.</p>
<section id="measuring-correlation">
<h2><span class="section-number">9.1. </span>Measuring correlation<a class="headerlink" href="#measuring-correlation" title="Permalink to this heading">#</a></h2>
<p>Scatter plots let us identify some simple patterns or structure in data.
<a class="reference internal" href="310-matrix.html#fig-body-pairplot"><span class="std std-numref">Figure 7.4</span></a> indicates that higher hip
circumferences <em>tend to</em> occur more often together with higher arm
circumferences and that the latter does not really tell us much
about height.</p>
<p>Let us explore some basic means for measuring
(expressing as a single number) the degree of association
between a set of pairs of points.</p>
<section id="pearson-linear-correlation-coefficient">
<span id="sec-pearson"></span><h3><span class="section-number">9.1.1. </span>Pearson linear correlation coefficient<a class="headerlink" href="#pearson-linear-correlation-coefficient" title="Permalink to this heading">#</a></h3>
<p>The Pearson <em>linear correlation</em> coefficient is given by:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
r(\boldsymbol{x}, \boldsymbol{y}) = \frac{1}{n} \sum_{i=1}^n
   \frac{x_i - \bar{x}}{s_{x}}
\,
   \frac{y_i - \bar{y}}{s_{y}},
\]</div>
</div>
<p>with <span class="math notranslate nohighlight">\(s_x, s_y\)</span> denoting the standard deviations
and <span class="math notranslate nohighlight">\(\bar{x}, \bar{y}\)</span> being the means of
<span class="math notranslate nohighlight">\(\boldsymbol{x}=(x_1,\dots,x_n)\)</span> and
<span class="math notranslate nohighlight">\(\boldsymbol{y}=(y_1,\dots,y_n)\)</span>, respectively.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Look carefully: we are computing the mean of the pairwise
products of standardised versions of the two vectors.
It is a normalised measure of how they <em>vary together</em>
(co-variance).</p>
<p>(*) Furthermore, <a class="reference internal" href="#sec-dot-cosine"><span class="std std-numref">Section 9.3.1</span></a> mentions that
<span class="math notranslate nohighlight">\(r\)</span> is the cosine of the angle between
centred and normalised versions of the vectors.</p>
</div>
<p>Here is how we can compute it manually:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">body</span><span class="p">[:,</span> <span class="mi">4</span><span class="p">]</span>  <span class="c1"># arm circumference</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">body</span><span class="p">[:,</span> <span class="mi">5</span><span class="p">]</span>  <span class="c1"># hip circumference</span>
<span class="n">x_std</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># z-scores for x</span>
<span class="n">y_std</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>  <span class="c1"># z-scores for y</span>
<span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x_std</span><span class="o">*</span><span class="n">y_std</span><span class="p">)</span>
<span class="c1">## 0.8680627457873239</span>
</pre></div>
</div>
<p>And here is the built-in function that implements the same formula:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pearsonr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1">## 0.8680627457873241</span>
</pre></div>
</div>
<p>Note the <code class="docutils literal notranslate"><span class="pre">[0]</span></code> part: the function returns more than we actually need.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Basic properties of Pearson’s <span class="math notranslate nohighlight">\(r\)</span> include:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(r(\boldsymbol{x}, \boldsymbol{y}) = r(\boldsymbol{y}, \boldsymbol{x})\)</span>
(symmetric);</p></li>
<li><p><span class="math notranslate nohighlight">\(|r(\boldsymbol{x}, \boldsymbol{y})| \le 1\)</span>
(bounded from below by -1 and from above by 1);</p></li>
<li><p><span class="math notranslate nohighlight">\(r(\boldsymbol{x}, \boldsymbol{y})=1\)</span> if and only if
<span class="math notranslate nohighlight">\(\boldsymbol{y}=a\boldsymbol{x}+b\)</span> for some <span class="math notranslate nohighlight">\(a&gt;0\)</span> and any <span class="math notranslate nohighlight">\(b\)</span>,
(reaches the maximum when one variable is an increasing
linear function of the other one);</p></li>
<li><p><span class="math notranslate nohighlight">\(r(\boldsymbol{x}, -\boldsymbol{y})=-r(\boldsymbol{x}, \boldsymbol{y})\)</span>
(negative scaling (reflection) of one variable changes the sign of the
coefficient);</p></li>
<li><p><span class="math notranslate nohighlight">\(r(\boldsymbol{x}, a\boldsymbol{y}+b)=r(\boldsymbol{x}, \boldsymbol{y})\)</span>
for any <span class="math notranslate nohighlight">\(a&gt;0\)</span> and <span class="math notranslate nohighlight">\(b\)</span> (invariant to translation and scaling of inputs
that does not change the sign of elements).</p></li>
</ol>
</div>
<p>To get more insight, below we shall illustrate some interesting
<em>correlations</em> using the following helper function
that draws a scatter plot and prints out Pearson’s <span class="math notranslate nohighlight">\(r\)</span>
(and Spearman’s <span class="math notranslate nohighlight">\(\rho\)</span> discussed in <a class="reference internal" href="#sec-spearman"><span class="std std-numref">Section 9.1.4</span></a>;
let us ignore it by then):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">axes_eq</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pearsonr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">rho</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">spearmanr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;$r = </span><span class="si">{</span><span class="n">r</span><span class="si">:</span><span class="s2">.3</span><span class="si">}</span><span class="s2">$, $</span><span class="se">\\</span><span class="s2">rho = </span><span class="si">{</span><span class="n">rho</span><span class="si">:</span><span class="s2">.3</span><span class="si">}</span><span class="s2">$&quot;</span><span class="p">,</span>
        <span class="n">fontdict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">axes_eq</span><span class="p">:</span> <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
</pre></div>
</div>
<section id="perfect-linear-correlation">
<h4><span class="section-number">9.1.1.1. </span>Perfect linear correlation<a class="headerlink" href="#perfect-linear-correlation" title="Permalink to this heading">#</a></h4>
<p>The aforementioned properties imply that
<span class="math notranslate nohighlight">\(r(\boldsymbol{x}, \boldsymbol{y})=-1\)</span> if and only if
<span class="math notranslate nohighlight">\(\boldsymbol{y}=a\boldsymbol{x}+b\)</span> for some <span class="math notranslate nohighlight">\(a&lt;0\)</span> and any <span class="math notranslate nohighlight">\(b\)</span>
(reaches the minimum when variables are
decreasing linear functions of each other)
Furthermore, a variable is trivially
perfectly correlated with itself, <span class="math notranslate nohighlight">\(r(\boldsymbol{x}, \boldsymbol{x})=1\)</span>.</p>
<p>Consequently, we get perfect <em>linear correlation</em> (-1 or 1) when
one variable is a scaled and shifted version (linear function)
of the other variable; see <a class="reference internal" href="#fig-corr-ex-0"><span class="std std-numref">Figure 9.1</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="mi">3</span><span class="p">,</span>  <span class="n">axes_eq</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># negative slope</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span> <span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span>    <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">+</span><span class="mi">10</span><span class="p">,</span> <span class="n">axes_eq</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># positive slope</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id24">
<span id="fig-corr-ex-0"></span><img alt="../_images/corr-ex-0-1.png" src="../_images/corr-ex-0-1.png" />
<figcaption>
<p><span class="caption-number">Figure 9.1 </span><span class="caption-text">Perfect linear correlation (negative and positive).</span><a class="headerlink" href="#id24" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>A negative correlation means that when one variable
increases, the other one decreases (like: a car’s braking distance vs
velocity).</p>
</section>
<section id="strong-linear-correlation">
<h4><span class="section-number">9.1.1.2. </span>Strong linear correlation<a class="headerlink" href="#strong-linear-correlation" title="Permalink to this heading">#</a></h4>
<p>Next, if two variables are <em>more or less</em> linear functions
of themselves, the correlations will be close to -1 or 1,
with the degree of association diminishing as the linear relationship
becomes less and less evident; see <a class="reference internal" href="#fig-corr-ex-1"><span class="std std-numref">Figure 9.2</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>      <span class="c1"># random x (whatever)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">x</span>                    <span class="c1"># y is a linear function of x</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># random Gaussian noise (expected value 0)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>         <span class="c1"># original y</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span> <span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">+</span><span class="mf">0.05</span><span class="o">*</span><span class="n">e</span><span class="p">)</span>  <span class="c1"># some noise added to y</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">);</span> <span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">+</span><span class="mf">0.1</span><span class="o">*</span><span class="n">e</span><span class="p">)</span>   <span class="c1"># more noise</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">);</span> <span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">+</span><span class="mf">0.25</span><span class="o">*</span><span class="n">e</span><span class="p">)</span>  <span class="c1"># even more noise</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id25">
<span id="fig-corr-ex-1"></span><img alt="../_images/corr-ex-1-3.png" src="../_images/corr-ex-1-3.png" />
<figcaption>
<p><span class="caption-number">Figure 9.2 </span><span class="caption-text">Linear correlation coefficients for data with different amounts of noise.</span><a class="headerlink" href="#id25" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Notice again that the arm and hip circumferences enjoy quite high positive
degree of linear correlation. Their scatter plot
(<a class="reference internal" href="310-matrix.html#fig-body-pairplot"><span class="std std-numref">Figure 7.4</span></a>) looks somewhat similar to one of the cases
presented here.</p>
<div class="proof proof-type-exercise" id="id26">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.1</span>
        
    </div><div class="proof-content">
<p>Draw a series of similar plots but
for the case of negatively correlated point pairs,
e.g., <span class="math notranslate nohighlight">\(y=-2x+5\)</span>.</p>
</div></div><div class="admonition important">
<p class="admonition-title">Important</p>
<p>As a rule of thumb, linear correlation degree of
0.9 or greater (or -0.9 or smaller) is quite decent.
Between -0.8 and 0.8 we probably should not be talking about two variables
being linearly correlated at all.
Some textbooks are more lenient, but we have higher standards.
In particular, it is not uncommon in social sciences to
consider 0.6 a decent degree of correlation, but this is like
building on sand. If a dataset at hand does not provide us with
strong evidence, it is our ethical duty to refrain ourselves
from making unjustified statements.
It is better to remain silent than to talk gibberish and misled
the recipients of our exercises on data analysis.</p>
</div>
</section>
<section id="no-linear-correlation-does-not-imply-independence">
<h4><span class="section-number">9.1.1.3. </span>No linear correlation does not imply independence<a class="headerlink" href="#no-linear-correlation-does-not-imply-independence" title="Permalink to this heading">#</a></h4>
<p>For two independent variables, we expect the correlation coefficient
be approximately equal to 0.
Nevertheless, correlation close to 0 does not
necessarily mean that two variables are unrelated to each other.
Pearson’s <span class="math notranslate nohighlight">\(r\)</span> is a <em>linear</em> correlation coefficient, so we
are quantifying only<a class="footnote-reference brackets" href="#footpearsonks" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> these types of relationships.
See <a class="reference internal" href="#fig-corr-ex-2"><span class="std std-numref">Figure 9.3</span></a> for an illustration of this fact.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>  <span class="c1"># independent (not correlated)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>         <span class="c1"># quadratic dependence</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>        <span class="c1"># absolute value</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">10</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">x</span><span class="p">))</span>   <span class="c1"># sine</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id27">
<span id="fig-corr-ex-2"></span><img alt="../_images/corr-ex-2-5.png" src="../_images/corr-ex-2-5.png" />
<figcaption>
<p><span class="caption-number">Figure 9.3 </span><span class="caption-text">Are these variable pairs really uncorrelated?</span><a class="headerlink" href="#id27" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="false-linear-correlations">
<h4><span class="section-number">9.1.1.4. </span>False linear correlations<a class="headerlink" href="#false-linear-correlations" title="Permalink to this heading">#</a></h4>
<p>What is more, sometimes we can detect <em>false</em> correlations –
when data are functionally dependent, the relationship
is not linear, but it kind of looks like linear.
Refer to <a class="reference internal" href="#fig-corr-ex-3"><span class="std std-numref">Figure 9.4</span></a> for some examples.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">0.6</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># sine</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>          <span class="c1"># logarithm</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">);</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>         <span class="c1"># exponential of square</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="mi">2</span><span class="o">+</span><span class="mf">0.2</span><span class="p">))</span>          <span class="c1"># reciprocal</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id28">
<span id="fig-corr-ex-3"></span><img alt="../_images/corr-ex-3-7.png" src="../_images/corr-ex-3-7.png" />
<figcaption>
<p><span class="caption-number">Figure 9.4 </span><span class="caption-text">Example nonlinear relationships that look like linear, at least to Pearson’s <span class="math notranslate nohighlight">\(r\)</span>.</span><a class="headerlink" href="#id28" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>No single measure is perfect – we are trying to compress
<span class="math notranslate nohighlight">\(2n\)</span> data points into a single number — it is obvious that
there will be many different datasets, sometimes remarkably diverse,
that will yield the same correlation degree.</p>
</section>
<section id="correlation-is-not-causation">
<h4><span class="section-number">9.1.1.5. </span>Correlation is not causation<a class="headerlink" href="#correlation-is-not-causation" title="Permalink to this heading">#</a></h4>
<p>A high correlation degree (either positive or negative) does not
mean that there is any <em>causal</em> relationship between the two variables.
We cannot say that having large arm circumference affects hip size
or vice versa. There might be some <em>latent</em> variable
that influences these two (e.g., maybe also related to weight?).</p>
<div class="proof proof-type-exercise" id="id29">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.2</span>
        
    </div><div class="proof-content">
<p>Quite often, medical advice is formulated
based on correlations and similar association-measuring tools.
We are expected to know how to interpret them, as it is never
a true cause-effect relationship; rather,
it is all about detecting common patterns in larger populations.
For instance, in “obesity increases the likelihood of lower back
pain and diabetes” we do not say that one necessarily
<em>implies</em> another or that if you are not overweight, there is no
risk of getting the two said conditions.
It might also work the other way around, as lower back pain
may lead to less exercise and then weight gain. Reality
is complex.
Find similar patterns in sets of health conditions.</p>
</div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Correlation analysis can aid in constructing regression models,
where we would like to
identify a transformation that expresses a variable
as a function of one or more other features.
For instance, when we say that <span class="math notranslate nohighlight">\(y\)</span> can be modelled approximately by <span class="math notranslate nohighlight">\(ax+b\)</span>,
regression analysis can identify the best matching <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>
coefficients; see <a class="reference internal" href="#sec-least-squares"><span class="std std-numref">Section 9.2.3</span></a> for more details.</p>
</div>
</section>
</section>
<section id="correlation-heat-map">
<h3><span class="section-number">9.1.2. </span>Correlation heat map<a class="headerlink" href="#correlation-heat-map" title="Permalink to this heading">#</a></h3>
<p>Calling <strong class="command">numpy.corrcoef</strong><code class="code docutils literal notranslate"><span class="pre">(body,</span> <span class="pre">rowvar=False)</span></code>
determines the linear correlation
coefficients between all pairs of variables in our dataset.
We can depict them nicely on a heat map based
on a fancified call to <strong class="command">matplotlib.pyplot.imshow</strong>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="k">def</span> <span class="nf">corrheatmap</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Draws a correlation heat map, given:</span>
<span class="sd">    * R - matrix of correlation coefficients between all pairs of variables,</span>
<span class="sd">    * labels - list of column names</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">R</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">R</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="ow">and</span> <span class="n">R</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># plot the heat map using a custom colour palette</span>
    <span class="c1"># (correlations are in [-1, 1])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s2">&quot;RdBu&quot;</span><span class="p">),</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># add text labels</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">R</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">j</span><span class="p">]</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">,</span>
                <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">R</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span><span class="o">&lt;</span><span class="mf">0.5</span> <span class="k">else</span> <span class="s2">&quot;white&quot;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">k</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s2">&quot;both&quot;</span><span class="p">,</span>
        <span class="n">labelbottom</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">labeltop</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">k</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">which</span><span class="o">=</span><span class="s2">&quot;both&quot;</span><span class="p">,</span>
        <span class="n">labelleft</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">labelright</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">left</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">right</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="#fig-cor-heat"><span class="std std-numref">Figure 9.5</span></a> for the plot.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">body</span><span class="p">,</span> <span class="n">rowvar</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">order</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>  <span class="c1"># chiefly for aesthetics</span>
<span class="n">corrheatmap</span><span class="p">(</span><span class="n">R</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ix_</span><span class="p">(</span><span class="n">order</span><span class="p">,</span> <span class="n">order</span><span class="p">)],</span> <span class="n">body_columns</span><span class="p">[</span><span class="n">order</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id30">
<span id="fig-cor-heat"></span><img alt="../_images/cor-heat-9.png" src="../_images/cor-heat-9.png" />
<figcaption>
<p><span class="caption-number">Figure 9.5 </span><span class="caption-text">A correlation heat map.</span><a class="headerlink" href="#id30" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Notice that we ordered<a class="footnote-reference brackets" href="#foothclust" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> the columns to reveal some naturally
occurring variable <em>clusters</em>: for instance,
arm, hip, waist circumference and weight are all quite strongly
correlated.</p>
<p>Of course, we have 1.0s on the main diagonal because a variable
is trivially correlated with itself.
Interestingly, this heat map is symmetric
which is due to the property
<span class="math notranslate nohighlight">\(r(\boldsymbol{x}, \boldsymbol{y}) = r(\boldsymbol{y}, \boldsymbol{x})\)</span>.</p>
<div class="proof proof-type-example" id="id31">

    <div class="proof-title">
        <span class="proof-type">Example 9.3</span>
        
    </div><div class="proof-content">
<p>(*)
To fetch the row and column index of the most correlated pair of
variables (either positively or negatively),
we should first take the upper (or lower) triangle of the correlation matrix
(see <strong class="command">numpy.triu</strong> or <strong class="command">numpy.tril</strong>) to ignore the irrelevant
and repeating items:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Ru</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">R</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">Ru</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1">## array([[0.  , 0.35, 0.55, 0.19, 0.91, 0.95, 0.9 ],</span>
<span class="c1">##        [0.  , 0.  , 0.67, 0.66, 0.15, 0.2 , 0.13],</span>
<span class="c1">##        [0.  , 0.  , 0.  , 0.48, 0.45, 0.46, 0.43],</span>
<span class="c1">##        [0.  , 0.  , 0.  , 0.  , 0.08, 0.1 , 0.03],</span>
<span class="c1">##        [0.  , 0.  , 0.  , 0.  , 0.  , 0.87, 0.85],</span>
<span class="c1">##        [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.9 ],</span>
<span class="c1">##        [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])</span>
</pre></div>
</div>
<p>and then find the location of the maximum:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unravel_index</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Ru</span><span class="p">),</span> <span class="n">Ru</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">pos</span>  <span class="c1"># (row, column)</span>
<span class="c1">## (0, 5)</span>
<span class="n">body_columns</span><span class="p">[</span> <span class="nb">list</span><span class="p">(</span><span class="n">pos</span><span class="p">)</span> <span class="p">]</span>  <span class="c1"># indexing by a tuple has a different meaning</span>
<span class="c1">## array([&#39;weight&#39;, &#39;hip circ.&#39;], dtype=&#39;&lt;U11&#39;)</span>
</pre></div>
</div>
<p>Weight and hip circumference is the most strongly correlated pair.</p>
<p>Note that <strong class="command">numpy.argmax</strong> returns an index in the
flattened (unidimensional) array. We had to use
<strong class="command">numpy.unravel_index</strong> to convert it to a two-dimensional
one.</p>
</div></div><div class="proof proof-type-example" id="id32">

    <div class="proof-title">
        <span class="proof-type">Example 9.4</span>
        
    </div><div class="proof-content">
<p>(*)
Use <strong class="command">seaborn.heatmap</strong> to draw the correlation heat map.</p>
</div></div></section>
<section id="linear-correlation-coefficients-on-transformed-data">
<h3><span class="section-number">9.1.3. </span>Linear correlation coefficients on transformed data<a class="headerlink" href="#linear-correlation-coefficients-on-transformed-data" title="Permalink to this heading">#</a></h3>
<p>Pearson’s coefficient can also be applied
on nonlinearly transformed versions of variables,
e.g., logarithms (remember incomes?), squares, square roots, etc.</p>
<p>Let us consider an excerpt from the 2020 CIA
<a class="reference external" href="https://www.cia.gov/the-world-factbook">World Factbook</a>,
where we have data on gross domestic product per capita (based
on purchasing power parity) and life expectancy at birth in many countries.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">world</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching-data/master/marek/world_factbook_2020_subset1.csv&quot;</span><span class="p">,</span>
    <span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">)[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span>  <span class="c1"># skip first row (column names)</span>
<span class="n">world</span><span class="p">[:</span><span class="mi">6</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># preview</span>
<span class="c1">## array([[ 2000. ,    52.8],</span>
<span class="c1">##        [12500. ,    79. ],</span>
<span class="c1">##        [15200. ,    77.5],</span>
<span class="c1">##        [11200. ,    74.8],</span>
<span class="c1">##        [49900. ,    83. ],</span>
<span class="c1">##        [ 6800. ,    61.3]])</span>
</pre></div>
</div>
<p><a class="reference internal" href="#fig-worldfactbook-gdp-life"><span class="std std-numref">Figure 9.6</span></a> depicts these data on
a scatter plot.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;per capita GDP PPP&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;life expectancy (years)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plot_corr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;log(per capita GDP PPP)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id33">
<span id="fig-worldfactbook-gdp-life"></span><img alt="../_images/WorldFactbook-gdp-life-11.png" src="../_images/WorldFactbook-gdp-life-11.png" />
<figcaption>
<p><span class="caption-number">Figure 9.6 </span><span class="caption-text">Scatter plots for life expectancy vs gross domestic product (purchasing power parity) on linear (left) and log-scale (right).</span><a class="headerlink" href="#id33" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>If we compute Pearson’s <span class="math notranslate nohighlight">\(r\)</span> between these two,
we will note a quite weak linear correlation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pearsonr</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1">## 0.656471945486374</span>
</pre></div>
</div>
<p>Anyhow, already the logarithm of GDP is quite strongly linearly
correlated with life expectancy:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pearsonr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1">## 0.8066505089380016</span>
</pre></div>
</div>
<p>which means that modelling our data via
<span class="math notranslate nohighlight">\(\boldsymbol{y}=a \log\boldsymbol{x}+b\)</span> could be an idea worth considering.</p>
</section>
<section id="spearman-rank-correlation-coefficient">
<span id="sec-spearman"></span><h3><span class="section-number">9.1.4. </span>Spearman rank correlation coefficient<a class="headerlink" href="#spearman-rank-correlation-coefficient" title="Permalink to this heading">#</a></h3>
<p>Sometimes we might be interested in measuring
the degree of any kind of <em>monotonic</em> correlation – to what extent
one variable is an increasing or decreasing function
of another one (linear, logarithmic, quadratic over the positive
domain, etc.).</p>
<p>Spearman’s rank correlation coefficient  is frequently used
in such a scenario:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\rho(\boldsymbol{x}, \boldsymbol{y}) = r(R(\boldsymbol{x}), R(\boldsymbol{y})),
\]</div>
</div>
<p>which is<a class="footnote-reference brackets" href="#footspearman" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a> the Pearson coefficient
computed over vectors of the corresponding
ranks of all the elements in <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>
(denoted with <span class="math notranslate nohighlight">\(R(\boldsymbol{x})\)</span> and <span class="math notranslate nohighlight">\(R(\boldsymbol{y})\)</span>, respectively).</p>
<p>Hence, the two following calls are equivalent:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">spearmanr</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1">## 0.8275220380818622</span>
<span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">pearsonr</span><span class="p">(</span>
    <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">rankdata</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]),</span>
    <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">rankdata</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1">## 0.8275220380818621</span>
</pre></div>
</div>
<p>Let us point out that this measure is invariant with respect to
monotone transformations of the input variables
(up to the sign):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">spearmanr</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]),</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]))[</span><span class="mi">0</span><span class="p">]</span>
<span class="c1">## -0.8275220380818622</span>
</pre></div>
</div>
<p>This is because such transformations do not change the observations’
ranks (or only reverse them).</p>
<div class="proof proof-type-exercise" id="id34">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.5</span>
        
    </div><div class="proof-content">
<p>We included the <span class="math notranslate nohighlight">\(\rho\)</span>s in all the outputs generated
by our <strong class="command">plot_corr</strong> function. Review all the above figures.</p>
</div></div><div class="proof proof-type-exercise" id="id35">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.6</span>
        
    </div><div class="proof-content">
<p>Apply <strong class="command">numpy.corrcoef</strong>
and <strong class="command">scipy.stats.rankdata</strong> (with the appropriate <code class="docutils literal notranslate"><span class="pre">axis</span></code> argument)
to compute the Spearman correlation matrix for all the variable
pairs in <code class="docutils literal notranslate"><span class="pre">body</span></code>. Draw it on a heat map.</p>
</div></div><div class="proof proof-type-exercise" id="id36">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.7</span>
        
    </div><div class="proof-content">
<p>(*) Draw the scatter plots of the ranks of each column
in the <code class="docutils literal notranslate"><span class="pre">world</span></code> and <code class="docutils literal notranslate"><span class="pre">body</span></code> datasets.</p>
</div></div></section>
</section>
<section id="regression-tasks">
<span id="sec-regression"></span><h2><span class="section-number">9.2. </span>Regression tasks<a class="headerlink" href="#regression-tasks" title="Permalink to this heading">#</a></h2>
<p>Let us assume that we are given a <em>training/reference</em> set of <span class="math notranslate nohighlight">\(n\)</span> points
in an <span class="math notranslate nohighlight">\(m\)</span>-dimensional space represented as a matrix
<span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{n\times m}\)</span> and a set of <span class="math notranslate nohighlight">\(n\)</span> corresponding
numeric outcomes <span class="math notranslate nohighlight">\(\boldsymbol{y}\in\mathbb{R}^n\)</span>.
<em>Regression</em> aims to find a function
between the <span class="math notranslate nohighlight">\(m\)</span> <em>independent/explanatory/predictor</em> variables
and a chosen <em>dependent/response/predicted</em> variable
that can be applied on any test point <span class="math notranslate nohighlight">\(\boldsymbol{x}'\in\mathbb{R}^m\)</span>:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\hat{y}' = f(x_1', x_2', \dots, x_m'),
\]</div>
</div>
<p>and which <em>approximates</em> the reference outcomes in a <em>usable</em> way.</p>
<section id="k-nearest-neighbour-regression">
<span id="sec-knn-regression"></span><h3><span class="section-number">9.2.1. </span><em>K</em>-nearest neighbour regression<a class="headerlink" href="#k-nearest-neighbour-regression" title="Permalink to this heading">#</a></h3>
<p>A quite straightforward approach to regression
relies on aggregating the reference outputs that are associated with
a few nearest neighbours of the point <span class="math notranslate nohighlight">\(\boldsymbol{x}'\)</span> tested;
compare <a class="reference internal" href="320-transform-matrix.html#sec-knn-vs-radius"><span class="std std-numref">Section 8.4.4</span></a>.</p>
<p>In <span class="math notranslate nohighlight">\(k\)</span>-nearest neighbour regression,
for a fixed <span class="math notranslate nohighlight">\(k\ge 1\)</span> and any given <span class="math notranslate nohighlight">\(\boldsymbol{x}'\in\mathbb{R}^m\)</span>,
<span class="math notranslate nohighlight">\(\hat{y}=f(\boldsymbol{x}')\)</span> is computed as follows.</p>
<ol class="arabic">
<li><p>Find the indices <span class="math notranslate nohighlight">\(N_k(\boldsymbol{x}')=\{i_1,\dots,i_k\}\)</span>
of the <span class="math notranslate nohighlight">\(k\)</span> points from
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span> closest to <span class="math notranslate nohighlight">\(\boldsymbol{x}'\)</span>, i.e., ones that fulfil
for all <span class="math notranslate nohighlight">\(j\not\in\{i_1,\dots,i_k\}\)</span>:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[ \|\mathbf{x}_{i_1,\cdot}-\boldsymbol{x}'\|
    \le\dots\le
    \| \mathbf{x}_{i_k,\cdot} -\boldsymbol{x}' \|
    \le
    \| \mathbf{x}_{j,\cdot} -\boldsymbol{x}' \|.
    \]</div>
</div>
</li>
<li><p>Return the arithmetic mean of <span class="math notranslate nohighlight">\((y_{i_1},\dots,y_{i_k})\)</span>
as the result.</p></li>
</ol>
<p>Here is a straightforward implementation
that generates the predictions for each point in <code class="docutils literal notranslate"><span class="pre">X_test</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">knn_regress</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">spatial</span><span class="o">.</span><span class="n">KDTree</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">k</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># indices of NNs</span>
    <span class="n">y_nn_pred</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>  <span class="c1"># corresponding reference outputs</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_nn_pred</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>For example, let us try expressing weight (the first column) as a function
of hip circumference (the sixth column) in the <code class="docutils literal notranslate"><span class="pre">body</span></code> dataset:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\text{weight}=f_1(\text{hip circumference})   \qquad  (+ \text{some error}).
\]</div>
</div>
<p>We can also model the life expectancy at birth in different countries
(<code class="docutils literal notranslate"><span class="pre">world</span></code> dataset) as a function of their GDP per capita (PPP):</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\text{life expectancy}=f_2(\text{GDP per capita})   \qquad  (+ \text{some error}).
\]</div>
</div>
<p>Both are instances of the <em>simple</em> regression problem,
i.e., where there is only one independent variable (<span class="math notranslate nohighlight">\(m=1\)</span>).
We can easily create an appealing visualisation thereof
by means of the following function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">knn_regress_plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">num_test_points</span><span class="o">=</span><span class="mi">1001</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    x - 1D vector - reference inputs</span>
<span class="sd">    y - 1D vector - corresponding outputs</span>
<span class="sd">    K - numbers of near neighbours to test</span>
<span class="sd">    num_test_points - number of points to test at</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">num_test_points</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">K</span><span class="p">:</span>
        <span class="n">_y</span> <span class="o">=</span> <span class="n">knn_regress</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>  <span class="c1"># see above</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;$k=</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">$&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
<p><a class="reference internal" href="#fig-knn-reg"><span class="std std-numref">Figure 9.7</span></a> depicts the fitted functions
for a few different <span class="math notranslate nohighlight">\(k\)</span>s.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">knn_regress_plot</span><span class="p">(</span><span class="n">body</span><span class="p">[:,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">body</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;hip circumference&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;weight&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">knn_regress_plot</span><span class="p">(</span><span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;per capita GDP PPP&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;life expectancy (years)&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id37">
<span id="fig-knn-reg"></span><img alt="../_images/knn-reg-13.png" src="../_images/knn-reg-13.png" />
<figcaption>
<p><span class="caption-number">Figure 9.7 </span><span class="caption-text"><span class="math notranslate nohighlight">\(k\)</span>-nearest neighbour regression curves for example datasets. The greater the <span class="math notranslate nohighlight">\(k\)</span>, the more coarse-grained the approximation.</span><a class="headerlink" href="#id37" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>We obtained a <em>smoothened</em> version of the original dataset.
The fact that we do not reproduce the reference data points in an exact
manner is reflected by the (figurative) error term in the above equations.
Its role is to emphasise the existence of some natural data variability;
after all, one’s weight is not purely determined by their hip size
and life is not all about money.</p>
<p>For small <span class="math notranslate nohighlight">\(k\)</span> we adapt to the data points better.
This can be worthwhile unless data are very noisy.
The greater the <span class="math notranslate nohighlight">\(k\)</span>, the smoother the approximation
at the cost of losing fine detail and restricted usability
at the domain boundaries (here: in the left and right part of the plots).</p>
<p>Usually, the number of neighbours is chosen by trial and error
(just like the number of bins in a histogram; compare
<a class="reference internal" href="210-vector.html#sec-how-many-bins"><span class="std std-numref">Section 4.3.3</span></a>).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>(**) Some methods use weighted arithmetic means for aggregating the
<span class="math notranslate nohighlight">\(k\)</span> reference outputs, with weights inversely proportional
to the distances to the neighbours (closer inputs are
considered more important).</p>
<p>Also, instead of few nearest neighbours, we can
easily compose some form of fixed-radius search regression,
by simply replacing <span class="math notranslate nohighlight">\(N_k(\boldsymbol{x}')\)</span>
with <span class="math notranslate nohighlight">\(B_r(\boldsymbol{x}')\)</span>; compare <a class="reference internal" href="320-transform-matrix.html#sec-knn-vs-radius"><span class="std std-numref">Section 8.4.4</span></a>.
Yet, note that this way we might
make the function undefined in sparsely populated regions of the domain.</p>
</div>
</section>
<section id="from-data-to-linear-models">
<span id="sec-linearmodel"></span><h3><span class="section-number">9.2.2. </span>From data to (linear) models<a class="headerlink" href="#from-data-to-linear-models" title="Permalink to this heading">#</a></h3>
<p>Unfortunately, to generate predictions for new data points,
<span class="math notranslate nohighlight">\(k\)</span>-nearest neighbours regression requires
that the training sample is available at all times.
It does not <em>synthesise</em> or <em>simplify</em> the inputs;
instead, it works as a kind of a black box.
If we were to provide a mathematical equation for the generated
prediction, it would be disgustingly long and obscure.</p>
<p>In such cases, to emphasise that <span class="math notranslate nohighlight">\(f\)</span> is dependent on the training sample,
we sometimes use the more explicit notation
<span class="math notranslate nohighlight">\(f(\boldsymbol{x}' | \mathbf{X}, \boldsymbol{y})\)</span>
or <span class="math notranslate nohighlight">\(f_{\mathbf{X}, \boldsymbol{y}}(\boldsymbol{x}')\)</span>.</p>
<p>In many contexts we might prefer creating a data <em>model</em> instead,
in the form of an easily interpretable mathematical function.
A simple yet still quite flexible choice
tackles regression problems via affine maps of the form:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
y = f(x_1, x_2, \dots, x_m) = c_1 x_1 + c_2 x_2 + \dots + c_m x_m + c_{m+1},
\]</div>
</div>
<p>or, in matrix multiplication terms:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
y = \mathbf{c} \mathbf{x}^T + c_{m+1},
\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{c}=[c_1\ c_2\ \cdots\ c_m]\)</span>
and <span class="math notranslate nohighlight">\(\mathbf{x}=[x_1\ x_2\ \cdots\ x_m]\)</span>.</p>
<p>For <span class="math notranslate nohighlight">\(m=1\)</span>, the above simply defines a straight line,
which we traditionally denote with:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
y = ax+b,
\]</div>
</div>
<p>i.e., where we mapped <span class="math notranslate nohighlight">\(x_1 \mapsto x\)</span>, <span class="math notranslate nohighlight">\(c_1 \mapsto a\)</span> (slope),
and <span class="math notranslate nohighlight">\(c_2 \mapsto b\)</span> (intercept).</p>
<p>For <span class="math notranslate nohighlight">\(m&gt;1\)</span>, we obtain different hyperplanes (high-dimensional
generalisations of the notion of a plane).</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>A separate intercept term “<span class="math notranslate nohighlight">\(+c_{m+1}\)</span>” in the defining equation
can be quite inconvenient to handle.
We will thus restrict ourselves to linear maps like:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[y = \mathbf{c} \mathbf{x}^T,\]</div>
</div>
<p>but where we can possibly have an explicit constant-1 component somewhere
<em>inside</em> <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. For instance:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\mathbf{x} = [x_1\ x_2\ \cdots\ x_m\ 1].\]</div>
</div>
<p>Together with
<span class="math notranslate nohighlight">\(\mathbf{c} = [c_1\ c_2\ \cdots\ c_m\ c_{m+1}]\)</span>,
as trivially <span class="math notranslate nohighlight">\(c_{m+1}\cdot 1=c_{m+1}\)</span>,
this new setting is equivalent to the original one.</p>
<p>Without loss of generality, from now on we assume that <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>
is <span class="math notranslate nohighlight">\(m\)</span>-dimensional, regardless of its having a constant-1 inside or not.</p>
</div>
</section>
<section id="least-squares-method">
<span id="sec-least-squares"></span><h3><span class="section-number">9.2.3. </span>Least squares method<a class="headerlink" href="#least-squares-method" title="Permalink to this heading">#</a></h3>
<p>A linear model is uniquely<a class="footnote-reference brackets" href="#footmodelserialise" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a> encoded using only
the coefficients <span class="math notranslate nohighlight">\(c_1,\dots,c_m\)</span>. To find them,
for each point <span class="math notranslate nohighlight">\(\mathbf{x}_{i,\cdot}\)</span> from the input (training) set,
we typically desire the <em>predicted</em> value:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\hat{y}_i = f(x_{i,1}, x_{i,2}, \dots, x_{i,m})
= f(\mathbf{x}_{i,\cdot}|\mathbf{c}) = \mathbf{c} \mathbf{x}_{i,\cdot}^T,
\]</div>
</div>
<p>to be as <em>close</em> to the corresponding reference <span class="math notranslate nohighlight">\(y_i\)</span> as possible.</p>
<p>There are many measures of <em>closeness</em>,
but the most popular one<a class="footnote-reference brackets" href="#footwhyssr" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a> uses the notion of
the <em>sum of squared residuals</em> (true minus predicted outputs):</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\mathrm{SSR}(\boldsymbol{c}|\mathbf{X},\mathbf{y})
= \sum_{i=1}^n \left( y_i - \hat{y}_i \right)^2
= \sum_{i=1}^n \left( y_i - (c_1 x_{i,1} + c_2 x_{i,2} + \dots + c_m x_{i,m}) \right)^2,
\]</div>
</div>
<p>which is a function of <span class="math notranslate nohighlight">\(\boldsymbol{c}=(c_1,\dots,c_m)\)</span>
(for fixed <span class="math notranslate nohighlight">\(\mathbf{X},\mathbf{y}\)</span>).</p>
<p>The <em>least squares</em> solution to the stated linear
regression problem will be defined by the coefficient vector <span class="math notranslate nohighlight">\(\boldsymbol{c}\)</span>
that minimises the SSR.
Based on what we said about matrix multiplication,
this is equivalent to solving the optimisation task:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\text{minimise}\
\left(\mathbf{y}-\mathbf{c} \mathbf{X}^T\right) \left(\mathbf{y}-\mathbf{c} \mathbf{X}^T\right)^T
\qquad\text{w.r.t. }{(c_1,\dots,c_m)\in\mathbb{R}^m},
\]</div>
</div>
<p>because
<span class="math notranslate nohighlight">\(\hat{\mathbf{y}}=\mathbf{c} \mathbf{X}^T\)</span> gives the predicted values
as a row vector
(the diligent readers are encouraged to check that on a piece of paper now),
<span class="math notranslate nohighlight">\(\mathbf{r}=\mathbf{y}-\hat{\mathbf{y}}\)</span> computes all the <span class="math notranslate nohighlight">\(n\)</span> residuals,
and <span class="math notranslate nohighlight">\(\mathbf{r}\mathbf{r}^T\)</span> gives their sum of squares.</p>
<p>The method of least squares is one of the
simplest and most natural approaches to regression analysis
(curve fitting). Its theoretical foundations (calculus…) were developed
more than 200 years ago by Gauss and then were polished by Legendre.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>(*)
Had the points lain on a hyperplane exactly (the interpolation problem),
<span class="math notranslate nohighlight">\(\mathbf{y} = \mathbf{c} \mathbf{X}^T\)</span> would have an exact solution,
equivalent to solving the linear system of equations
<span class="math notranslate nohighlight">\(\mathbf{y}-\mathbf{c} \mathbf{X}^T =\mathbf{0}\)</span>.
However, in our setting we assume that there might be some
measurement errors or other discrepancies between the reality and the
theoretical model. To account for this,
we are trying to solve a more general problem
of finding a hyperplane for which
<span class="math notranslate nohighlight">\(\|\mathbf{y}-\mathbf{c} \mathbf{X}^T\|^2\)</span> is as small as possible.</p>
<p>This optimisation task can be solved analytically
(compute the partial derivatives of SSR with respect to
each <span class="math notranslate nohighlight">\(c_1,\dots,c_m\)</span>, equate them to 0, and solve a simple system
of linear equations). This spawns
<span class="math notranslate nohighlight">\(\mathbf{c} = \mathbf{y} \mathbf{X} (\mathbf{X}^T \mathbf{X})^{-1}\)</span>,
where <span class="math notranslate nohighlight">\(\mathbf{A}^{-1}\)</span> is the inverse of a matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>,
i.e., the matrix such that
<span class="math notranslate nohighlight">\(\mathbf{A} \mathbf{A}^{-1}=\mathbf{A}^{-1} \mathbf{A} =\mathbf{I}\)</span>;
compare <strong class="command">numpy.linalg.inv</strong>.
As inverting larger matrices directly is not too robust numerically,
we would rather rely on a more specialised algorithm.</p>
<p>The <strong class="command">scipy.linalg.lstsq</strong> function that we use below
provides a quite numerically stable (yet, see <a class="reference internal" href="#sec-ill-condition"><span class="std std-numref">Section 9.2.9</span></a>)
procedure that is based on the singular value decomposition of the model
matrix.</p>
</div>
<div style="margin-top: 1em"></div><p>Let us go back to the NHANES study excerpt
and express weight (the first column) as function
of hip circumference (the sixth column)
again, but this time using an affine map of the form<a class="footnote-reference brackets" href="#footerror" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a>:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\text{weight}=a\cdot\text{hip circumference} + b  \qquad (+ \text{some error}).
\]</div>
</div>
<p>The <em>design (model) matrix</em> <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>
and the reference <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>s are:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_original</span> <span class="o">=</span> <span class="n">body</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">5</span><span class="p">]]</span>     <span class="c1"># a column vector</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">x_original</span><span class="o">**</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>  <span class="c1"># hip circumference, 1s</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">body</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>          <span class="c1"># weight</span>
</pre></div>
</div>
<p>We used the vectorised exponentiation operator
to convert each <span class="math notranslate nohighlight">\(x_i\)</span> (the <span class="math notranslate nohighlight">\(i\)</span>-th hip circumference) to a pair
<span class="math notranslate nohighlight">\(\mathbf{x}_{i,\cdot}=(x_i^1, x_i^0) = (x_i, 1)\)</span>, which is a nice trick
to append a column of <span class="math notranslate nohighlight">\(1\)</span>s to a matrix.
This way, we included the intercept term in the model
(as discussed in <a class="reference internal" href="#sec-linearmodel"><span class="std std-numref">Section 9.2.2</span></a>).
Here is a preview:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">preview_indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">13</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">[</span><span class="n">preview_indices</span><span class="p">,</span> <span class="p">:]</span>
<span class="c1">## array([[ 92.5,   1. ],</span>
<span class="c1">##        [106.7,   1. ],</span>
<span class="c1">##        [ 96.3,   1. ],</span>
<span class="c1">##        [102. ,   1. ],</span>
<span class="c1">##        [ 94.8,   1. ],</span>
<span class="c1">##        [ 97.5,   1. ]])</span>
<span class="n">y_train</span><span class="p">[</span><span class="n">preview_indices</span><span class="p">]</span>
<span class="c1">## array([55.4, 62. , 66.2, 77.2, 64.2, 56.8])</span>
</pre></div>
</div>
<p>Let us determine the least squares solution to our regression problem:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.linalg</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<p>That’s it.
The optimal coefficients vector (the one that minimises the SSR) is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">c</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">c</span>
<span class="c1">## array([  1.3052463 , -65.10087248])</span>
</pre></div>
</div>
<p>The estimated model is:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\text{weight}=1.305\cdot\text{hip circumference} -65.1 \qquad (+ \text{some error}).
\]</div>
</div>
<p>Let us contemplate the fact that the model is nicely interpretable.
For instance, as hip circumference increases, we expect the weights
to be greater and greater.
As we said before, it does not mean that there is some <em>causal</em>
relationship between the two (for instance, there can be some
latent variables that affect both of them).
Instead, there is some general tendency regarding
how the data align in the sample space.
For instance, that the “best guess” (according to the current model –
there can be many; see below) weight for a person with
hip circumference of 100 cm is 65.4 kg.
Thanks to such models, we might understand certain phenomena better
or find some proxies for different variables (especially if measuring
them directly is tedious, costly, dangerous, etc.).</p>
<p>Let us determine the predicted weights for all of the participants:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">c</span> <span class="o">@</span> <span class="n">X_train</span><span class="o">.</span><span class="n">T</span>
<span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">y_pred</span><span class="p">[</span><span class="n">preview_indices</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># preview</span>
<span class="c1">## array([55.63, 74.17, 60.59, 68.03, 58.64, 62.16])</span>
</pre></div>
</div>
<p>The scatter plot and the fitted regression line in <a class="reference internal" href="#fig-hip-weight-lm"><span class="std std-numref">Figure 9.8</span></a>
indicates a fair fit but, of course, there is some natural variability.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_original</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>  <span class="c1"># scatter plot</span>
<span class="n">_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">x_original</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">x_original</span><span class="o">.</span><span class="n">max</span><span class="p">()])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">_y</span> <span class="o">=</span> <span class="n">c</span> <span class="o">@</span> <span class="p">(</span><span class="n">_x</span><span class="o">**</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="s2">&quot;r-&quot;</span><span class="p">)</span>  <span class="c1"># a line that goes through the two extreme points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;hip circumference&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;weight&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id38">
<span id="fig-hip-weight-lm"></span><img alt="../_images/hip-weight-lm-15.png" src="../_images/hip-weight-lm-15.png" />
<figcaption>
<p><span class="caption-number">Figure 9.8 </span><span class="caption-text">The least squares line for weight vs hip circumference.</span><a class="headerlink" href="#id38" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<div class="proof proof-type-exercise" id="id39">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.8</span>
        
    </div><div class="proof-content">
<p>The <a class="reference external" href="https://github.com/gagolews/teaching-data/raw/master/r/anscombe.csv">Anscombe quartet</a>
is a famous example dataset, where we have four pairs of
variables that have almost identical means, variances, and linear correlation
coefficients. Even though they can be approximated by the same straight
line, their scatter plots are vastly different.
Reflect upon this toy example.</p>
</div></div></section>
<section id="analysis-of-residuals">
<h3><span class="section-number">9.2.4. </span>Analysis of residuals<a class="headerlink" href="#analysis-of-residuals" title="Permalink to this heading">#</a></h3>
<p>The residuals (i.e., the estimation errors – what we expected vs what
we got), for the chosen 6 observations are
visualised in <a class="reference internal" href="#fig-residuals"><span class="std std-numref">Figure 9.9</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">r</span> <span class="o">=</span> <span class="n">y_train</span> <span class="o">-</span> <span class="n">y_pred</span>  <span class="c1"># residuals</span>
<span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="n">preview_indices</span><span class="p">],</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># preview</span>
<span class="c1">## array([ -0.23, -12.17,   5.61,   9.17,   5.56,  -5.36])</span>
</pre></div>
</div>
<figure class="align-default" id="id40">
<span id="fig-residuals"></span><img alt="../_images/residuals-17.png" src="../_images/residuals-17.png" />
<figcaption>
<p><span class="caption-number">Figure 9.9 </span><span class="caption-text">Example residuals in a simple linear regression task.</span><a class="headerlink" href="#id40" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>We wanted the squared residuals (on average – across all the points)
to be as small as possible.
The least squares method assures that
this is the case <em>relative to the chosen model</em>,
i.e., a linear one.
Nonetheless, it still does not mean that what we obtained
constitutes a good fit to the training data.
Thus, we need to perform the <em>analysis of residuals</em>.</p>
<p>Interestingly, the average of residuals is always zero:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i) = 0.
\]</div>
</div>
<p>Therefore, if we want to summarise the residuals into a single number,
we can use, for example, the root mean squared error instead:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
 \mathrm{RMSE}(\mathbf{y}, \hat{\mathbf{y}})
 = \sqrt{\frac{1}{n} \sum_{i=1}^n (y_i-\hat{y}_i)^2}.
\]</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
<span class="c1">## 6.948470091176111</span>
</pre></div>
</div>
<p>Hopefully we can see that RMSE is a function of SSR that
we sought to minimise above.</p>
<p>Alternatively, we can compute the mean absolute error:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
 \mathrm{MAE}(\mathbf{y}, \hat{\mathbf{y}})
 = \frac{1}{n} \sum_{i=1}^n |y_i-\hat{y}_i|.
\]</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
<span class="c1">## 5.207073583769202</span>
</pre></div>
</div>
<p>MAE is nicely interpretable: it measures by
how many kilograms we err <em>on average</em>. Not bad.</p>
<div class="proof proof-type-exercise" id="id41">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.9</span>
        
    </div><div class="proof-content">
<p>Fit a regression line
explaining weight as a function of the waist circumference
and compute the corresponding RMSE and MAE.
Are they better than when hip circumference is used as an
explanatory variable?</p>
</div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Generally, fitting simple (involving one independent variable)
linear models can only make sense for highly linearly correlated variables.
Interestingly, if <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> are both standardised,
and <span class="math notranslate nohighlight">\(r\)</span> is their Pearson’s coefficient, then the least squares
solution is given by <span class="math notranslate nohighlight">\(y=rx\)</span>.</p>
</div>
<p>To verify whether a fitted model is not extremely wrong
(e.g., when we fit a linear model to data that clearly follows
a different functional relationship),
a plot of residuals against the fitted values
can be of help; see <a class="reference internal" href="#fig-resid-vs-fit"><span class="std std-numref">Figure 9.10</span></a>.
Ideally, the points are expected to be aligned totally
at random therein, without any dependence structure
(homoscedasticity).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>  <span class="c1"># horizontal line at y=0</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;fitted values&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;residuals&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id42">
<span id="fig-resid-vs-fit"></span><img alt="../_images/resid-vs-fit-19.png" src="../_images/resid-vs-fit-19.png" />
<figcaption>
<p><span class="caption-number">Figure 9.10 </span><span class="caption-text">Residuals vs fitted values for the linear model explaining weight as a function of hip circumference. The variance of residuals slightly increases as <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> increases. This is not ideal, but it could be much worse than this.</span><a class="headerlink" href="#id42" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<div class="proof proof-type-exercise" id="id43">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.10</span>
        
    </div><div class="proof-content">
<p>Compare<a class="footnote-reference brackets" href="#footknnas" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a> the RMSE and MAE for the <span class="math notranslate nohighlight">\(k\)</span>-nearest neighbour
regression curves depicted in the left side of <a class="reference internal" href="#fig-knn-reg"><span class="std std-numref">Figure 9.7</span></a>.
Also, draw the residuals vs fitted plot.</p>
</div></div><div style="margin-top: 1em"></div><p>For linear models fitted using the least squares method,
it can be shown that it holds:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\frac{1}{n} \sum_{i=1}^{n} \left(y_i-\bar{y}\right)^2 =
\frac{1}{n} \sum_{i=1}^{n} \left(\hat{y}_i-\bar{\hat{y}}\right)^2 +
\frac{1}{n} \sum_{i=1}^{n} \left(y_i-\hat{y}_i\right)^2.
\]</div>
</div>
<p>In other words, the variance of the dependent variable (left)
can be decomposed into the sum of the variance of the predictions
and the averaged squared residuals.
Multiplying the above by <span class="math notranslate nohighlight">\(n\)</span>, we have that
the <em>total</em> sum of squares is equal to the
<em>explained</em> sum of squares plus the <em>residual</em> sum of squares:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\mathrm{TSS} = \mathrm{ESS} + \mathrm{RSS}.
\]</div>
</div>
<p>We yearn for ESS to be as close to TSS as possible.
Equivalently, it would be jolly nice to have RSS equal to 0.</p>
<p>The <em>coefficient of determination</em> (unadjusted R-Squared,
sometimes referred to as simply the <em>score</em>)
is a popular normalised, unitless measure
that is easier to interpret than raw ESS or RSS
when we have no domain-specific knowledge of the modelled problem.
It is given by:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
R^2(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\mathrm{ESS}}{\mathrm{TSS}}
= 1 - \frac{\mathrm{RSS}}{\mathrm{TSS}}
= 1-\frac{s_r^2}{s_y^2}.
\]</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y_train</span><span class="o">-</span><span class="n">y_pred</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
<span class="c1">## 0.8959634726270759</span>
</pre></div>
</div>
<p>The coefficient of determination in the current context<a class="footnote-reference brackets" href="#footscore" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a>
is thus the proportion of variance of the
dependent variable explained by the independent variables in the model.
The closer it is to 1, the better.
A dummy model that always returns the mean of <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>
gives R-squared of 0.</p>
<p>In our case, <span class="math notranslate nohighlight">\(R^2\simeq 0.9\)</span> is quite high, which indicates a
rather good fit.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>(*) There are certain statistical results that can be relied upon
provided that the residuals are independent random variables
with expectation zero and the same variance (e.g.,
the Gauss–Markov theorem). Further, if they are normally distributed,
then we have several hypothesis tests available (e.g.,
for the significance of coefficients). This is why in various textbooks
such assumptions are additionally verified.
But we do not go that far in this introductory course.</p>
</div>
</section>
<section id="multiple-regression">
<h3><span class="section-number">9.2.5. </span>Multiple regression<a class="headerlink" href="#multiple-regression" title="Permalink to this heading">#</a></h3>
<p>As another example, let us fit a model involving two
independent variables, arm and hip circumference:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">body</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]],</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># append a column of 1s</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1">## array([  1.3 ,   0.9 , -63.38])</span>
</pre></div>
</div>
<p>We fitted the plane:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\text{weight} = 1.3\, \text{arm circumference} + 0.9\, \text{hip circumference}  -63.38 .\]</div>
</div>
<p>We skip the visualisation part for we do not expect it to result
in a readable plot: these are multidimensional data.
The coefficient of determination is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">c</span> <span class="o">@</span> <span class="n">X_train</span><span class="o">.</span><span class="n">T</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">y_train</span> <span class="o">-</span> <span class="n">y_pred</span>
<span class="mi">1</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">r</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
<span class="c1">## 0.9243996585518783</span>
</pre></div>
</div>
<p>Root mean squared error:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
<span class="c1">## 5.923223870044694</span>
</pre></div>
</div>
<p>Mean absolute error:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
<span class="c1">## 4.431548244333893</span>
</pre></div>
</div>
<p>It is a slightly better model than the previous one.
We can predict the participants’ weights with better precision,
at the cost of an increased model’s complexity.</p>
</section>
<section id="variable-transformation-and-linearisable-models">
<span id="sec-linearisation"></span><h3><span class="section-number">9.2.6. </span>Variable transformation and linearisable models (*)<a class="headerlink" href="#variable-transformation-and-linearisable-models" title="Permalink to this heading">#</a></h3>
<p>We are not restricted merely to linear functions
of the input variables. By applying arbitrary
transformations upon the columns of the design matrix, we can cover
many diverse scenarios.</p>
<p>For instance, a polynomial model involving two variables:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
g(v_1, v_2)
= \beta_0 + \beta_1 v_1 + \beta_2 v_1^2 + \beta_3 v_1 v_2 + \beta_4 v_2 + \beta_5 v_2^2,
\]</div>
</div>
<p>can be obtained by substituting
<span class="math notranslate nohighlight">\(x_1=1\)</span>, <span class="math notranslate nohighlight">\(x_2=v_1\)</span>, <span class="math notranslate nohighlight">\(x_3=v_1^2\)</span>, <span class="math notranslate nohighlight">\(x_4=v_1 v_2\)</span>, <span class="math notranslate nohighlight">\(x_5=v_2\)</span>, <span class="math notranslate nohighlight">\(x_6=v_2^2\)</span>,
and then fitting a linear model involving six variables:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
f(x_1, x_2, \dots, x_6) = c_1 x_1 + c_2 x_2 + \dots + x_6 x_6.
\]</div>
</div>
<p>The design matrix is made of rubber, it can handle almost anything.
If we have a linear model, but with respect to transformed
data, the algorithm does not care. This is the beauty of the underlying
mathematics; see also <span id="id10">[<a class="reference internal" href="999-bibliography.html#id146" title="Box, G.E.P. and Cox, D.R. (1964).  An analysis of transformations. Journal of the Royal Statistical Society. Series B (Methodological), 26(2):211–252.">10</a>]</span>.</p>
<p>A creative modeller can also turn models such as <span class="math notranslate nohighlight">\(u=c e^{av}\)</span> into <span class="math notranslate nohighlight">\(y=ax+b\)</span>
by replacing <span class="math notranslate nohighlight">\(y=\log u\)</span>, <span class="math notranslate nohighlight">\(x=v\)</span>, and <span class="math notranslate nohighlight">\(b=\log c\)</span>. There are numerous
possibilities based on the properties
of the <span class="math notranslate nohighlight">\(\log\)</span> and <span class="math notranslate nohighlight">\(\exp\)</span> functions listed in <a class="reference internal" href="220-transform-vector.html#sec-vecfun"><span class="std std-numref">Section 5.2</span></a>.
We call them <em>linearisable models</em>.</p>
<div style="margin-top: 1em"></div><p>As an example, let us model the life expectancy at birth
in different countries as a function of their GDP per capita (PPP).</p>
<p>We will consider four different models:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(y=c_1+c_2 x\)</span> (linear),</p></li>
<li><p><span class="math notranslate nohighlight">\(y=c_1+c_2 x+c_3x^2\)</span> (quadratic),</p></li>
<li><p><span class="math notranslate nohighlight">\(y=c_1+c_2 x+c_3x^2+c_4x^3\)</span> (cubic),</p></li>
<li><p><span class="math notranslate nohighlight">\(y=c_1+c_2\log x\)</span> (logarithmic).</p></li>
</ol>
<p>Here are the helper functions that create the model matrices:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_model_matrix1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">make_model_matrix2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">make_model_matrix3</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">make_model_matrix4</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">make_model_matrix1</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="s2">&quot;linear model&quot;</span>
<span class="n">make_model_matrix2</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="s2">&quot;quadratic model&quot;</span>
<span class="n">make_model_matrix3</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="s2">&quot;cubic model&quot;</span>
<span class="n">make_model_matrix4</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="s2">&quot;logarithmic model&quot;</span>

<span class="n">model_matrix_makers</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">make_model_matrix1</span><span class="p">,</span>
    <span class="n">make_model_matrix2</span><span class="p">,</span>
    <span class="n">make_model_matrix3</span><span class="p">,</span>
    <span class="n">make_model_matrix4</span>
<span class="p">]</span>
<span class="n">x_original</span> <span class="o">=</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">Xs_train</span> <span class="o">=</span> <span class="p">[</span> <span class="n">make_model_matrix</span><span class="p">(</span><span class="n">x_original</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">make_model_matrix</span> <span class="ow">in</span> <span class="n">model_matrix_makers</span> <span class="p">]</span>
</pre></div>
</div>
<p>Fitting the models:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_train</span> <span class="o">=</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">cs</span> <span class="o">=</span> <span class="p">[</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">X_train</span> <span class="ow">in</span> <span class="n">Xs_train</span> <span class="p">]</span>
</pre></div>
</div>
<p>Their coefficients of determination are equal to:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Xs_train</span><span class="p">)):</span>
    <span class="n">R2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y_train</span> <span class="o">-</span> <span class="n">cs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">@</span> <span class="n">Xs_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_matrix_makers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="vm">__name__</span><span class="si">:</span><span class="s2">20</span><span class="si">}</span><span class="s2"> R2=</span><span class="si">{</span><span class="n">R2</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1">## linear model         R2=0.431</span>
<span class="c1">## quadratic model      R2=0.567</span>
<span class="c1">## cubic model          R2=0.607</span>
<span class="c1">## logarithmic model    R2=0.651</span>
</pre></div>
</div>
<p>The logarithmic model is thus the best (out of the models we
considered). The four models are depicted in
<a class="reference internal" href="#fig-gdp-life-four-models"><span class="std std-numref">Figure 9.11</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_original</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_original</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">x_original</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">101</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">model_matrix_makers</span><span class="p">)):</span>
    <span class="n">_y</span> <span class="o">=</span> <span class="n">cs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">@</span> <span class="n">model_matrix_makers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">_x</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">_y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">model_matrix_makers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;per capita GDP PPP&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;life expectancy (years)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id44">
<span id="fig-gdp-life-four-models"></span><img alt="../_images/gdp-life-four-models-21.png" src="../_images/gdp-life-four-models-21.png" />
<figcaption>
<p><span class="caption-number">Figure 9.11 </span><span class="caption-text">Different models for life expectancy vs GDP.</span><a class="headerlink" href="#id44" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<div class="proof proof-type-exercise" id="id45">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.11</span>
        
    </div><div class="proof-content">
<p>Draw box plots and histograms of residuals
for each model as well as the scatter plots of residuals vs fitted values.</p>
</div></div></section>
<section id="descriptive-vs-predictive-power">
<h3><span class="section-number">9.2.7. </span>Descriptive vs predictive power (*)<a class="headerlink" href="#descriptive-vs-predictive-power" title="Permalink to this heading">#</a></h3>
<p>We <em>approximated</em> the life vs GDP relationship
using a few different functions. Nevertheless, we see that the above
quadratic and cubic models possibly do not make
much sense, semantically speaking.
Sure, as far as individual points <em>in the training set</em> are concerned,
they do fit the data better than the linear model.
After all, they have smaller mean squared errors (again: at these
given points).
Looking at the way they behave, one does not need a
university degree in economics/social policy to conclude that they
are not the best <em>description</em> of how the reality behaves (on average).</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Naturally, a model’s goodness of fit to observed data tends to improve as the
model’s complexity increases. The Razor principle
(by William of Ockham et al.) advises that if some phenomenon can be explained
in many different ways, the simplest explanation should be chosen
(<em>do not multiply entities</em> [here: introduce independent variables]
<em>without necessity</em>).</p>
</div>
<p>In particular, the more independent variables we have in the model,
the greater the <span class="math notranslate nohighlight">\(R^2\)</span> coefficient will be.
We can try correcting for this phenomenon
by considering the <em>adjusted</em> <span class="math notranslate nohighlight">\(R^2\)</span>:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\bar{R}^2(\mathbf{y},\hat{\mathbf{y}}) = 1 - (1-{R}^2(\mathbf{y}, \hat{\mathbf{y}}))\frac{n-1}{n-m-1},
\]</div>
</div>
<p>which, to some extent, penalises more complex models.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>(**) Model quality measures adjusted for the number of model
parameters, <span class="math notranslate nohighlight">\(m\)</span>, can also be useful in automated variable selection.
For example, the Akaike Information Criterion is
a popular measure given by:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\mathrm{AIC} = 2m+n\log(\mathrm{SSR})-n\log n.
\]</div>
</div>
<p>Furthermore, the Bayes Information Criterion is defined via:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\mathrm{BIC} = m\log n+n\log(\mathrm{SSR})-n\log n.
\]</div>
</div>
<p>Unfortunately, they are both dependent on the scale of <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>.</p>
</div>
<p>We should also be interested in a model’s <em>predictive</em> power, i.e.,
how well does it generalise to data points that we do not have now
(or pretend we do not have) but might face in the future.
As we observe the modelled reality only at a few
different points, the question is how the model performs
when filling the gaps between the dots it connects.</p>
<p>In particular, we must be careful when <em>extrapolating</em>
the data, i.e., making predictions outside of its usual domain.
For example, the linear model predicts the following life expectancy
for an imaginary country with $500 000 per capita GDP:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">@</span> <span class="n">model_matrix_makers</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">500000</span><span class="p">]))</span><span class="o">.</span><span class="n">T</span>
<span class="c1">## array([164.3593753])</span>
</pre></div>
</div>
<p>and the quadratic one gives:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">@</span> <span class="n">model_matrix_makers</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">500000</span><span class="p">]))</span><span class="o">.</span><span class="n">T</span>
<span class="c1">## array([-364.10630779])</span>
</pre></div>
</div>
<p>Nonsense.</p>
<div class="proof proof-type-example" id="id46">

    <div class="proof-title">
        <span class="proof-type">Example 9.12</span>
        
    </div><div class="proof-content">
<p>Let us consider the following theoretical illustration.
Assume that a true model of some reality is <span class="math notranslate nohighlight">\(y=5+3x^3\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">true_model</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">5</span> <span class="o">+</span> <span class="mi">3</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>Still, for some reason we are only able to gather a small (<span class="math notranslate nohighlight">\(n=25\)</span>)
sample from this model. What is even worse, it is subject to
some measurement error:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">25</span><span class="p">)</span>                           <span class="c1"># random xs on [0, 1]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">true_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># true_model(x) + noise</span>
</pre></div>
</div>
<p>The least-squares fitting of <span class="math notranslate nohighlight">\(y=c_1+c_2 x^3\)</span> to the above gives:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X03</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">c03</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X03</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ssr03</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">c03</span> <span class="o">@</span> <span class="n">X03</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">c03</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1">## array([5.01, 3.13])</span>
</pre></div>
</div>
<p>which is not too far, but still somewhat<a class="footnote-reference brackets" href="#footmle" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a> distant from the true
coefficients, 5 and 3.</p>
<p>We can also fit a more flexible cubic polynomial,
<span class="math notranslate nohighlight">\(y=c_1+c_2 x+c_3 x^2+c_4 x_3\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X0123</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">c0123</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X0123</span><span class="p">,</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">ssr0123</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">c0123</span> <span class="o">@</span> <span class="n">X0123</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">c0123</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1">## array([4.89, 0.32, 0.57, 2.23])</span>
</pre></div>
</div>
<p>In terms of the SSR, this more complex model of
course explains <em>the training data</em> better:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ssr03</span><span class="p">,</span> <span class="n">ssr0123</span>
<span class="c1">## (1.0612111154029558, 0.9619488226837544)</span>
</pre></div>
</div>
<p>Yet, it is farther away from the <em>truth</em>
(which, whilst performing the fitting task based only on given
<span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span>, is unknown).
We may thus say that the first model <em>generalises</em>
better on yet-to-be-observed data;
see <a class="reference internal" href="#fig-true-model-vs-guess"><span class="std std-numref">Figure 9.12</span></a> for an illustration.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">true_model</span><span class="p">(</span><span class="n">_x</span><span class="p">),</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;true model&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">c0123</span> <span class="o">@</span> <span class="p">(</span><span class="n">_x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;fitted model y=x**[0, 1, 2, 3]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">c03</span> <span class="o">@</span> <span class="p">(</span><span class="n">_x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;fitted model y=x**[0, 3]&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id47">
<span id="fig-true-model-vs-guess"></span><img alt="../_images/true-model-vs-guess-23.png" src="../_images/true-model-vs-guess-23.png" />
<figcaption>
<p><span class="caption-number">Figure 9.12 </span><span class="caption-text">The true (theoretical) model vs some guesstimates (fitted based on noisy data). More degrees of freedom is not always better.</span><a class="headerlink" href="#id47" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</div></div><div class="proof proof-type-example" id="id48">

    <div class="proof-title">
        <span class="proof-type">Example 9.13</span>
        
    </div><div class="proof-content">
<p>(**)
We defined the sum of squared residuals
(and its function, the root mean squared error) by means
of the averaged deviation from the reference values.
They are subject to error themselves, though.
Even though they are our best-shot approximation of the truth,
they should be taken with a degree of scepticism.</p>
<p>In the above example, given the true (reference) model <span class="math notranslate nohighlight">\(f\)</span> defined
over the domain <span class="math notranslate nohighlight">\(D\)</span> (in our case, <span class="math notranslate nohighlight">\(f(x)=5+3x^3\)</span> and <span class="math notranslate nohighlight">\(D=[0,1]\)</span>)
and an empirically fitted model <span class="math notranslate nohighlight">\(\hat{f}\)</span>, we can compute
the square root of the integrated squared error
over the whole <span class="math notranslate nohighlight">\(D\)</span>:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\mathrm{RMSE}(f, \hat{f}) = \sqrt{
    \int_D (f(x)-\hat{f}(x))^2\, dx
}.
\]</div>
</div>
<p>For polynomials and other simple functions,
RMSE can be computed analytically.
More generally, we can approximate it numerically by sampling the above
at sufficiently many points and applying the trapezoidal rule
(e.g., <span id="id12">[<a class="reference internal" href="999-bibliography.html#id142" title="Press, W.H., Teukolsky, S.A., Vetterling, W.T., and Flannery, B.P. (2007).  Numerical Recipes. The Art of Scientific Computing. Cambridge University Press.">74</a>]</span>). As this can be an educative programming exercise,
below we consider a range of polynomial models of different degrees.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cs</span><span class="p">,</span> <span class="n">rmse_train</span><span class="p">,</span> <span class="n">rmse_test</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>  <span class="c1"># result containers</span>
<span class="n">ps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># polynomial degrees</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">ps</span><span class="p">:</span>           <span class="c1"># for each polynomial degree:</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># fit</span>
    <span class="n">cs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">c</span> <span class="o">@</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>   <span class="c1"># predictions</span>
    <span class="n">rmse_train</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y</span><span class="o">-</span><span class="n">y_pred</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)))</span>  <span class="c1"># RMSE</span>

    <span class="n">_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">101</span><span class="p">)</span>                         <span class="c1"># many _xs</span>
    <span class="n">_y</span> <span class="o">=</span> <span class="n">c</span> <span class="o">@</span> <span class="p">(</span><span class="n">_x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">p</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>      <span class="c1"># f(_x)</span>
    <span class="n">_r</span> <span class="o">=</span> <span class="p">(</span><span class="n">true_model</span><span class="p">(</span><span class="n">_x</span><span class="p">)</span> <span class="o">-</span> <span class="n">_y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>                       <span class="c1"># residuals</span>
    <span class="n">rmse_test</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">diff</span><span class="p">(</span><span class="n">_x</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">_r</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">+</span><span class="n">_r</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># trapezoidal rule for integration</span>
    <span class="p">)))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ps</span><span class="p">,</span> <span class="n">rmse_train</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;RMSE (training set)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ps</span><span class="p">,</span> <span class="n">rmse_test</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;RMSE (theoretical)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;model complexity (polynomial degree)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id49">
<span id="fig-true-model-vs-polynomials-rmse"></span><img alt="../_images/true-model-vs-polynomials-rmse-25.png" src="../_images/true-model-vs-polynomials-rmse-25.png" />
<figcaption>
<p><span class="caption-number">Figure 9.13 </span><span class="caption-text">Small RMSE on training data does not necessarily imply good generalisation abilities.</span><a class="headerlink" href="#id49" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#fig-true-model-vs-polynomials-rmse"><span class="std std-numref">Figure 9.13</span></a> shows that a model’s
ability to make correct generalisations onto unseen data
improves as the complexity increases, at least initially.
However, then it becomes worse. It is quite a typical behaviour.
In fact, the model with the smallest RMSE on the
training set, <em>overfits</em> to the input sample,
see <a class="reference internal" href="#fig-true-model-vs-polynomials"><span class="std std-numref">Figure 9.14</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">true_model</span><span class="p">(</span><span class="n">_x</span><span class="p">),</span> <span class="s2">&quot;--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;true model&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">]:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">cs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">@</span> <span class="p">(</span><span class="n">_x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">ps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;fitted degree-</span><span class="si">{</span><span class="n">ps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2"> polynomial&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id50">
<span id="fig-true-model-vs-polynomials"></span><img alt="../_images/true-model-vs-polynomials-27.png" src="../_images/true-model-vs-polynomials-27.png" />
<figcaption>
<p><span class="caption-number">Figure 9.14 </span><span class="caption-text">Under- and overfitting to training data.</span><a class="headerlink" href="#id50" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</div></div><div class="admonition important">
<p class="admonition-title">Important</p>
<p>When evaluating a model’s quality in terms of predictive power on unseen
data, we should go beyond inspecting its behaviour merely on the points
from the training sample. As the <em>truth</em> is usually not known (if it were,
we would not need any guessing), a common approach in case where we have
a dataset of a considerable size is to divide it (randomly;
see <a class="reference internal" href="410-data-frame.html#sec-random-sampling"><span class="std std-numref">Section 10.5.4</span></a>) into two parts:</p>
<ul class="simple">
<li><p>training sample (say, 60%) – used to fit a model,</p></li>
<li><p>test sample (the remaining 40%) – used to assess its quality
(e.g., by means of RMSE).</p></li>
</ul>
<p>This might <em>emulate</em> an environment where some new data arrives later,
see <a class="reference internal" href="430-group-by.html#sec-train-test-split"><span class="std std-numref">Section 12.3.3</span></a> for more details.</p>
<p>Furthermore, if model selection is required, we may apply a
training/validation/test split (say, 60/20/20%;
see <a class="reference internal" href="430-group-by.html#sec-model-validation"><span class="std std-numref">Section 12.3.4</span></a>).
Here, many models are constructed on the training set,
the validation set is used to compute the metrics
and choose the best model, and then the test set gives the final
model’s valuation to assure its usefulness/uselessness
(because we do not want it to overfit to the test set).</p>
</div>
<p>Overall, models must never be blindly trusted. Common sense must always
be applied. The fact that we fitted something using a sophisticated
procedure on a dataset that was hard to obtain does not justify
its use.
Mediocre models must be discarded, and we should move on,
regardless of how much time/resources we have invested whilst
developing them.
Too many bad models go into production and make our daily lives harder.
We need to end this madness.</p>
</section>
<section id="fitting-regression-models-with-scikit-learn">
<span id="sec-sklearnlm"></span><h3><span class="section-number">9.2.8. </span>Fitting regression models with <strong class="program">scikit-learn</strong> (*)<a class="headerlink" href="#fitting-regression-models-with-scikit-learn" title="Permalink to this heading">#</a></h3>
<p><a class="reference external" href="https://scikit-learn.org/stable/index.html"><strong class="program">scikit-learn</strong></a>
(<strong class="program">sklearn</strong>; <span id="id13">[<a class="reference internal" href="999-bibliography.html#id7" title="Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. (2011).  Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.">72</a>]</span>) is a huge Python package
built on top of <strong class="program">numpy</strong>, <strong class="program">scipy</strong>,
and <strong class="program">matplotlib</strong>. It has a consistent API and implements or
provides wrappers for many regression, classification, clustering, and
dimensionality reduction algorithms (amongst others).</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong class="program">scikit-learn</strong> is very convenient. Nevertheless, it permits
us to fit models even when we do not understand
the mathematics behind them. This is dangerous: it is
like driving a sports car without the necessary skills
and, at the same time, wearing a blindfold.
Advanced students and practitioners will appreciate it, but
if used by beginners, it needs to be handled with care.
We should not mistake something’s being easily accessible
with its being safe to use.
Remember that if we are given a procedure for which we are not able
to provide its definition/mathematical properties/explain
its idealised version using pseudocode,
we are expected to refrain from using it (see Rule#7).</p>
</div>
<p>Because of the above, we shall only present a quick demo of
<strong class="program">scikit-learn</strong>’s API.
Let us do that by fitting a multiple linear regression model
for, again, weight as a function of the arm and the hip circumference:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">body</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">body</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>In <strong class="program">scikit-learn</strong>, once we construct an object representing
the model to be fitted, the <strong class="command">fit</strong> method determines the optimal
parameters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sklearn.linear_model</span>
<span class="n">lm</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">lm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">lm</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">coef_</span>
<span class="c1">## (-63.383425410947524, array([1.30457807, 0.8986582 ]))</span>
</pre></div>
</div>
<p>We, of course, obtained the same solution
as with <strong class="command">scipy.linalg.lstsq</strong>.</p>
<p>Computing the predicted values can be done via the
<strong class="command">predict</strong> method.
For example, we can calculate the coefficient of determination:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span>
<span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="c1">## 0.9243996585518783</span>
</pre></div>
</div>
<p>The above function is convenient, but can we really recall the formula for
the score and what it measures?
We should always be able to do that.</p>
</section>
<section id="ill-conditioned-model-matrices">
<span id="sec-ill-condition"></span><h3><span class="section-number">9.2.9. </span>Ill-conditioned model matrices (*)<a class="headerlink" href="#ill-conditioned-model-matrices" title="Permalink to this heading">#</a></h3>
<p>Our approach to regression analysis relies on solving
an optimisation problem (the method least squares).
Nevertheless, sometimes the “optimal” solution that the algorithm returns
might have nothing to do with the <em>true</em> minimum.
And this is despite the fact that we have the theoretical
results stating that the solution is unique<a class="footnote-reference brackets" href="#footunique" id="id14" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a>
(the objective is convex).
The problem stems from our using the computer’s finite-precision
floating point arithmetic; compare <a class="reference internal" href="220-transform-vector.html#sec-fp-error"><span class="std std-numref">Section 5.5.6</span></a>.</p>
<div style="margin-top: 1em"></div><p>Let us fit a degree-4 polynomial to the life expectancy vs per capita GDP
dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_original</span> <span class="o">=</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_original</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">world</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">cs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</pre></div>
</div>
<p>We store the estimated model coefficients in a dictionary
because many methods will follow next.
First, <strong class="command">scipy</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">res</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">cs</span><span class="p">[</span><span class="s2">&quot;scipy_X&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">cs</span><span class="p">[</span><span class="s2">&quot;scipy_X&quot;</span><span class="p">]</span>
<span class="c1">## array([ 2.33103950e-16,  6.42872371e-12,  1.34162021e-07,</span>
<span class="c1">##        -2.33980973e-12,  1.03490968e-17])</span>
</pre></div>
</div>
<p>If we drew the fitted polynomial now (see <a class="reference internal" href="#fig-ill-cond"><span class="std std-numref">Figure 9.15</span></a>),
we would see that the fit is unbelievably bad.
The result returned by <strong class="command">scipy.linalg.lstsq</strong>
is now not at all optimal. All coefficients are approximately equal to 0.</p>
<div style="margin-top: 1em"></div><p>It turns out that the fitting problem is extremely <em>ill-conditioned</em>
(and it is not the algorithm’s fault): GDPs range from very small
to very large ones. Furthermore, taking them to the fourth power
breeds numbers of ever greater range.
Finding the least squares solution involves some form of matrix inverse
(not necessarily directly) and our model matrix may be close to singular
(one that is not invertible).</p>
<p>As a measure of the model matrix’s ill-conditioning,
we often use the <em>condition number</em>, denoted
<span class="math notranslate nohighlight">\(\kappa(\mathbf{X}^T)\)</span>. It is
the ratio of the largest to the smallest
<em>singular values</em><a class="footnote-reference brackets" href="#footsingularvalue" id="id15" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a> of <span class="math notranslate nohighlight">\(\mathbf{X}^T\)</span>,
which are returned by the <strong class="command">scipy.linalg.lstsq</strong> method itself:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>    <span class="c1"># singular values of X_train.T</span>
<span class="n">s</span>
<span class="c1">## array([5.63097211e+20, 7.90771769e+14, 4.48366565e+09, 6.77575417e+04,</span>
<span class="c1">##        5.76116463e+00])</span>
</pre></div>
</div>
<p>Note that they are already sorted nonincreasingly.
The condition number <span class="math notranslate nohighlight">\(\kappa(\mathbf{X}^T)\)</span> is equal to:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">s</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># condition number (largest/smallest singular value)</span>
<span class="c1">## 9.774017018467434e+19</span>
</pre></div>
</div>
<p>As a rule of thumb, if the condition number is <span class="math notranslate nohighlight">\(10^k\)</span>,
we are losing <span class="math notranslate nohighlight">\(k\)</span> digits of numerical precision when performing
the underlying computations. As the above number is exceptionally large,
we are thus currently faced with a very ill-conditioned problem.
If the values in <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> or <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> are perturbed even
slightly, we might expect significant changes in the
computed regression coefficients.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>(**) The least squares regression problem can be solved by means
of the singular value decomposition of the model matrix,
see <a class="reference internal" href="#sec-svd"><span class="std std-numref">Section 9.3.4</span></a>.
Let <span class="math notranslate nohighlight">\(\mathbf{U}\mathbf{S}\mathbf{Q}\)</span> be the SVD of
<span class="math notranslate nohighlight">\(\mathbf{X}^T\)</span>.
Then <span class="math notranslate nohighlight">\(\mathbf{c} = \mathbf{U} \mathbf{S}^{-1} \mathbf{Q} \mathbf{y}\)</span>,
with <span class="math notranslate nohighlight">\(\mathbf{S}^{-1}=\mathrm{diag}(1/s_{1,1},\dots,1/s_{m,m})\)</span>.
As <span class="math notranslate nohighlight">\(s_{1,1}\ge\dots\ge s_{m,m}\)</span> gives the singular
values of <span class="math notranslate nohighlight">\(\mathbf{X}^T\)</span>, the aforementioned
condition number can simply be computed as <span class="math notranslate nohighlight">\(s_{1,1}/s_{m,m}\)</span>.</p>
</div>
<div style="margin-top: 1em"></div><p>Let us verify the method used by <strong class="program">scikit-learn</strong>.
As it fits the intercept separately, we expect it to be slightly
better-behaving. Nevertheless, let us keep in mind that it is merely a
wrapper around <strong class="command">scipy.linalg.lstsq</strong> with a different API.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sklearn.linear_model</span>
<span class="n">lm</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">linear_model</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">(</span><span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">lm</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">cs</span><span class="p">[</span><span class="s2">&quot;sklearn&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">lm</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">coef_</span><span class="p">]</span>
<span class="n">cs</span><span class="p">[</span><span class="s2">&quot;sklearn&quot;</span><span class="p">]</span>
<span class="c1">## array([ 6.92257708e+01,  5.05752755e-13,  1.38835643e-08,</span>
<span class="c1">##        -2.18869346e-13,  9.09347772e-19])</span>
</pre></div>
</div>
<p>Here is the condition number of the underlying model matrix:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lm</span><span class="o">.</span><span class="n">singular_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">lm</span><span class="o">.</span><span class="n">singular_</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="c1">## 1.4026032298428496e+16</span>
</pre></div>
</div>
<p>The condition number is also enormous.
Still, <strong class="program">scikit-learn</strong> did not warn us about this being the case
(insert frowning face emoji here).
Had we trusted the solution returned by it, we would end up
with conclusions from our data analysis built on sand.
As we said in <a class="reference internal" href="#sec-sklearnlm"><span class="std std-numref">Section 9.2.8</span></a>, the package designers assumed
that the users know what they are doing. This is okay, we are all
adults here, although some of us are still learning.</p>
<div style="margin-top: 1em"></div><p>Overall, if the model matrix is close to singular, the computation of its
inverse is prone to enormous numerical errors.
One way of dealing with this is to remove highly correlated variables
(the multicollinearity problem). Interestingly, standardisation can
<em>sometimes</em> make the fitting more numerically stable.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> be a standardised version of the model matrix
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span> with the intercept part (the column of <span class="math notranslate nohighlight">\(1\)</span>s) not included,
i.e., with <span class="math notranslate nohighlight">\(\mathbf{z}_{\cdot,j} = (\mathbf{x}_{\cdot,j}-\bar{x}_j)/s_j\)</span>
where <span class="math notranslate nohighlight">\(\bar{x}_j\)</span> and <span class="math notranslate nohighlight">\(s_j\)</span> denotes the arithmetic mean
and standard deviation of the <span class="math notranslate nohighlight">\(j\)</span>-th column in <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.
If <span class="math notranslate nohighlight">\((d_1,\dots,d_{m-1})\)</span> is the least squares solution
for <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>, then the least squares solution to the underlying
original regression problem is:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\boldsymbol{c}=\left(
    \bar{y}-\sum_{j=1}^{m-1} \frac{d_j}{s_j} \bar{x}_j,
    \frac{d_1}{s_1},
    \frac{d_2}{s_2},
    \dots,
    \frac{d_{m-1}}{s_{m-1}}
\right),
\]</div>
</div>
<p>with the first term corresponding to the intercept.</p>
<p>Let us test this approach with <strong class="command">scipy.linalg.lstsq</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">stds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">Z_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">-</span><span class="n">means</span><span class="p">)</span><span class="o">/</span><span class="n">stds</span>
<span class="n">resZ</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">Z_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">c_scipyZ</span> <span class="o">=</span> <span class="n">resZ</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">stds</span>
<span class="n">cs</span><span class="p">[</span><span class="s2">&quot;scipy_Z&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">c_scipyZ</span> <span class="o">@</span> <span class="n">means</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">c_scipyZ</span><span class="p">]</span>
<span class="n">cs</span><span class="p">[</span><span class="s2">&quot;scipy_Z&quot;</span><span class="p">]</span>
<span class="c1">## array([ 6.35946784e+01,  1.04541932e-03, -2.41992445e-08,</span>
<span class="c1">##         2.39133533e-13, -8.13307828e-19])</span>
</pre></div>
</div>
<p>The condition number is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="n">resZ</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">s</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="c1">## 139.42792257372338</span>
</pre></div>
</div>
<p>This is still far from perfect (we would prefer a value close to 1)
but nevertheless way better.</p>
<div style="margin-top: 1em"></div><p><a class="reference internal" href="#fig-ill-cond"><span class="std std-numref">Figure 9.15</span></a> depicts the three fitted models,
each claiming to be <em>the</em> solution to the
original regression problem.
Note that, luckily, we know that in our case the logarithmic model
is better than the polynomial one.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_original</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_original</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">x_original</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">101</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">_X</span> <span class="o">=</span> <span class="n">_x</span><span class="o">**</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="k">for</span> <span class="n">lab</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">cs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">ssr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_train</span> <span class="o">-</span> <span class="n">c</span> <span class="o">@</span> <span class="n">X_train</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">_x</span><span class="p">,</span> <span class="n">c</span> <span class="o">@</span> <span class="n">_X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">lab</span><span class="si">:</span><span class="s2">10</span><span class="si">}</span><span class="s2"> SSR=</span><span class="si">{</span><span class="n">ssr</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;per capita GDP PPP&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;life expectancy (years)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id51">
<span id="fig-ill-cond"></span><img alt="../_images/ill-cond-29.png" src="../_images/ill-cond-29.png" />
<figcaption>
<p><span class="caption-number">Figure 9.15 </span><span class="caption-text">Ill-conditioned model matrix can give a very wrong model.</span><a class="headerlink" href="#id51" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Always check the model matrix’s condition number.</p>
</div>
<div class="proof proof-type-exercise" id="id52">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.14</span>
        
    </div><div class="proof-content">
<p>Check the condition numbers of all the models fitted so far in this chapter
via the least squares method.</p>
</div></div><p>To be strict, if we read a paper in, say, social or medical sciences
(amongst others) where the researchers fit a regression model but
do not provide the model matrix’s condition number,
it is worthwhile to doubt the conclusions they make.</p>
<p>On a final note, we might wonder why the standardisation
is not done automatically by the least squares solver.
As usual with most numerical methods, there is no one-fits-all solution:
e.g., when there are columns of extremely small variance
or there are outliers in data.
This is why we need to study all the topics deeply:
to be able to respond flexibly
to many different scenarios ourselves.</p>
</section>
</section>
<section id="finding-interesting-combinations-of-variables">
<span id="sec-pca"></span><h2><span class="section-number">9.3. </span>Finding interesting combinations of variables (*)<a class="headerlink" href="#finding-interesting-combinations-of-variables" title="Permalink to this heading">#</a></h2>
<section id="dot-products-angles-collinearity-and-orthogonality">
<span id="sec-dot-cosine"></span><h3><span class="section-number">9.3.1. </span>Dot products, angles, collinearity, and orthogonality<a class="headerlink" href="#dot-products-angles-collinearity-and-orthogonality" title="Permalink to this heading">#</a></h3>
<p>It turns out that the dot product (<a class="reference internal" href="320-transform-matrix.html#sec-matrix-multiply"><span class="std std-numref">Section 8.3</span></a>)
has a nice geometrical interpretation:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\boldsymbol{x}\cdot\boldsymbol{y} = \|\boldsymbol{x}\|\, \|\boldsymbol{y}\|\,
\cos\alpha,
\]</div>
</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is the angle between two given vectors
<span class="math notranslate nohighlight">\(\boldsymbol{x},\boldsymbol{y}\in\mathbb{R}^n\)</span>.
In plain English, it is the product of the magnitudes of the two vectors
and the cosine of the angle between them.</p>
<p>We can obtain the cosine part by computing the dot product
of the <em>normalised</em> vectors, i.e., such that their magnitudes are equal to 1:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\cos\alpha = \frac{\boldsymbol{x}}{\|\boldsymbol{x}\|}\cdot\frac{\boldsymbol{y}}{\|\boldsymbol{y}\|}.
\]</div>
</div>
<p>For example, consider two vectors in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>,
<span class="math notranslate nohighlight">\(\boldsymbol{u}=(1/2, 0)\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{v}=(\sqrt{2}/2, \sqrt{2}/2)\)</span>,
which are depicted in <a class="reference internal" href="#fig-two-vectors-angle"><span class="std std-numref">Figure 9.16</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
<p>Their dot product is equal to:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">u</span><span class="o">*</span><span class="n">v</span><span class="p">)</span>
<span class="c1">## 0.3535533905932738</span>
</pre></div>
</div>
<p>The dot product of their normalised versions, i.e., the cosine
of the angle between them is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">u_norm</span> <span class="o">=</span> <span class="n">u</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">u</span><span class="o">*</span><span class="n">u</span><span class="p">))</span>
<span class="n">v_norm</span> <span class="o">=</span> <span class="n">v</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">v</span><span class="o">*</span><span class="n">v</span><span class="p">))</span>  <span class="c1"># BTW: this vector is already normalised</span>
<span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">u_norm</span><span class="o">*</span><span class="n">v_norm</span><span class="p">)</span>
<span class="c1">## 0.7071067811865476</span>
</pre></div>
</div>
<p>The angle itself can be determined by referring to the
inverse of the cosine function, i.e., arccosine.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">arccos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">u_norm</span><span class="o">*</span><span class="n">v_norm</span><span class="p">))</span> <span class="o">*</span> <span class="mi">180</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span>
<span class="c1">## 45.0</span>
</pre></div>
</div>
<p>Notice that we converted the angle from radians to degrees.</p>
<figure class="align-default" id="id53">
<span id="fig-two-vectors-angle"></span><img alt="../_images/two-vectors-angle-31.png" src="../_images/two-vectors-angle-31.png" />
<figcaption>
<p><span class="caption-number">Figure 9.16 </span><span class="caption-text">Example vectors and the angle between them.</span><a class="headerlink" href="#id53" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>If two vectors are collinear
(<em>codirectional</em>, one is a scaled version of another, angle <span class="math notranslate nohighlight">\(0\)</span>),
then <span class="math notranslate nohighlight">\(\cos 0 = 1\)</span>. If they point in opposite directions
(<span class="math notranslate nohighlight">\(\pm\pi=\pm 180^\circ\)</span> angle), then <span class="math notranslate nohighlight">\(\cos \pm\pi=-1\)</span>.
For vectors that are <em>orthogonal</em>
(perpendicular, <span class="math notranslate nohighlight">\(\pm\frac{\pi}{2}=\pm 90^circ\)</span> angle),
we get <span class="math notranslate nohighlight">\(\cos\pm\frac{\pi}{2}=0\)</span>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>(**)
The standard deviation <span class="math notranslate nohighlight">\(s\)</span> of a vector <span class="math notranslate nohighlight">\(\boldsymbol{x}\in\mathbb{R}^n\)</span>
that has already been centred (whose components’ mean is 0)
is a scaled version of its magnitude, i.e., <span class="math notranslate nohighlight">\(s = \|\boldsymbol{x}\|/\sqrt{n}\)</span>.
Looking at the definition of the Pearson linear correlation coefficient
(<a class="reference internal" href="#sec-pearson"><span class="std std-numref">Section 9.1.1</span></a>),
we see that it is the dot product of the standardised versions
of two vectors <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> divided by
the number of elements therein. If the vectors are
centred, we can rewrite the formula equivalently as
<span class="math notranslate nohighlight">\(r(\boldsymbol{x}, \boldsymbol{y})=
\frac{\boldsymbol{x}}{\|\boldsymbol{x}\|}\cdot\frac{\boldsymbol{y}}{\|\boldsymbol{y}\|}\)</span> and thus <span class="math notranslate nohighlight">\(r(\boldsymbol{x}, \boldsymbol{y})= \cos\alpha\)</span>.
It is not easy to imagine vectors in high-dimensional spaces,
but from this observation we can at least imply the fact
that <span class="math notranslate nohighlight">\(r\)</span> is bounded between -1 and 1.
In this context, being not linearly correlated corresponds to the vectors’
orthogonality.</p>
</div>
</section>
<section id="geometric-transformations-of-points">
<span id="sec-geometric-transform"></span><h3><span class="section-number">9.3.2. </span>Geometric transformations of points<a class="headerlink" href="#geometric-transformations-of-points" title="Permalink to this heading">#</a></h3>
<p>For certain square matrices of size <span class="math notranslate nohighlight">\(m\times m\)</span>,
matrix multiplication can be thought of as an application
of the corresponding geometrical transformation of points in <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span></p>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> be a matrix of shape <span class="math notranslate nohighlight">\(n\times m\)</span>,
which we treat as representing the coordinates of
<span class="math notranslate nohighlight">\(n\)</span> points in an <span class="math notranslate nohighlight">\(m\)</span>-dimensional space.
For instance, if we are given a diagonal matrix:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{S}=\mathrm{diag}(s_1, s_2,\dots, s_m)=
\left[
\begin{array}{cccc}
s_1 &amp; 0 &amp; \dots &amp; 0 \\
0 &amp; s_2 &amp; \dots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; s_m \\
\end{array}
\right],
\end{split}\]</div>
</div>
<p>then <span class="math notranslate nohighlight">\(\mathbf{X} \mathbf{S}\)</span> represents <em>scaling</em> (stretching) with respect
to the individual axes of the coordinate system because:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X} \mathbf{S} =
\left[
\begin{array}{cccc}
s_1 x_{1,1} &amp; s_2 x_{1,2} &amp; \dots &amp; s_m x_{1,m} \\
s_1 x_{2,1} &amp; s_2 x_{2,2} &amp; \dots &amp; s_m x_{2,m} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
s_1 x_{n-1,1} &amp; s_2 x_{n-1,2} &amp; \dots &amp; s_m x_{n-1,m} \\
s_1 x_{n,1} &amp; s_2 x_{n,2} &amp; \dots &amp; s_m x_{n,m} \\
\end{array}
\right].
\end{split}\]</div>
</div>
<p>The above can be expressed in <strong class="program">numpy</strong> without
referring to the matrix multiplication.
A notation like <code class="docutils literal notranslate"><span class="pre">X</span> <span class="pre">*</span> <span class="pre">np.array([s1,</span> <span class="pre">s2,</span> <span class="pre">...,</span> <span class="pre">sm]).reshape(1,</span> <span class="pre">-1)</span></code>
will suffice (elementwise multiplication and proper shape broadcasting).</p>
<div style="margin-top: 1em"></div><p>Furthermore, let <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> is an <em>orthonormal</em><a class="footnote-reference brackets" href="#footortho" id="id16" role="doc-noteref"><span class="fn-bracket">[</span>12<span class="fn-bracket">]</span></a> matrix, i.e.,
a square matrix whose columns and rows
are unit vectors (normalised), all orthogonal to each other:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\| \mathbf{q}_{i,\cdot} \| = 1\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{q}_{i,\cdot}\cdot\mathbf{q}_{k,\cdot} = 0\)</span> for all <span class="math notranslate nohighlight">\(i,k\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\| \mathbf{q}_{\cdot,j} \| = 1\)</span> for all <span class="math notranslate nohighlight">\(j\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{q}_{\cdot,j}\cdot\mathbf{q}_{\cdot,k} = 0\)</span> for all <span class="math notranslate nohighlight">\(j,k\)</span>.</p></li>
</ul>
<p>In such a case, <span class="math notranslate nohighlight">\(\mathbf{X}\mathbf{Q}\)</span> represents a combination
of rotations and reflections.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>By definition, a matrix <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span>
is <em>orthonormal</em> if and only if
<span class="math notranslate nohighlight">\(\mathbf{Q}^T \mathbf{Q} = \mathbf{Q} \mathbf{Q}^T = \mathbf{I}\)</span>.
It is due to the <span class="math notranslate nohighlight">\(\cos\pm\frac{\pi}{2}=0\)</span> interpretation
of the dot products of normalised orthogonal vectors.</p>
</div>
<p>In particular, the matrix representing the rotation in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>
about the origin <span class="math notranslate nohighlight">\((0, 0)\)</span> by the counterclockwise angle <span class="math notranslate nohighlight">\(\alpha\)</span>:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{R}(\alpha) = \left[
\begin{array}{cc}
 \cos \alpha &amp; \sin\alpha \\
-\sin \alpha &amp; \cos\alpha \\
\end{array}
\right],
\end{split}\]</div>
</div>
<p>is orthonormal (which can be easily verified using the basic
trigonometric equalities).
Furthermore:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\left[
\begin{array}{cc}
 1 &amp; 0 \\
 0 &amp; -1 \\
\end{array}
\right]
\quad\text{ and }\quad
\left[
\begin{array}{cc}
 -1 &amp; 0 \\
 0 &amp; 1 \\
\end{array}
\right],
\end{split}\]</div>
</div>
<p>represent the two reflections, one
against the x- and the other against the y-axis, respectively.
Both are orthonormal matrices too.</p>
<div style="margin-top: 1em"></div><p>Consider a dataset <span class="math notranslate nohighlight">\(\mathbf{X}'\)</span> in <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">12345</span><span class="p">)</span>
<span class="n">Xp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.25</span>
</pre></div>
</div>
<p>and its scaled, rotated, and translated (shifted) version:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X} =
\mathbf{X}'\ \left[
\begin{array}{cc}
 2 &amp; 0 \\
0 &amp; 0.5 \\
\end{array}
\right]\ \left[
\begin{array}{cc}
 \cos \frac{\pi}{6} &amp; \sin\frac{\pi}{6} \\
-\sin \frac{\pi}{6} &amp; \cos\frac{\pi}{6} \\
\end{array}
\right] + \left[
\begin{array}{cc}
3 &amp; 2 \\
\end{array}
\right].
\end{split}\]</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="n">S</span>
<span class="c1">## array([[2. , 0. ],</span>
<span class="c1">##        [0. , 0.5]])</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">/</span><span class="mi">6</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">alpha</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">alpha</span><span class="p">)],</span>
    <span class="p">[</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">alpha</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">alpha</span><span class="p">)]</span>
<span class="p">])</span>
<span class="n">Q</span>
<span class="c1">## array([[ 0.8660254,  0.5      ],</span>
<span class="c1">##        [-0.5      ,  0.8660254]])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">Xp</span> <span class="o">@</span> <span class="n">S</span> <span class="o">@</span> <span class="n">Q</span> <span class="o">+</span> <span class="n">t</span>
</pre></div>
</div>
<figure class="align-default" id="id54">
<span id="fig-geom-trans"></span><img alt="../_images/geom-trans-33.png" src="../_images/geom-trans-33.png" />
<figcaption>
<p><span class="caption-number">Figure 9.17 </span><span class="caption-text">A dataset and its scaled, rotated, and shifted version.</span><a class="headerlink" href="#id54" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>We can consider <span class="math notranslate nohighlight">\(\mathbf{X}=\mathbf{X}' \mathbf{S} \mathbf{Q} + \mathbf{t}\)</span>
a version of <span class="math notranslate nohighlight">\(\mathbf{X}'\)</span> in a new coordinate system (basis),
see <a class="reference internal" href="#fig-geom-trans"><span class="std std-numref">Figure 9.17</span></a>.
Each column in the transformed matrix is a
shifted linear combination of the columns in the original matrix:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\mathbf{x}_{\cdot,j} = t_j + \sum_{k=1}^m (s_{k, k} q_{k, j})
\mathbf{x}_{\cdot, k}'.
\]</div>
</div>
<p>The computing of such linear combinations of columns
is not rare during a dataset’s preprocessing step,
especially if they are on the same scale or are unitless.
As a matter of fact,
the standardisation itself is a form of scaling and translation.</p>
<div class="proof proof-type-exercise" id="id55">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.15</span>
        
    </div><div class="proof-content">
<p>Assume that we have a dataset with two columns representing the number
of apples and the number of oranges in clients’ baskets.
What orthonormal and scaling transforms should be
applied to obtain a matrix that gives the total number of fruits and
<em>surplus</em> apples (e.g., to convert a row <span class="math notranslate nohighlight">\((4, 7)\)</span> to <span class="math notranslate nohighlight">\((11, -3)\)</span>)?</p>
</div></div></section>
<section id="matrix-inverse">
<span id="sec-matrix-inverse"></span><h3><span class="section-number">9.3.3. </span>Matrix inverse<a class="headerlink" href="#matrix-inverse" title="Permalink to this heading">#</a></h3>
<p>The <em>inverse</em> of a square matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>
(if it exists) is denoted with <span class="math notranslate nohighlight">\(\mathbf{A}^{-1}\)</span> – it is the matrix
fulfilling the identity:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\mathbf{A}^{-1} \mathbf{A} = \mathbf{A} \mathbf{A}^{-1} = \mathbf{I}.
\]</div>
</div>
<p>Noting that the identity matrix <span class="math notranslate nohighlight">\(\mathbf{I}\)</span> is the neutral element
of the matrix multiplication, the above is thus the analogue
of the inverse of a scalar: something like
<span class="math notranslate nohighlight">\(3 \cdot 3^{-1} = 3\cdot \frac{1}{3} = \frac{1}{3} \cdot 3 = 1\)</span>.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>For any invertible matrices of admissible shapes,
it might be shown that the following noteworthy properties hold:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((\mathbf{A}^{-1})^T = (\mathbf{A}^T)^{-1}\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\((\mathbf{A}\mathbf{B})^{-1} = \mathbf{B}^{-1} \mathbf{A}^{-1}\)</span>,</p></li>
<li><p>a matrix equality <span class="math notranslate nohighlight">\(\mathbf{A}=\mathbf{B}\mathbf{C}\)</span> holds if and only if
<span class="math notranslate nohighlight">\(\mathbf{A} \mathbf{C}^{-1}=\mathbf{B}\mathbf{C}\mathbf{C}^{-1}=\mathbf{B}\)</span>;
this is also equivalent to
<span class="math notranslate nohighlight">\(\mathbf{B}^{-1} \mathbf{A}=\mathbf{B}^{-1} \mathbf{B}\mathbf{C}=\mathbf{C}\)</span>.</p></li>
</ul>
</div>
<p>Matrix inverse  to identify the inverses of geometrical
transformations.
Knowing that <span class="math notranslate nohighlight">\(\mathbf{X} = \mathbf{X}'\mathbf{S}\mathbf{Q}+\mathbf{t}\)</span>,
we can recreate the original matrix by applying:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\mathbf{X}' = (\mathbf{X}-\mathbf{t}) (\mathbf{S}\mathbf{Q})^{-1}
= (\mathbf{X}-\mathbf{t}) \mathbf{Q}^{-1} \mathbf{S}^{-1}.
\]</div>
</div>
<p>It is worth knowing that if <span class="math notranslate nohighlight">\(\mathbf{S}=\mathrm{diag}(s_1,s_2,\dots,s_m)\)</span>
is a diagonal matrix, then its inverse is
<span class="math notranslate nohighlight">\(\mathbf{S}^{-1} = \mathrm{diag}(1/s_1,1/s_2,\dots,1/s_m)\)</span>,
which we can denote as <span class="math notranslate nohighlight">\((1/\mathbf{S})\)</span>.
In addition, the inverse of an orthonormal matrix <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span>
is always equal to its transpose,
<span class="math notranslate nohighlight">\(\mathbf{Q}^{-1}=\mathbf{Q}^T\)</span>.
Luckily, we will not be inverting other matrices in this introductory course.</p>
<p>As a consequence:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\mathbf{X}' = (\mathbf{X}-\mathbf{t}) \mathbf{Q}^{T} (1/\mathbf{S}).
\]</div>
</div>
<p>Let us verify this numerically (testing equality up to some inherent
round-off error):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">Xp</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="o">-</span><span class="n">t</span><span class="p">)</span> <span class="o">@</span> <span class="n">Q</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S</span><span class="p">)))</span>
<span class="c1">## True</span>
</pre></div>
</div>
</section>
<section id="singular-value-decomposition">
<span id="sec-svd"></span><h3><span class="section-number">9.3.4. </span>Singular value decomposition<a class="headerlink" href="#singular-value-decomposition" title="Permalink to this heading">#</a></h3>
<p>It turns out that given any real <span class="math notranslate nohighlight">\(n\times m\)</span> matrix <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>
with <span class="math notranslate nohighlight">\(n\ge m\)</span>, we can find an interesting scaling and orthonormal
transform that, when applied on a dataset whose columns are already
normalised, yields exactly <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
<p>Namely, the singular value decomposition (SVD in the so-called compact form)
is a factorisation:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[\mathbf{X} = \mathbf{U}\mathbf{S}\mathbf{Q},\]</div>
</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{U}\)</span> is an <span class="math notranslate nohighlight">\(n\times m\)</span> semi-orthonormal matrix
(its columns are orthonormal vectors;
it holds <span class="math notranslate nohighlight">\(\mathbf{U}^T \mathbf{U} = \mathbf{I}\)</span>),</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{S}\)</span> is an <span class="math notranslate nohighlight">\(m\times m\)</span> diagonal matrix such
that <span class="math notranslate nohighlight">\(s_{1,1}\ge s_{2,2}\ge\dots\ge s_{m,m}\ge 0\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> is an <span class="math notranslate nohighlight">\(m\times m\)</span> orthonormal matrix.</p></li>
</ul>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>In data analysis, we usually apply the SVD on matrices that
have already been centred (so that their column means are all 0).</p>
</div>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.linalg</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">X_centred</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X_centred</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>And now:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">U</span><span class="p">[:</span><span class="mi">6</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># preview first few rows</span>
<span class="c1">## array([[-0.00195072,  0.00474569],</span>
<span class="c1">##        [-0.00510625, -0.00563582],</span>
<span class="c1">##        [ 0.01986719,  0.01419324],</span>
<span class="c1">##        [ 0.00104386,  0.00281853],</span>
<span class="c1">##        [ 0.00783406,  0.01255288],</span>
<span class="c1">##        [ 0.01025205, -0.0128136 ]])</span>
</pre></div>
</div>
<p>The norms of all the columns in <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> are all equal to 1
(and hence standard deviations are <span class="math notranslate nohighlight">\(1/\sqrt{n}\)</span>).
Consequently, they are on the same scale:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>  <span class="c1"># compare</span>
<span class="c1">## (array([0.01, 0.01]), 0.01)</span>
</pre></div>
</div>
<p>What is more, they are orthogonal: their dot products
are all equal to 0. Regarding what we said about Pearson’s
linear correlation coefficient and its relation to dot products of normalised vectors, we imply that the columns in <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> are not
linearly correlated. In some sense, they form <em>independent</em> dimensions.</p>
<p>Now, it holds <span class="math notranslate nohighlight">\(\mathbf{S} = \mathrm{diag}(s_1,\dots,s_m)\)</span>,
with the elements on the diagonal being:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">s</span>
<span class="c1">## array([49.72180455, 12.5126241 ])</span>
</pre></div>
</div>
<p>The elements on the main diagonal of <span class="math notranslate nohighlight">\(\mathbf{S}\)</span>
are used to scale the corresponding columns in <span class="math notranslate nohighlight">\(\mathbf{U}\)</span>.
The fact that they are ordered decreasingly
means that the first column in <span class="math notranslate nohighlight">\(\mathbf{U}\mathbf{S}\)</span>
has the greatest standard deviation,
the second column has the second greatest variability,
and so forth.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="n">US</span> <span class="o">=</span> <span class="n">U</span> <span class="o">@</span> <span class="n">S</span>
<span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">US</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># equal to s/np.sqrt(n)</span>
<span class="c1">## array([0.49721805, 0.12512624])</span>
</pre></div>
</div>
<p>Multiplying <span class="math notranslate nohighlight">\(\mathbf{U}\mathbf{S}\)</span> by <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> simply
rotates and/or reflects the dataset.
This brings  <span class="math notranslate nohighlight">\(\mathbf{U}\mathbf{S}\)</span> to a new coordinate system
where, by construction, the dataset projected onto the direction
determined by the first row in <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span>, i.e., <span class="math notranslate nohighlight">\(\mathbf{q}_{1,\cdot}\)</span>
has the largest variance,
projection onto <span class="math notranslate nohighlight">\(\mathbf{q}_{2,\cdot}\)</span> has the second largest variance,
and so on.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Q</span>
<span class="c1">## array([[ 0.86781968,  0.49687926],</span>
<span class="c1">##        [-0.49687926,  0.86781968]])</span>
</pre></div>
</div>
<p>This is why we refer to the rows in <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span>
as <em>principal directions</em> (or <em>components</em>).
Their scaled versions (proportional
to the standard deviations along them) are depicted in
<a class="reference internal" href="#fig-principal-directions"><span class="std std-numref">Figure 9.18</span></a>.
Note that we have more or less recreated the steps needed
to construct <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> from <span class="math notranslate nohighlight">\(\mathbf{X}'\)</span> above
(by the way we generated <span class="math notranslate nohighlight">\(\mathbf{X}'\)</span>, we expect it to
have linearly uncorrelated columns;
yet, <span class="math notranslate nohighlight">\(\mathbf{X}'\)</span> and <span class="math notranslate nohighlight">\(\mathbf{U}\)</span> have different
column variances).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_centred</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_centred</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span>
    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Q</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">Q</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
    <span class="n">facecolor</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">,</span> <span class="n">length_includes_head</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span>
    <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Q</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">Q</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">),</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.02</span><span class="p">,</span>
    <span class="n">facecolor</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;white&quot;</span><span class="p">,</span> <span class="n">length_includes_head</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id56">
<span id="fig-principal-directions"></span><img alt="../_images/principal-directions-35.png" src="../_images/principal-directions-35.png" />
<figcaption>
<p><span class="caption-number">Figure 9.18 </span><span class="caption-text">Principal directions of an example dataset (scaled so that they are proportional to the standard deviations along them).</span><a class="headerlink" href="#id56" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="dimensionality-reduction-with-svd">
<h3><span class="section-number">9.3.5. </span>Dimensionality reduction with SVD<a class="headerlink" href="#dimensionality-reduction-with-svd" title="Permalink to this heading">#</a></h3>
<p>Let us consider the following example three-dimensional dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">chainlink</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching-data/master/clustering/fcps_chainlink.csv&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><a class="reference internal" href="310-matrix.html#sec-visual-multidim"><span class="std std-numref">Section 7.4</span></a> said that
the plotting is always done on a two-dimensional surface (be it the computer
screen or book page). We can look at the dataset
only from one <em>angle</em> at a time.</p>
<p>In particular, a scatter plot matrix only depicts the dataset
from the perspective of the axes of the Cartesian coordinate system
(standard basis); see <a class="reference internal" href="#fig-chainlink-pairplot"><span class="std std-numref">Figure 9.19</span></a>
(we used a function we defined in <a class="reference internal" href="310-matrix.html#sec-pairplot"><span class="std std-numref">Section 7.4.3</span></a>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pairplot</span><span class="p">(</span><span class="n">chainlink</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;axis1&quot;</span><span class="p">,</span> <span class="s2">&quot;axis2&quot;</span><span class="p">,</span> <span class="s2">&quot;axis3&quot;</span><span class="p">])</span>  <span class="c1"># our function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id57">
<span id="fig-chainlink-pairplot"></span><img alt="../_images/chainlink-pairplot-37.png" src="../_images/chainlink-pairplot-37.png" />
<figcaption>
<p><span class="caption-number">Figure 9.19 </span><span class="caption-text">Views from the perspective of the main axes.</span><a class="headerlink" href="#id57" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>These viewpoints by no means must reveal the true geometric
structure of the dataset. However,
we know that we can rotate the virtual
camera and find some more <em>interesting</em> angle.
It turns out that our dataset represents two nonintersecting rings,
hopefully visible <a class="reference internal" href="#fig-chainlink-rotated"><span class="std std-numref">Figure 9.20</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s2">&quot;3d&quot;</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s2">&quot;#ffffff00&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">chainlink</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">chainlink</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">chainlink</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">azim</span><span class="o">=</span><span class="mi">45</span><span class="p">,</span> <span class="n">vertical_axis</span><span class="o">=</span><span class="s2">&quot;z&quot;</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s2">&quot;3d&quot;</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s2">&quot;#ffffff00&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">chainlink</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">chainlink</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">chainlink</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mi">37</span><span class="p">,</span> <span class="n">azim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vertical_axis</span><span class="o">=</span><span class="s2">&quot;z&quot;</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s2">&quot;3d&quot;</span><span class="p">,</span> <span class="n">facecolor</span><span class="o">=</span><span class="s2">&quot;#ffffff00&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">chainlink</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">chainlink</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">chainlink</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">view_init</span><span class="p">(</span><span class="n">elev</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">azim</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">vertical_axis</span><span class="o">=</span><span class="s2">&quot;z&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id58">
<span id="fig-chainlink-rotated"></span><img alt="../_images/chainlink-rotated-39.png" src="../_images/chainlink-rotated-39.png" />
<figcaption>
<p><span class="caption-number">Figure 9.20 </span><span class="caption-text">Different views of the same dataset.</span><a class="headerlink" href="#id58" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>It turns out that we may find a noteworthy viewpoint using the SVD.
Namely, we can perform the decomposition
of a centred dataset which we denote with <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\mathbf{X} = \mathbf{U}\mathbf{S}\mathbf{Q}.
\]</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.linalg</span>
<span class="n">X_centered</span> <span class="o">=</span> <span class="n">chainlink</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">chainlink</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X_centered</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Then, considering its rotated/reflected version:</p>
<div class="math-wrapper docutils container">
<div class="math notranslate nohighlight">
\[
\mathbf{P}=\mathbf{X} \mathbf{Q}^{-1} = \mathbf{U}\mathbf{S},
\]</div>
</div>
<p>we know that its first column has the highest variance,
the second column has the second highest variability,
and so on.
It might indeed be worth looking at that dataset from that
<em>most informative</em> perspective.</p>
<p><a class="reference internal" href="#fig-chainlink-svd"><span class="std std-numref">Figure 9.21</span></a> gives the scatter plot for <span class="math notranslate nohighlight">\(\mathbf{p}_{\cdot,1}\)</span>
and <span class="math notranslate nohighlight">\(\mathbf{p}_{\cdot,2}\)</span>. Maybe this does not reveal the true
geometric structure of the dataset (no single two-dimensional projection
can do that), but at least it is better than the initial
ones (from the pairs plot).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">P2</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># the same as (U@np.diag(s))[:, :2]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">P2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">P2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id59">
<span id="fig-chainlink-svd"></span><img alt="../_images/chainlink-svd-41.png" src="../_images/chainlink-svd-41.png" />
<figcaption>
<p><span class="caption-number">Figure 9.21 </span><span class="caption-text">The view from the two principal axes.</span><a class="headerlink" href="#id59" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>What we just did is a kind of <em>dimensionality reduction</em>.
We found a viewpoint (in the form of an orthonormal matrix, being
a mixture of rotations and reflections) on <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>
such that its orthonormal projection onto the first two axes of
the Cartesian coordinate system is the most informative<a class="footnote-reference brackets" href="#footrankkapprox" id="id17" role="doc-noteref"><span class="fn-bracket">[</span>13<span class="fn-bracket">]</span></a>
(in terms of having the highest variance along these axes).</p>
</section>
<section id="principal-component-analysis">
<h3><span class="section-number">9.3.6. </span>Principal component analysis<a class="headerlink" href="#principal-component-analysis" title="Permalink to this heading">#</a></h3>
<p><em>Principal component analysis</em> (PCA) is a fancy name for the entire process
involving our brainstorming upon what happens along the projections onto
the most variable dimensions.
It can be used not only for data visualisation and deduplication,
but also for feature engineering (as it creates new columns
that are linear combinations of existing ones).</p>
<p>Let us consider a few chosen countrywise 2016
<a class="reference external" href="https://ssi.wi.th-koeln.de/">Sustainable Society Indices</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ssi</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching-data/master/marek/ssi_2016_indicators.csv&quot;</span><span class="p">,</span>
    <span class="n">comment</span><span class="o">=</span><span class="s2">&quot;#&quot;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ssi</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">19</span><span class="p">]</span> <span class="p">])</span>  <span class="c1"># select columns, make matrix</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">X</span><span class="p">[:</span><span class="mi">6</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># preview</span>
<span class="c1">## array([[ 9.32      ,  8.13333333,  8.386     ,  8.5757    ,  5.46249573],</span>
<span class="c1">##        [ 8.74      ,  7.71666667,  7.346     ,  6.8426    ,  6.2929302 ],</span>
<span class="c1">##        [ 5.11      ,  4.31666667,  8.788     ,  9.2035    ,  3.91062849],</span>
<span class="c1">##        [ 9.61      ,  7.93333333,  5.97      ,  5.5232    ,  7.75361284],</span>
<span class="c1">##        [ 8.95      ,  7.81666667,  8.032     ,  8.2639    ,  4.42350654],</span>
<span class="c1">##        [10.        ,  8.65      ,  1.        ,  1.        ,  9.66401848]])</span>
</pre></div>
</div>
<p>Each index is on the scale from 0 to 10. These are, in this order:</p>
<ol class="arabic simple">
<li><p>Safe Sanitation,</p></li>
<li><p>Healthy Life,</p></li>
<li><p>Energy Use,</p></li>
<li><p>Greenhouse Gases,</p></li>
<li><p>Gross Domestic Product.</p></li>
</ol>
<p>Above we displayed the data corresponding to the
6 following countries:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">countries</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">ssi</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>  <span class="c1"># select the 1st column from the data frame</span>
<span class="n">countries</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>  <span class="c1"># preview</span>
<span class="c1">## [&#39;Albania&#39;, &#39;Algeria&#39;, &#39;Angola&#39;, &#39;Argentina&#39;, &#39;Armenia&#39;, &#39;Australia&#39;]</span>
</pre></div>
</div>
<p>This is a five-dimensional dataset. We cannot easily visualise it.
Observing that the pairs plot does not reveal too much is left as an exercise.
Let us thus perform the SVD decomposition of a standardised
version of this dataset, <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>
(recall that the centring is necessary, at the very least).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Z</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>The standard deviations of the data projected onto
the consecutive principal components (columns in <span class="math notranslate nohighlight">\(\mathbf{U}\mathbf{S}\)</span>) are:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">s</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="c1">## array([2.02953531, 0.7529221 , 0.3943008 , 0.31897889, 0.23848286])</span>
</pre></div>
</div>
<p>It is customary to check the ratios of the cumulative variances
explained by the consecutive principal components, which
is a normalised measure of their importances.
We can compute them by calling:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">s</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">s</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="c1">## array([0.82380272, 0.93718105, 0.96827568, 0.98862519, 1.        ])</span>
</pre></div>
</div>
<p>As in some sense the variability within the first two components covers
c. 94% of the variability of the whole dataset,
we can restrict ourselves only to a two-dimensional projection
of this dataset (actually, we are quite lucky here – or someone
has selected these countrywise indices for us in a very
clever fashion).</p>
<p>The rows in <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> define the <em>loadings</em>, which
give the coefficients defining the linear combinations of the
rows in <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> that correspond to the principal components.</p>
<p>Let us try to interpret them.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># loadings – the first principal axis</span>
<span class="c1">## array([-0.43, -0.43,  0.44,  0.45, -0.47])</span>
</pre></div>
</div>
<p>The first row in <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> consists of similar values,
but with different signs.
We can consider them a scaled version of the average
Energy Use (column 3), Greenhouse Gases (4), and
MINUS Safe Sanitation (1), MINUS Healthy Life (2),
MINUS Gross Domestic Product (5).
We could call this a measure of a country’s overall
eco-unfriendliness(?) because countries with low Healthy Life and high
Greenhouse Gasses will score highly on this scale.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># loadings – the second principal axis</span>
<span class="c1">## array([ 0.52,  0.5 ,  0.52,  0.45, -0.02])</span>
</pre></div>
</div>
<p>The second row in <span class="math notranslate nohighlight">\(\mathbf{Q}\)</span> defines a scaled version of the average
of Safe Sanitation (1), Healthy Life (2), Energy Use (3),
and Greenhouse Gases (4), almost completely ignoring the GDP (5).
Can we call it a measure of industrialisation? Something like this.
But this naming is just for fun<a class="footnote-reference brackets" href="#footfun" id="id18" role="doc-noteref"><span class="fn-bracket">[</span>14<span class="fn-bracket">]</span></a>.</p>
<p><a class="reference internal" href="#fig-pca-country"><span class="std std-numref">Figure 9.22</span></a> is a scatter plot of the countries projected onto
the said two principal directions. For readability, we only display a few
chosen labels. This is merely a projection/approximation,
but it might be an interesting one for some practitioners.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">P2</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">s</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># == Y @ Q[:2, :].T</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">P2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">P2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">which</span> <span class="o">=</span> <span class="p">[</span>   <span class="c1"># hand-crafted/artisan</span>
    <span class="mi">141</span><span class="p">,</span> <span class="mi">117</span><span class="p">,</span> <span class="mi">69</span><span class="p">,</span> <span class="mi">123</span><span class="p">,</span> <span class="mi">35</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">93</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span>
    <span class="mi">104</span><span class="p">,</span> <span class="mi">122</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">134</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">94</span><span class="p">,</span> <span class="mi">114</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">34</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">77</span><span class="p">,</span>
    <span class="mi">64</span><span class="p">,</span> <span class="mi">67</span><span class="p">,</span> <span class="mi">152</span><span class="p">,</span> <span class="mi">135</span><span class="p">,</span> <span class="mi">148</span><span class="p">,</span> <span class="mi">99</span><span class="p">,</span> <span class="mi">149</span><span class="p">,</span> <span class="mi">126</span><span class="p">,</span> <span class="mi">111</span><span class="p">,</span> <span class="mi">57</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">63</span>
<span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">which</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">P2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">P2</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">countries</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ha</span><span class="o">=</span><span class="s2">&quot;center&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;1st principal component (eco-unfriendliness?)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;2nd principal component (industrialisation?)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id60">
<span id="fig-pca-country"></span><img alt="../_images/pca-country-43.png" src="../_images/pca-country-43.png" />
<figcaption>
<p><span class="caption-number">Figure 9.22 </span><span class="caption-text">An example principal component analysis of countries.</span><a class="headerlink" href="#id60" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="further-reading">
<h2><span class="section-number">9.4. </span>Further reading<a class="headerlink" href="#further-reading" title="Permalink to this heading">#</a></h2>
<p>Other approaches to regression via linear models
include ridge and lasso, the latter having the nice property of
automatically getting rid of noninformative variables from the model.
Furthermore, instead of minimising squared residuals, we can also consider,
e.g., least absolute deviation.</p>
<p>There are many other approaches to dimensionality reduction,
also nonlinear ones, including kernel PCA, feature agglomeration
via hierarchical clustering, autoencoders, t-SNE, etc.</p>
<p>A popular introductory text in statistical learning is <span id="id19">[<a class="reference internal" href="999-bibliography.html#id55" title="Hastie, T., Tibshirani, R., and Friedman, J. (2017).  The Elements of Statistical Learning. Springer-Verlag. URL: https://hastie.su.domains/ElemStatLearn.">47</a>]</span>.
We recommend <span id="id20">[<a class="reference internal" href="999-bibliography.html#id154" title="Aggarwal, C.C. (2015).  Data Mining: The Textbook. Springer.">2</a>, <a class="reference internal" href="999-bibliography.html#id56" title="Bishop, C. (2006).  Pattern Recognition and Machine Learning. Springer-Verlag. URL: https://www.microsoft.com/en-us/research/people/cmbishop.">8</a>, <a class="reference internal" href="999-bibliography.html#id120" title="Blum, A., Hopcroft, J., and Kannan, R. (2020).  Foundations of Data Science. Cambridge University Press. URL: https://www.cs.cornell.edu/jeh/book.pdf.">9</a>, <a class="reference internal" href="999-bibliography.html#id80" title="Devroye, L., Györfi, L., and Lugosi, G. (1996).  A Probabilistic Theory of Pattern Recognition. Springer. DOI: 10.1007/978-1-4612-0711-5.">22</a>, <a class="reference internal" href="999-bibliography.html#id106" title="Efron, B. and Hastie, T. (2016).  Computer Age Statistical Inference: Algorithms, Evidence, and Data Science. Cambridge University Press.">24</a>]</span>
for more advanced students.
Computing-orientated students could benefit from
checking out <span id="id21">[<a class="reference internal" href="999-bibliography.html#id158" title="Monahan, J.F. (2011).  Numerical Methods of Statistics. Cambridge University Press.">65</a>]</span>.</p>
</section>
<section id="exercises">
<h2><span class="section-number">9.5. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this heading">#</a></h2>
<div class="proof proof-type-exercise" id="id61">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.16</span>
        
    </div><div class="proof-content">
<p>Why correlation is not causation?</p>
</div></div><div class="proof proof-type-exercise" id="id62">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.17</span>
        
    </div><div class="proof-content">
<p>What does the linear correlation of 0.9 mean?
What? about the rank correlation of 0.9?
And the linear correlation of 0.0?</p>
</div></div><div class="proof proof-type-exercise" id="id63">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.18</span>
        
    </div><div class="proof-content">
<p>How is Spearman’s coefficient related to Pearson’s one?</p>
</div></div><div class="proof proof-type-exercise" id="id64">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.19</span>
        
    </div><div class="proof-content">
<p>State the optimisation problem behind the least squares fitting
of linear models.</p>
</div></div><div class="proof proof-type-exercise" id="id65">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.20</span>
        
    </div><div class="proof-content">
<p>What are the different ways of the numerical summarising of residuals?</p>
</div></div><div class="proof proof-type-exercise" id="id66">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.21</span>
        
    </div><div class="proof-content">
<p>Why is it important for the residuals to be homoscedastic?</p>
</div></div><div class="proof proof-type-exercise" id="id67">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.22</span>
        
    </div><div class="proof-content">
<p>Is a more complex model always better?</p>
</div></div><div class="proof proof-type-exercise" id="id68">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.23</span>
        
    </div><div class="proof-content">
<p>Why must extrapolation be handled with care?</p>
</div></div><div class="proof proof-type-exercise" id="id69">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.24</span>
        
    </div><div class="proof-content">
<p>Why did we say that novice users should refrain from using
<strong class="program">scikit-learn</strong>?</p>
</div></div><div class="proof proof-type-exercise" id="id70">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.25</span>
        
    </div><div class="proof-content">
<p>What is the condition number of a model matrix and why is it
worthwhile to always check it?</p>
</div></div><div class="proof proof-type-exercise" id="id71">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.26</span>
        
    </div><div class="proof-content">
<p>What is the geometrical interpretation of the dot product of
two normalised vectors?</p>
</div></div><div class="proof proof-type-exercise" id="id72">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.27</span>
        
    </div><div class="proof-content">
<p>How can we verify if two vectors are orthonormal? What is an orthonormal
projection? What is the inverse of an orthonormal matrix?</p>
</div></div><div class="proof proof-type-exercise" id="id73">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.28</span>
        
    </div><div class="proof-content">
<p>What is the inverse of a diagonal matrix?</p>
</div></div><div class="proof proof-type-exercise" id="id74">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.29</span>
        
    </div><div class="proof-content">
<p>Characterise the general properties of the three matrices obtained
by performing the singular value decomposition of a given matrix
of shape <span class="math notranslate nohighlight">\(n\times m\)</span>.</p>
</div></div><div class="proof proof-type-exercise" id="id75">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.30</span>
        
    </div><div class="proof-content">
<p>How can we obtain the first principal component of a given centred matrix?</p>
</div></div><div class="proof proof-type-exercise" id="id76">

    <div class="proof-title">
        <span class="proof-type">Exercise 9.31</span>
        
    </div><div class="proof-content">
<p>How can we compute the ratios of the variances explained by the
consecutive principal components?</p>
</div></div><hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="footpearsonks" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">1</a><span class="fn-bracket">]</span></span>
<p>Note that in <a class="reference internal" href="230-distribution.html#sec-ks-test"><span class="std std-numref">Section 6.2.3</span></a>, we were also
testing one concrete hypothesis: whether a distribution
was normal or whether it was anything else.
We only know that if the data really follow that distribution,
the null hypothesis will not be rejected in 0.1% of the cases.
The rest is silence.</p>
</aside>
<aside class="footnote brackets" id="foothclust" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">2</a><span class="fn-bracket">]</span></span>
<p>(**) This can be done automatically by some
hierarchical clustering algorithm applied
onto the correlation matrix converted to a distance one,
e.g., <span class="math notranslate nohighlight">\(1-|\mathbf{R}|\)</span> or <span class="math notranslate nohighlight">\(1-\mathbf{R}^2\)</span>.</p>
</aside>
<aside class="footnote brackets" id="footspearman" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">3</a><span class="fn-bracket">]</span></span>
<p>If a method Y is nothing else than X on transformed
data, we do not consider it a totally new method.</p>
</aside>
<aside class="footnote brackets" id="footmodelserialise" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">4</a><span class="fn-bracket">]</span></span>
<p>To memorise the model
for further reference, we only need to serialise
its <span class="math notranslate nohighlight">\(m\)</span> coefficients, e.g., in a JSON or CSV file.</p>
</aside>
<aside class="footnote brackets" id="footwhyssr" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">5</a><span class="fn-bracket">]</span></span>
<p>Due to computability and mathematical analysability,
which we usually explore in more advanced courses on statistical data analysis such as <span id="id22">[<a class="reference internal" href="999-bibliography.html#id56" title="Bishop, C. (2006).  Pattern Recognition and Machine Learning. Springer-Verlag. URL: https://www.microsoft.com/en-us/research/people/cmbishop.">8</a>, <a class="reference internal" href="999-bibliography.html#id80" title="Devroye, L., Györfi, L., and Lugosi, G. (1996).  A Probabilistic Theory of Pattern Recognition. Springer. DOI: 10.1007/978-1-4612-0711-5.">22</a>, <a class="reference internal" href="999-bibliography.html#id55" title="Hastie, T., Tibshirani, R., and Friedman, J. (2017).  The Elements of Statistical Learning. Springer-Verlag. URL: https://hastie.su.domains/ElemStatLearn.">47</a>]</span>.</p>
</aside>
<aside class="footnote brackets" id="footerror" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">6</a><span class="fn-bracket">]</span></span>
<p>We sometimes explicitly list the error term that
corresponds to the residuals. This is to assure the reader that
we are not naïve and that we know what we are doing.
We see from the scatter plot of the involved variables
that the data do not lie on a straight line perfectly.
Each model is merely an idealisation/simplification of the described
reality. It is wise to remind ourselves about that every so often.</p>
</aside>
<aside class="footnote brackets" id="footknnas" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">7</a><span class="fn-bracket">]</span></span>
<p>In <span class="math notranslate nohighlight">\(k\)</span>-nearest neighbour regression, we are not
aiming to minimise anything in particular. If the model
is performing well with respect to some metrics such as RMSE or MAE,
we can consider ourselves
lucky. Nevertheless, some asymptotic results guarantee
the optimality of the outcomes generated for large sample sizes
(e.g., consistency); see, e.g., <span id="id23">[<a class="reference internal" href="999-bibliography.html#id80" title="Devroye, L., Györfi, L., and Lugosi, G. (1996).  A Probabilistic Theory of Pattern Recognition. Springer. DOI: 10.1007/978-1-4612-0711-5.">22</a>]</span>.</p>
</aside>
<aside class="footnote brackets" id="footscore" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">8</a><span class="fn-bracket">]</span></span>
<p>For a model that is <em>not</em> generated via least squares,
the coefficient of determination can also be negative,
particularly when the fit is extremely bad.
Also, note that this measure is dataset-dependent.
Therefore, it ought not to be used for comparing
models explaining different dependent variables.</p>
</aside>
<aside class="footnote brackets" id="footmle" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">9</a><span class="fn-bracket">]</span></span>
<p>For large <span class="math notranslate nohighlight">\(n\)</span>, we expect to pinpoint the true coefficients
exactly. In our scenario (independent,
normally distributed errors with the expectation of 0),
the least squares method is the maximum likelihood estimator
of the model parameters. As a consequence, it is consistent.</p>
</aside>
<aside class="footnote brackets" id="footunique" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">10</a><span class="fn-bracket">]</span></span>
<p>There are methods in statistical learning
where there might be multiple local minima – this is even more
difficult; see <a class="reference internal" href="430-group-by.html#sec-local-minima"><span class="std std-numref">Section 12.4.4</span></a>.</p>
</aside>
<aside class="footnote brackets" id="footsingularvalue" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">11</a><span class="fn-bracket">]</span></span>
<p>(**) Being themselves the square roots of eigenvalues
of <span class="math notranslate nohighlight">\(\mathbf{X}^T \mathbf{X}\)</span>. Equivalently,
<span class="math notranslate nohighlight">\(\kappa(\mathbf{X}^T)=\|(\mathbf{X}^T)^{-1}\|\, \|\mathbf{X}^T\|\)</span>
with respect to the spectral norm. Seriously, we really need linear
algebra when we even remotely think about practising data science.
Let us add it to our life skills bucket list.</p>
</aside>
<aside class="footnote brackets" id="footortho" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">12</a><span class="fn-bracket">]</span></span>
<p>Orthonormal matrices are sometimes simply
referred to as orthogonal ones.</p>
</aside>
<aside class="footnote brackets" id="footrankkapprox" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id17">13</a><span class="fn-bracket">]</span></span>
<p>(**) The Eckart–Young–Mirsky theorem states that
<span class="math notranslate nohighlight">\(\mathbf{U}_{\cdot, :k} \mathbf{S}_{:k, :k} \mathbf{Q}_{:k, \cdot}\)</span>
(where “<span class="math notranslate nohighlight">\(:k\)</span>” denotes “first <span class="math notranslate nohighlight">\(k\)</span> rows or columns”)
is the best rank-<span class="math notranslate nohighlight">\(k\)</span> approximation of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>
with respect to both the Frobenius and spectral norms.</p>
</aside>
<aside class="footnote brackets" id="footfun" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id18">14</a><span class="fn-bracket">]</span></span>
<p>Nonetheless, someone might take these results seriously
and scribble a research thesis about it.
Mathematics, unlike the brains of ordinary mortals,
does not need our imperfect interpretations/fairy tales
to function properly. We need more maths in our lives.</p>
</aside>
</aside>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="410-data-frame.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title"><span class="section-number">10. </span>Introducing data frames</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="320-transform-matrix.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title"><span class="section-number">8. </span>Processing multidimensional data</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
              
              
              Copyright &#169; 2022–2023 by <a href="https://www.gagolewski.com">Marek Gagolewski</a>.
              Some rights reserved. Licensed under <a href='https://creativecommons.org/licenses/by-nc-nd/4.0/'>CC BY-NC-ND 4.0</a>.
              Built with <a href="https://sphinx-doc.org/">Sphinx</a>
              and a customised <a href="https://github.com/pradyunsg/furo">Furo</a> theme.
              Last updated on 2023-07-10T13:20:52+1000.
              This site will never display any ads: it is a non-profit project.
              It does not collect any data.
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            In this chapter
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">9. Exploring relationships between variables</a><ul>
<li><a class="reference internal" href="#measuring-correlation">9.1. Measuring correlation</a><ul>
<li><a class="reference internal" href="#pearson-linear-correlation-coefficient">9.1.1. Pearson linear correlation coefficient</a><ul>
<li><a class="reference internal" href="#perfect-linear-correlation">9.1.1.1. Perfect linear correlation</a></li>
<li><a class="reference internal" href="#strong-linear-correlation">9.1.1.2. Strong linear correlation</a></li>
<li><a class="reference internal" href="#no-linear-correlation-does-not-imply-independence">9.1.1.3. No linear correlation does not imply independence</a></li>
<li><a class="reference internal" href="#false-linear-correlations">9.1.1.4. False linear correlations</a></li>
<li><a class="reference internal" href="#correlation-is-not-causation">9.1.1.5. Correlation is not causation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#correlation-heat-map">9.1.2. Correlation heat map</a></li>
<li><a class="reference internal" href="#linear-correlation-coefficients-on-transformed-data">9.1.3. Linear correlation coefficients on transformed data</a></li>
<li><a class="reference internal" href="#spearman-rank-correlation-coefficient">9.1.4. Spearman rank correlation coefficient</a></li>
</ul>
</li>
<li><a class="reference internal" href="#regression-tasks">9.2. Regression tasks</a><ul>
<li><a class="reference internal" href="#k-nearest-neighbour-regression">9.2.1. <em>K</em>-nearest neighbour regression</a></li>
<li><a class="reference internal" href="#from-data-to-linear-models">9.2.2. From data to (linear) models</a></li>
<li><a class="reference internal" href="#least-squares-method">9.2.3. Least squares method</a></li>
<li><a class="reference internal" href="#analysis-of-residuals">9.2.4. Analysis of residuals</a></li>
<li><a class="reference internal" href="#multiple-regression">9.2.5. Multiple regression</a></li>
<li><a class="reference internal" href="#variable-transformation-and-linearisable-models">9.2.6. Variable transformation and linearisable models (*)</a></li>
<li><a class="reference internal" href="#descriptive-vs-predictive-power">9.2.7. Descriptive vs predictive power (*)</a></li>
<li><a class="reference internal" href="#fitting-regression-models-with-scikit-learn">9.2.8. Fitting regression models with <strong class="program">scikit-learn</strong> (*)</a></li>
<li><a class="reference internal" href="#ill-conditioned-model-matrices">9.2.9. Ill-conditioned model matrices (*)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#finding-interesting-combinations-of-variables">9.3. Finding interesting combinations of variables (*)</a><ul>
<li><a class="reference internal" href="#dot-products-angles-collinearity-and-orthogonality">9.3.1. Dot products, angles, collinearity, and orthogonality</a></li>
<li><a class="reference internal" href="#geometric-transformations-of-points">9.3.2. Geometric transformations of points</a></li>
<li><a class="reference internal" href="#matrix-inverse">9.3.3. Matrix inverse</a></li>
<li><a class="reference internal" href="#singular-value-decomposition">9.3.4. Singular value decomposition</a></li>
<li><a class="reference internal" href="#dimensionality-reduction-with-svd">9.3.5. Dimensionality reduction with SVD</a></li>
<li><a class="reference internal" href="#principal-component-analysis">9.3.6. Principal component analysis</a></li>
</ul>
</li>
<li><a class="reference internal" href="#further-reading">9.4. Further reading</a></li>
<li><a class="reference internal" href="#exercises">9.5. Exercises</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/scripts/furo.js"></script>
    <script src="../_static/proof.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </body>
</html>