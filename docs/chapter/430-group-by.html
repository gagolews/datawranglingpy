<!DOCTYPE html>


<html class="writer-html5" lang="en" >
<!-- Copyright (C) 2020-2022, Marek Gagolewski <https://www.gagolewski.com> -->

<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>12. Processing Data in Groups &mdash; Minimalist Data Wrangling with Python by Marek Gagolewski</title>
  

  
  
  <script async defer src="https://buttons.github.io/buttons.js"></script>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/proof.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />

  
  

  
  

  
    <link rel="canonical" href="https://datawranglingpy.gagolewski.com/chapter/430-group-by.html" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/proof.js"></script>
        <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="13. Accessing Databases" href="440-sql.html" />
    <link rel="prev" title="11. Handling Categorical Data" href="420-categorical.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html"> Minimalist Data Wrangling with Python
          

          
          </a>

          <div class="version">
          An Open-Access Textbook<br />    by <a style="color: inherit" href="https://www.gagolewski.com">Marek     Gagolewski</a><br />    v1.0.2
          </div>

<!--
          
            
            
              <div class="version">
                by Marek Gagolewski
              </div>
            
          
-->

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search phrase..." />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">About</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.gagolewski.com/">Author</a></li>
<li class="toctree-l1"><a class="reference external" href="https://datawranglingpy.gagolewski.com/datawranglingpy.pdf">This Book in PDF</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/gagolews/datawranglingpy/">Report Bugs or Typos (GitHub)</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/gagolews/teaching-data">Datasets</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Start Here</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="000-preface.html">Preface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introducing Python</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="110-setup.html">1. Getting Started with Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="120-scalar.html">2. Scalar Types and Control Structures in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="130-sequential.html">3. Sequential and Other Types in Python</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Unidimensional Data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="210-vector.html">4. Unidimensional Numeric Data and Their Empirical Distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="220-transform-vector.html">5. Processing Unidimensional Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="230-distribution.html">6. Continuous Probability Distributions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multidimensional Data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="310-matrix.html">7. Multidimensional Numeric Data at a Glance</a></li>
<li class="toctree-l1"><a class="reference internal" href="320-transform-matrix.html">8. Processing Multidimensional Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="330-relationship.html">9. Exploring Relationships Between Variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Heterogeneous Data</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="410-data-frame.html">10. Introducing Data Frames</a></li>
<li class="toctree-l1"><a class="reference internal" href="420-categorical.html">11. Handling Categorical Data</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">12. Processing Data in Groups</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#basic-methods">12.1. Basic Methods</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#aggregating-data-in-groups">12.1.1. Aggregating Data in Groups</a></li>
<li class="toctree-l3"><a class="reference internal" href="#transforming-data-in-groups">12.1.2. Transforming Data in Groups</a></li>
<li class="toctree-l3"><a class="reference internal" href="#manual-splitting-into-subgroups">12.1.3. Manual Splitting into Subgroups (*)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#plotting-data-in-groups">12.2. Plotting Data in Groups</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#series-of-box-plots">12.2.1. Series of Box Plots</a></li>
<li class="toctree-l3"><a class="reference internal" href="#series-of-bar-plots">12.2.2. Series of Bar Plots</a></li>
<li class="toctree-l3"><a class="reference internal" href="#semitransparent-histograms">12.2.3. Semitransparent Histograms</a></li>
<li class="toctree-l3"><a class="reference internal" href="#scatterplots-with-group-information">12.2.4. Scatterplots with Group Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="#grid-trellis-plots">12.2.5. Grid (Trellis) Plots</a></li>
<li class="toctree-l3"><a class="reference internal" href="#comparing-ecdfs-with-the-kolmogorovsmirnov-test">12.2.6. Comparing ECDFs with the Kolmogorov–Smirnov Test (*)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#comparing-quantiles">12.2.7. Comparing Quantiles</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#classification-tasks">12.3. Classification Tasks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#k-nearest-neighbour-classification">12.3.1. <em>K</em>-Nearest Neighbour Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="#assessing-the-quality-of-predictions">12.3.2. Assessing the Quality of Predictions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#splitting-into-training-and-test-sets">12.3.3. Splitting into Training and Test Sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="#validating-many-models-parameter-selection">12.3.4. Validating Many Models (Parameter Selection) (*)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#clustering-tasks">12.4. Clustering Tasks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#k-means-method">12.4.1. <em>K</em>-Means Method</a></li>
<li class="toctree-l3"><a class="reference internal" href="#solving-k-means-is-hard">12.4.2. Solving <em>K</em>-means Is Hard</a></li>
<li class="toctree-l3"><a class="reference internal" href="#lloyd-s-algorithm">12.4.3. Lloyd’s Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="#local-minima">12.4.4. Local Minima</a></li>
<li class="toctree-l3"><a class="reference internal" href="#random-restarts">12.4.5. Random Restarts</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#further-reading">12.5. Further Reading</a></li>
<li class="toctree-l2"><a class="reference internal" href="#exercises">12.6. Exercises</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="440-sql.html">13. Accessing Databases</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Data Types</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="510-text.html">14. Text Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="520-missingness.html">15. Missing, Censored, and Questionable Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="530-time-series.html">16. Time Series</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="998-changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="999-bibliography.html">Bibliography</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Minimalist Data Wrangling with Python</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          



















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li><span class="section-number">12. </span>Processing Data in Groups</li>
    
    
      <li class="wy-breadcrumbs-aside">

        
        
        <a class="github-button" href="https://github.com/gagolews/datawranglingpy" data-icon="octicon-star" data-size="large" data-show-count="true" aria-label="Star gagolews/datawranglingpy on GitHub">Star</a>
        


        
      </li>
    
  </ul>

  
  <div class="rst-breadcrumbs-buttons" role="navigation" aria-label="breadcrumb navigation">
      
        <a href="440-sql.html" class="btn btn-neutral float-right" title="13. Accessing Databases" accesskey="n">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
      
      
        <a href="420-categorical.html" class="btn btn-neutral float-left" title="11. Handling Categorical Data" accesskey="p"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
      
  </div>
  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section class="tex2jax_ignore mathjax_ignore" id="processing-data-in-groups">
<span id="chap-group-by"></span><h1><span class="section-number">12. </span>Processing Data in Groups<a class="headerlink" href="#processing-data-in-groups" title="Permalink to this heading"></a></h1>
<blockquote>
<div><p><em>The open-access textbook</em> Minimalist Data
Wrangling with Python <em>by <a class="reference external" href="https://www.gagolewski.com">Marek Gagolewski</a>
is, and will remain, freely available for everyone’s enjoyment
(also in <a class="reference external" href="https://datawranglingpy.gagolewski.com/datawranglingpy.pdf">PDF</a>).
It is a non-profit project.
Although available online, it is a whole course;
it should be read from the beginning to the end.
Refer to the <a class="reference internal" href="000-preface.html#chap-preface"><span class="std std-ref">Preface</span></a> for general introductory remarks.</em></p>
</div></blockquote>
<p>Let us consider another subset of the US Centres for Disease Control
and Prevention National Health and Nutrition Examination Survey,
this time carrying some body measures
(<a class="reference external" href="https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/P_BMX.htm">P_BMX</a>)
together with demographics
(<a class="reference external" href="https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/P_DEMO.htm">P_DEMO</a>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching-data/master/marek/nhanes_p_demo_bmx_2020.csv&quot;</span><span class="p">,</span>
    <span class="n">comment</span><span class="o">=</span><span class="s2">&quot;#&quot;</span><span class="p">)</span>
<span class="n">nhanes</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">nhanes</span>
    <span class="o">.</span><span class="n">loc</span><span class="p">[</span>
        <span class="p">(</span><span class="n">nhanes</span><span class="o">.</span><span class="n">DMDBORN4</span> <span class="o">&lt;=</span> <span class="mi">2</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">nhanes</span><span class="o">.</span><span class="n">RIDAGEYR</span> <span class="o">&gt;=</span> <span class="mi">18</span><span class="p">),</span>
        <span class="p">[</span><span class="s2">&quot;RIDAGEYR&quot;</span><span class="p">,</span> <span class="s2">&quot;BMXWT&quot;</span><span class="p">,</span> <span class="s2">&quot;BMXHT&quot;</span><span class="p">,</span> <span class="s2">&quot;BMXBMI&quot;</span><span class="p">,</span> <span class="s2">&quot;RIAGENDR&quot;</span><span class="p">,</span> <span class="s2">&quot;DMDBORN4&quot;</span><span class="p">]</span>
    <span class="p">]</span>  <span class="c1"># age &gt;= 18 and only US and non-US born</span>
    <span class="o">.</span><span class="n">rename</span><span class="p">({</span>
        <span class="s2">&quot;RIDAGEYR&quot;</span><span class="p">:</span> <span class="s2">&quot;age&quot;</span><span class="p">,</span>
        <span class="s2">&quot;BMXWT&quot;</span><span class="p">:</span> <span class="s2">&quot;weight&quot;</span><span class="p">,</span>
        <span class="s2">&quot;BMXHT&quot;</span><span class="p">:</span> <span class="s2">&quot;height&quot;</span><span class="p">,</span>
        <span class="s2">&quot;BMXBMI&quot;</span><span class="p">:</span> <span class="s2">&quot;bmival&quot;</span><span class="p">,</span>
        <span class="s2">&quot;RIAGENDR&quot;</span><span class="p">:</span> <span class="s2">&quot;gender&quot;</span><span class="p">,</span>
        <span class="s2">&quot;DMDBORN4&quot;</span><span class="p">:</span> <span class="s2">&quot;usborn&quot;</span>
    <span class="p">},</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># rename columns</span>
    <span class="o">.</span><span class="n">dropna</span><span class="p">()</span>   <span class="c1"># remove missing values</span>
    <span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>We consider only the adult (at least 18 years old) participants,
whose country of birth (the US or not) is well-defined.
Let us recode the <code class="docutils literal notranslate"><span class="pre">usborn</span></code> and <code class="docutils literal notranslate"><span class="pre">gender</span></code> variables (for readability)
and introduce the BMI categories:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;usborn&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">nhanes</span><span class="o">.</span><span class="n">usborn</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;category&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">cat</span><span class="o">.</span><span class="n">rename_categories</span><span class="p">([</span><span class="s2">&quot;yes&quot;</span><span class="p">,</span> <span class="s2">&quot;no&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;str&quot;</span><span class="p">)</span>       <span class="c1"># recode usborn</span>
<span class="p">)</span>
<span class="n">nhanes</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;gender&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">nhanes</span><span class="o">.</span><span class="n">gender</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;category&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">cat</span><span class="o">.</span><span class="n">rename_categories</span><span class="p">([</span><span class="s2">&quot;male&quot;</span><span class="p">,</span> <span class="s2">&quot;female&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;str&quot;</span><span class="p">)</span>  <span class="c1"># recode gender</span>
<span class="p">)</span>
<span class="n">nhanes</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;bmicat&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span>                             <span class="c1"># new column</span>
    <span class="n">nhanes</span><span class="o">.</span><span class="n">bmival</span><span class="p">,</span>
    <span class="n">bins</span><span class="o">=</span>  <span class="p">[</span> <span class="mi">0</span><span class="p">,</span>             <span class="mf">18.5</span><span class="p">,</span>        <span class="mi">25</span><span class="p">,</span>            <span class="mi">30</span><span class="p">,</span>       <span class="n">np</span><span class="o">.</span><span class="n">inf</span> <span class="p">],</span>
    <span class="n">labels</span><span class="o">=</span><span class="p">[</span>   <span class="s2">&quot;underweight&quot;</span><span class="p">,</span>    <span class="s2">&quot;normal&quot;</span><span class="p">,</span>  <span class="s2">&quot;overweight&quot;</span><span class="p">,</span>  <span class="s2">&quot;obese&quot;</span>       <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Here is a preview of this data frame:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<span class="c1">##    age  weight  height  bmival  gender usborn      bmicat</span>
<span class="c1">## 0   29    97.1   160.2    37.8  female     no       obese</span>
<span class="c1">## 1   49    98.8   182.3    29.7    male    yes  overweight</span>
<span class="c1">## 2   36    74.3   184.2    21.9    male    yes      normal</span>
<span class="c1">## 3   68   103.7   185.3    30.2    male    yes       obese</span>
<span class="c1">## 4   76    83.3   177.1    26.6    male    yes  overweight</span>
</pre></div>
</div>
<p>We have a mix of categorical (gender, US born-ness, BMI category)
and numerical (age, weight, height, BMI) variables.
Unless we had encoded qualitative variables as integers,
this would not be possible with plain matrices,
at least not with a single one.</p>
<p>In this section, we will treat the categorical columns
as grouping variables, so that we can, e.g., summarise
or visualise the data <em>in each group</em> separately, because
it is likely that data distributions vary across different factor levels.
This is much like having many data frames stored in one object,
e.g., the heights of women and men separately.</p>
<p><code class="docutils literal notranslate"><span class="pre">nhanes</span></code> is thus an example of heterogeneous data at their best.</p>
<section id="basic-methods">
<h2><span class="section-number">12.1. </span>Basic Methods<a class="headerlink" href="#basic-methods" title="Permalink to this heading"></a></h2>
<p><code class="docutils literal notranslate"><span class="pre">DataFrame</span></code> and <code class="docutils literal notranslate"><span class="pre">Series</span></code> objects are equipped with the <strong class="command">groupby</strong>
methods, which assist in performing a wide range
of operations in data groups defined by one
or more data frame columns (compare <span id="id1">[<a class="reference internal" href="999-bibliography.html#id35" title="H. Wickham. The split-apply-combine strategy for data analysis. Journal of Statistical Software, 40(1):1–29, 2011. doi:10.18637/jss.v040.i01.">Wic11</a>]</span>).</p>
<p>They return objects of class <code class="docutils literal notranslate"><span class="pre">DataFrameGroupBy</span></code>
and <code class="docutils literal notranslate"><span class="pre">SeriesGroupby</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">type</span><span class="p">(</span><span class="n">nhanes</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">))</span>
<span class="c1">## &lt;class &#39;pandas.core.groupby.generic.DataFrameGroupBy&#39;&gt;</span>
<span class="nb">type</span><span class="p">(</span><span class="n">nhanes</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">height</span><span class="p">)</span>  <span class="c1"># or (...)[&quot;height&quot;]</span>
<span class="c1">## &lt;class &#39;pandas.core.groupby.generic.SeriesGroupBy&#39;&gt;</span>
</pre></div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>When we wish to browse the list of available attributes
in the <strong class="program">pandas</strong> manual, it is worth knowing that
<code class="docutils literal notranslate"><span class="pre">DataFrameGroupBy</span></code> and <code class="docutils literal notranslate"><span class="pre">SeriesGroupBy</span></code> are separate types.
Still, they have many methods and slots in common,
because they both inherit from (extend) the <code class="docutils literal notranslate"><span class="pre">GroupBy</span></code> class.</p>
</div>
<div class="proof proof-type-exercise" id="id26">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.1</span>
        
    </div><div class="proof-content">
<p>Skim through the
<a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/reference/groupby.html">documentation</a>
of the said classes.</p>
</div></div><p>For example, the <strong class="command">pandas.DataFrameGroupBy.size</strong>
method determines the number of observations in each group:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="c1">## gender</span>
<span class="c1">## female    4514</span>
<span class="c1">## male      4271</span>
<span class="c1">## dtype: int64</span>
</pre></div>
</div>
<p>It returns an object of type <code class="docutils literal notranslate"><span class="pre">Series</span></code>.
We can also perform the grouping with respect
to a combination of levels in two qualitative columns:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="s2">&quot;bmicat&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="c1">## gender  bmicat     </span>
<span class="c1">## female  underweight      93</span>
<span class="c1">##         normal         1161</span>
<span class="c1">##         overweight     1245</span>
<span class="c1">##         obese          2015</span>
<span class="c1">## male    underweight      65</span>
<span class="c1">##         normal         1074</span>
<span class="c1">##         overweight     1513</span>
<span class="c1">##         obese          1619</span>
<span class="c1">## dtype: int64</span>
</pre></div>
</div>
<p>This yields a <code class="docutils literal notranslate"><span class="pre">Series</span></code> with a hierarchical index (as
discussed in <a class="reference internal" href="410-data-frame.html#sec-df-index"><span class="std std-numref">Section 10.1.3</span></a>). Nevertheless, we can always call
<strong class="command">reset_index</strong> to convert it to standalone columns:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="s2">&quot;bmicat&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s2">&quot;counts&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="c1">##    gender       bmicat  counts</span>
<span class="c1">## 0  female  underweight      93</span>
<span class="c1">## 1  female       normal    1161</span>
<span class="c1">## 2  female   overweight    1245</span>
<span class="c1">## 3  female        obese    2015</span>
<span class="c1">## 4    male  underweight      65</span>
<span class="c1">## 5    male       normal    1074</span>
<span class="c1">## 6    male   overweight    1513</span>
<span class="c1">## 7    male        obese    1619</span>
</pre></div>
</div>
<p>Take note of the <strong class="command">rename</strong> part. It gave us some readable
column names.</p>
<p>Furthermore, it is possible to group rows in a data frame
using a list of any <code class="docutils literal notranslate"><span class="pre">Series</span></code> objects, i.e., not just column names in
a given data frame; see <a class="reference internal" href="530-time-series.html#sec-df-datetime"><span class="std std-numref">Section 16.2.3</span></a>
for an example.</p>
<div class="proof proof-type-exercise" id="id27">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.2</span>
        
    </div><div class="proof-content">
<p>(*) Note the difference between
<strong class="command">pandas.GroupBy.count</strong> and <strong class="command">pandas.GroupBy.size</strong> methods
(by reading their documentation).</p>
</div></div><section id="aggregating-data-in-groups">
<h3><span class="section-number">12.1.1. </span>Aggregating Data in Groups<a class="headerlink" href="#aggregating-data-in-groups" title="Permalink to this heading"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">DataFrameGroupBy</span></code> and <code class="docutils literal notranslate"><span class="pre">SeriesGroupBy</span></code> classes are equipped
with several well-known aggregation functions. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="c1">##    gender        age     weight      height     bmival</span>
<span class="c1">## 0  female  48.956580  78.351839  160.089189  30.489189</span>
<span class="c1">## 1    male  49.653477  88.589932  173.759541  29.243620</span>
</pre></div>
</div>
<p>The arithmetic mean was computed only
on numeric columns<a class="footnote-reference brackets" href="#footoop" id="id2">1</a>.
Further, a few common aggregates are generated by <strong class="command">describe</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">height</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="c1">##    gender   count        mean       std  ...    25%    50%    75%    max</span>
<span class="c1">## 0  female  4514.0  160.089189  7.035483  ...  155.3  160.0  164.8  189.3</span>
<span class="c1">## 1    male  4271.0  173.759541  7.702224  ...  168.5  173.8  178.9  199.6</span>
<span class="c1">## </span>
<span class="c1">## [2 rows x 9 columns]</span>
</pre></div>
</div>
<p>But we can always apply a custom list of functions by
using <strong class="command">aggregate</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">nhanes</span><span class="o">.</span>
    <span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">]]</span><span class="o">.</span>
    <span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">)</span><span class="o">.</span>
    <span class="n">aggregate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">median</span><span class="p">,</span> <span class="nb">len</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span>
    <span class="n">reset_index</span><span class="p">()</span>
<span class="p">)</span>
<span class="c1">##    gender      height               ...     weight                        </span>
<span class="c1">##                  mean median   len  ...       mean median   len &lt;lambda_0&gt;</span>
<span class="c1">## 0  female  160.089189  160.0  4514  ...  78.351839   74.1  4514     110.85</span>
<span class="c1">## 1    male  173.759541  173.8  4271  ...  88.589932   85.0  4271     102.90</span>
<span class="c1">## </span>
<span class="c1">## [2 rows x 9 columns]</span>
</pre></div>
</div>
<p>The result’s <code class="docutils literal notranslate"><span class="pre">columns</span></code> slot features a hierarchical index.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The column names in the output object
are generated by reading the applied functions’ <code class="docutils literal notranslate"><span class="pre">__name__</span></code> slots,
see, e.g., <strong class="command">print</strong><code class="code docutils literal notranslate"><span class="pre">(np.mean.__name__)</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mr</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="mi">2</span>
<span class="n">mr</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="s2">&quot;midrange&quot;</span>
<span class="p">(</span><span class="n">nhanes</span><span class="o">.</span>
    <span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">]]</span><span class="o">.</span>
    <span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">)</span><span class="o">.</span>
    <span class="n">aggregate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">mr</span><span class="p">])</span><span class="o">.</span>
    <span class="n">reset_index</span><span class="p">()</span>
<span class="p">)</span>
<span class="c1">##    gender      height              weight         </span>
<span class="c1">##                  mean midrange       mean midrange</span>
<span class="c1">## 0  female  160.089189     29.1  78.351839   110.85</span>
<span class="c1">## 1    male  173.759541     27.5  88.589932   102.90</span>
</pre></div>
</div>
</div>
</section>
<section id="transforming-data-in-groups">
<h3><span class="section-number">12.1.2. </span>Transforming Data in Groups<a class="headerlink" href="#transforming-data-in-groups" title="Permalink to this heading"></a></h3>
<p>We can easily transform individual columns relative to different
data groups by means of the <strong class="command">transform</strong> method for <code class="docutils literal notranslate"><span class="pre">GroupBy</span></code>
objects.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">std0</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">std0</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="s2">&quot;std0&quot;</span>

<span class="k">def</span> <span class="nf">standardise</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">/</span><span class="n">std0</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">nhanes</span><span class="p">[</span><span class="s2">&quot;height_std&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">nhanes</span><span class="o">.</span>
    <span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="s2">&quot;gender&quot;</span><span class="p">]]</span><span class="o">.</span>
    <span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">)</span><span class="o">.</span>
    <span class="n">transform</span><span class="p">(</span><span class="n">standardise</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">nhanes</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<span class="c1">##    age  weight  height  bmival  gender usborn      bmicat  height_std</span>
<span class="c1">## 0   29    97.1   160.2    37.8  female     no       obese    0.015752</span>
<span class="c1">## 1   49    98.8   182.3    29.7    male    yes  overweight    1.108960</span>
<span class="c1">## 2   36    74.3   184.2    21.9    male    yes      normal    1.355671</span>
<span class="c1">## 3   68   103.7   185.3    30.2    male    yes       obese    1.498504</span>
<span class="c1">## 4   76    83.3   177.1    26.6    male    yes  overweight    0.433751</span>
</pre></div>
</div>
<p>The new column gives the <em>relative</em> z-scores:
a woman with a relative z-score of 0
has height of 160.1 cm, whereas
a man with the same z-score has
height of 173.8 cm.</p>
<p>We can check that the means and standard deviations
in both groups are equal to 0 and 1:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">nhanes</span><span class="o">.</span>
    <span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="s2">&quot;height_std&quot;</span><span class="p">]]</span><span class="o">.</span>
    <span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">)</span><span class="o">.</span>
    <span class="n">aggregate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">std0</span><span class="p">])</span>
<span class="p">)</span>
<span class="c1">##             height              height_std     </span>
<span class="c1">##               mean      std0          mean std0</span>
<span class="c1">## gender                                         </span>
<span class="c1">## female  160.089189  7.034703 -1.351747e-15  1.0</span>
<span class="c1">## male    173.759541  7.701323  3.145329e-16  1.0</span>
</pre></div>
</div>
<p>Interestingly, it is likely a bug in <strong class="program">pandas</strong> that
<code class="docutils literal notranslate"><span class="pre">groupby(&quot;gender&quot;).aggregate([np.std])</span></code> somewhat passes <code class="docutils literal notranslate"><span class="pre">ddof=1</span></code>
to <strong class="command">numpy.std</strong>, hence our using a custom function.</p>
<div class="proof proof-type-exercise" id="id28">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.3</span>
        
    </div><div class="proof-content">
<p>Create a data frame comprised of the five tallest men
and the five tallest women.</p>
</div></div></section>
<section id="manual-splitting-into-subgroups">
<h3><span class="section-number">12.1.3. </span>Manual Splitting into Subgroups (*)<a class="headerlink" href="#manual-splitting-into-subgroups" title="Permalink to this heading"></a></h3>
<p>It turns out that <code class="docutils literal notranslate"><span class="pre">GroupBy</span></code> objects and their derivatives
are <em>iterable</em>; compare <a class="reference internal" href="130-sequential.html#sec-iterable"><span class="std std-numref">Section 3.4</span></a>. As a consequence,
the grouped data frames and series can be easily processed manually
in case where the built-in methods are insufficient (i.e.,
not so rarely).</p>
<p>Let us consider a small sample of our data frame.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grouped</span> <span class="o">=</span> <span class="p">(</span><span class="n">nhanes</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
    <span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="s2">&quot;height&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">)</span>
<span class="p">)</span>
<span class="nb">list</span><span class="p">(</span><span class="n">grouped</span><span class="p">)</span>
<span class="c1">## [(&#39;female&#39;,    gender  weight  height</span>
<span class="c1">## 0  female    97.1   160.2), (&#39;male&#39;,   gender  weight  height</span>
<span class="c1">## 1   male    98.8   182.3</span>
<span class="c1">## 2   male    74.3   184.2</span>
<span class="c1">## 3   male   103.7   185.3</span>
<span class="c1">## 4   male    83.3   177.1)]</span>
</pre></div>
</div>
<p>The way Python formatted the above output is imperfect,
so we need to contemplate it for a tick.
We see that when iterating through a <code class="docutils literal notranslate"><span class="pre">GroupBy</span></code> object,
we get access to pairs giving all the levels of the grouping
variable and the subsets of the input data frame
corresponding to these categories.</p>
<p>Here is a simple example where we make use of the above fact:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">level</span><span class="p">,</span> <span class="n">df</span> <span class="ow">in</span> <span class="n">grouped</span><span class="p">:</span>
    <span class="c1"># level is a string label</span>
    <span class="c1"># df is a data frame - we can do whatever we want</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;There are </span><span class="si">{</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> subject(s) with gender=`</span><span class="si">{</span><span class="n">level</span><span class="si">}</span><span class="s2">`.&quot;</span><span class="p">)</span>
<span class="c1">## There are 1 subject(s) with gender=`female`.</span>
<span class="c1">## There are 4 subject(s) with gender=`male`.</span>
</pre></div>
</div>
<p>We see that splitting followed by manual processing of the chunks
in a loop is quite tedious in the case where we would merely like to compute
some basic aggregates.
These scenarios are extremely common. No wonder why the <strong class="program">pandas</strong>
developers introduced a convenient interface
in the form of the <strong class="command">pandas.DataFrame.groupby</strong>
and <strong class="command">pandas.Series.groupby</strong> methods and the <code class="docutils literal notranslate"><span class="pre">DataFrameGroupBy</span></code>
and <code class="docutils literal notranslate"><span class="pre">SeriesGroupby</span></code> classes.
Still, for more ambitious tasks, the low-level way to perform
the splitting will come in handy.</p>
<div class="proof proof-type-exercise" id="id29">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.4</span>
        
    </div><div class="proof-content">
<p>(**) Using the manual splitting and <strong class="command">matplotlib.pyplot.boxplot</strong>,
draw a box-and-whisker plot of heights grouped by BMI category
(four boxes side by side).</p>
</div></div><div class="proof proof-type-exercise" id="id30">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.5</span>
        
    </div><div class="proof-content">
<p>(**) Using the manual splitting,
compute the relative z-scores of the height
column separately for each BMI category.</p>
</div></div><div class="proof proof-type-example" id="id31">

    <div class="proof-title">
        <span class="proof-type">Example 12.6</span>
        
    </div><div class="proof-content">
<p>Let us also demonstrate that the splitting can be done manually
without the use of <strong class="program">pandas</strong>.
Namely, calling <strong class="command">numpy.split</strong><code class="code docutils literal notranslate"><span class="pre">(a,</span> <span class="pre">ind)</span></code> returns a list
with <code class="docutils literal notranslate"><span class="pre">a</span></code> (being an array-like object, e.g., a vector, a matrix, or a data
frame) partitioned rowwisely into <strong class="command">len</strong><code class="code docutils literal notranslate"><span class="pre">(ind)+1</span></code> chunks
at indexes given by <code class="docutils literal notranslate"><span class="pre">ind</span></code>.
For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;one&quot;</span><span class="p">,</span> <span class="s2">&quot;two&quot;</span><span class="p">,</span> <span class="s2">&quot;three&quot;</span><span class="p">,</span> <span class="s2">&quot;four&quot;</span><span class="p">,</span> <span class="s2">&quot;five&quot;</span><span class="p">,</span> <span class="s2">&quot;six&quot;</span><span class="p">,</span> <span class="s2">&quot;seven&quot;</span><span class="p">,</span> <span class="s2">&quot;eight&quot;</span><span class="p">,</span> <span class="s2">&quot;nine&quot;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
<span class="c1">## array([&#39;one&#39;, &#39;two&#39;], dtype=&#39;&lt;U5&#39;)</span>
<span class="c1">## array([&#39;three&#39;, &#39;four&#39;, &#39;five&#39;, &#39;six&#39;], dtype=&#39;&lt;U5&#39;)</span>
<span class="c1">## array([&#39;seven&#39;, &#39;eight&#39;, &#39;nine&#39;], dtype=&#39;&lt;U5&#39;)</span>
</pre></div>
</div>
<p>To split a data frame into groups defined by a categorical column,
we can first sort it with respect to the criterion of interest,
for instance, the <code class="docutils literal notranslate"><span class="pre">gender</span></code> data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes_srt</span> <span class="o">=</span> <span class="n">nhanes</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;stable&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Then, we can use <strong class="command">numpy.unique</strong> to fetch the indexes of
first occurrences of each series of identical labels:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">levels</span><span class="p">,</span> <span class="n">where</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">nhanes_srt</span><span class="o">.</span><span class="n">gender</span><span class="p">,</span> <span class="n">return_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">levels</span><span class="p">,</span> <span class="n">where</span>
<span class="c1">## (array([&#39;female&#39;, &#39;male&#39;], dtype=object), array([   0, 4514]))</span>
</pre></div>
</div>
<p>This can now be used for dividing the sorted data frame into chunks:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes_grp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">nhanes_srt</span><span class="p">,</span> <span class="n">where</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>  <span class="c1"># where[0] is not interesting</span>
</pre></div>
</div>
<p>We obtained a list of data frames split at rows specified by <code class="docutils literal notranslate"><span class="pre">where[1:]</span></code>.
Here is a preview of the first and the last row in each chunk:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">levels</span><span class="p">)):</span>
    <span class="c1"># process (levels[i], nhanes_grp[i])</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;level=&#39;</span><span class="si">{</span><span class="n">levels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39;; preview:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">nhanes_grp</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">:</span> <span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1">## level=&#39;female&#39;; preview:</span>
<span class="c1">##       age  weight  height  bmival  gender usborn bmicat  height_std</span>
<span class="c1">## 0      29    97.1   160.2    37.8  female     no  obese    0.015752</span>
<span class="c1">## 8781   67    82.8   147.8    37.9  female     no  obese   -1.746938</span>
<span class="c1">## </span>
<span class="c1">## level=&#39;male&#39;; preview:</span>
<span class="c1">##       age  weight  height  bmival gender usborn      bmicat  height_std</span>
<span class="c1">## 1      49    98.8   182.3    29.7   male    yes  overweight    1.108960</span>
<span class="c1">## 8784   74    59.7   167.5    21.3   male     no      normal   -0.812788</span>
</pre></div>
</div>
<p>Within each subgroup, we can apply any operation we have learned
so far: our imagination is the only major limiting factor.
For instance, we can aggregate some columns:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes_agg</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nb">dict</span><span class="p">(</span>
        <span class="n">level</span><span class="o">=</span><span class="n">t</span><span class="o">.</span><span class="n">gender</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>  <span class="c1"># they are all the same here – take first</span>
        <span class="n">height_mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">height</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
        <span class="n">weight_mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">weight</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">nhanes_grp</span>
<span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">nhanes_agg</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1">## {&#39;level&#39;: &#39;female&#39;, &#39;height_mean&#39;: 160.09, &#39;weight_mean&#39;: 78.35}</span>
<span class="nb">print</span><span class="p">(</span><span class="n">nhanes_agg</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="c1">## {&#39;level&#39;: &#39;male&#39;, &#39;height_mean&#39;: 173.76, &#39;weight_mean&#39;: 88.59}</span>
</pre></div>
</div>
<p>The resulting list of dictionaries can be combined to form a data frame:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">nhanes_agg</span><span class="p">)</span>
<span class="c1">##     level  height_mean  weight_mean</span>
<span class="c1">## 0  female       160.09        78.35</span>
<span class="c1">## 1    male       173.76        88.59</span>
</pre></div>
</div>
<p>Furthermore, a simple trick to allow grouping with respect to more than
one column is to apply <strong class="command">numpy.unique</strong> on a string vector
that combines the levels of the grouping variables, e.g.,
by concatenating them like <code class="docutils literal notranslate"><span class="pre">nhanes_srt.gender</span> <span class="pre">+</span> <span class="pre">&quot;___&quot;</span> <span class="pre">+</span> <span class="pre">nhanes_srt.bmicat</span></code>
(assuming that <code class="docutils literal notranslate"><span class="pre">nhanes_srt</span></code> is ordered with respect to these
two criteria).</p>
</div></div></section>
</section>
<section id="plotting-data-in-groups">
<span id="sec-groupby-plot"></span><h2><span class="section-number">12.2. </span>Plotting Data in Groups<a class="headerlink" href="#plotting-data-in-groups" title="Permalink to this heading"></a></h2>
<p>The <strong class="program">seaborn</strong> package is particularly convenient for
plotting grouped data – it is highly interoperable with <strong class="program">pandas</strong>.</p>
<section id="series-of-box-plots">
<h3><span class="section-number">12.2.1. </span>Series of Box Plots<a class="headerlink" href="#series-of-box-plots" title="Permalink to this heading"></a></h3>
<p><a class="reference internal" href="#fig-gender-usborn-bmi"><span class="std std-numref">Figure 12.1</span></a> depicts a box plot with four
boxes side by side:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;bmival&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;usborn&quot;</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">nhanes</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;Paired&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id32">
<span id="fig-gender-usborn-bmi"></span><img alt="../_images/gender-usborn-bmi-1.png" src="../_images/gender-usborn-bmi-1.png" />
<figcaption>
<p><span class="caption-number">Figure 12.1 </span><span class="caption-text">The distribution of BMIs for different genders and countries of birth</span><a class="headerlink" href="#id32" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>Let us contemplate for a while how easy it is now to compare
the BMI distribution in different groups.
Here, we have two grouping variables, as specified
by the <code class="docutils literal notranslate"><span class="pre">y</span></code> and <code class="docutils literal notranslate"><span class="pre">hue</span></code> arguments.</p>
<div class="proof proof-type-exercise" id="id33">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.7</span>
        
    </div><div class="proof-content">
<p>Create a similar series of violin plots.</p>
</div></div><div class="proof proof-type-exercise" id="id34">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.8</span>
        
    </div><div class="proof-content">
<p>(*) Add the average BMIs in each group to the
above box plot using <strong class="command">matplotlib.pyplot.plot</strong>.
Check <code class="docutils literal notranslate"><span class="pre">ylim</span></code> to determine the range on the y-axis.</p>
</div></div></section>
<section id="series-of-bar-plots">
<h3><span class="section-number">12.2.2. </span>Series of Bar Plots<a class="headerlink" href="#series-of-bar-plots" title="Permalink to this heading"></a></h3>
<p>In <a class="reference internal" href="#fig-gender-bmicat"><span class="std std-numref">Figure 12.2</span></a>, on the other hand,
we have a bar plot representing a contingency table
(obtained in a different way than in <a class="reference internal" href="420-categorical.html#chap-categorical"><span class="std std-numref">Chapter 11</span></a>):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span>
    <span class="n">y</span><span class="o">=</span><span class="s2">&quot;counts&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;bmicat&quot;</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;Paired&quot;</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="p">(</span>
        <span class="n">nhanes</span><span class="o">.</span>
        <span class="n">groupby</span><span class="p">([</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="s2">&quot;bmicat&quot;</span><span class="p">])</span><span class="o">.</span>
        <span class="n">size</span><span class="p">()</span><span class="o">.</span>
        <span class="n">rename</span><span class="p">(</span><span class="s2">&quot;counts&quot;</span><span class="p">)</span><span class="o">.</span>
        <span class="n">reset_index</span><span class="p">()</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id35">
<span id="fig-gender-bmicat"></span><img alt="../_images/gender-bmicat-3.png" src="../_images/gender-bmicat-3.png" />
<figcaption>
<p><span class="caption-number">Figure 12.2 </span><span class="caption-text">Number of persons for each gender and BMI category</span><a class="headerlink" href="#id35" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<div class="proof proof-type-exercise" id="id36">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.9</span>
        
    </div><div class="proof-content">
<p>Draw a similar bar plot where the bar heights sum to 100%
for each gender.</p>
</div></div><div class="proof proof-type-exercise" id="id37">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.10</span>
        
    </div><div class="proof-content">
<p>Using the two-sample chi-squared test, verify
whether the BMI category distributions for men and women
differ significantly from each other.</p>
</div></div></section>
<section id="semitransparent-histograms">
<h3><span class="section-number">12.2.3. </span>Semitransparent Histograms<a class="headerlink" href="#semitransparent-histograms" title="Permalink to this heading"></a></h3>
<p><a class="reference internal" href="#fig-hist-transparent-weight-usborn"><span class="std std-numref">Figure 12.3</span></a> illustrates
that playing with semitransparent objects can make comparisons
easy. By passing <code class="docutils literal notranslate"><span class="pre">common_norm=False</span></code>, we scaled each density
histogram separately, which is the behaviour we desire if samples
are of different lengths.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">nhanes</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;usborn&quot;</span><span class="p">,</span>
    <span class="n">element</span><span class="o">=</span><span class="s2">&quot;step&quot;</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">,</span> <span class="n">common_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id38">
<span id="fig-hist-transparent-weight-usborn"></span><img alt="../_images/hist-transparent-weight-usborn-5.png" src="../_images/hist-transparent-weight-usborn-5.png" />
<figcaption>
<p><span class="caption-number">Figure 12.3 </span><span class="caption-text">The weight distribution of the US-born participants has a higher mean and variance</span><a class="headerlink" href="#id38" title="Permalink to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="scatterplots-with-group-information">
<h3><span class="section-number">12.2.4. </span>Scatterplots with Group Information<a class="headerlink" href="#scatterplots-with-group-information" title="Permalink to this heading"></a></h3>
<p>Scatterplots for grouped data can display category information
using points of different colours and/or styles, compare
<a class="reference internal" href="#fig-height-weight-gender"><span class="std std-numref">Figure 12.4</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">nhanes</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">markers</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id39">
<span id="fig-height-weight-gender"></span><img alt="../_images/height-weight-gender-7.png" src="../_images/height-weight-gender-7.png" />
<figcaption>
<p><span class="caption-number">Figure 12.4 </span><span class="caption-text">Weight vs height grouped by gender</span><a class="headerlink" href="#id39" title="Permalink to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="grid-trellis-plots">
<span id="sec-trellis"></span><h3><span class="section-number">12.2.5. </span>Grid (Trellis) Plots<a class="headerlink" href="#grid-trellis-plots" title="Permalink to this heading"></a></h3>
<p>Grid plot (also known as trellis, panel, or lattice plots)
are a way to visualise data separately for each factor level.
All the plots share the same coordinate ranges which makes them easily
comparable. For instance, <a class="reference internal" href="#fig-grid-weight"><span class="std std-numref">Figure 12.5</span></a> depicts a series
of histograms of weights grouped by a combination of two
categorical variables.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span><span class="n">nhanes</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="n">row</span><span class="o">=</span><span class="s2">&quot;usborn&quot;</span><span class="p">)</span>
<span class="n">grid</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">)</span>
<span class="c1"># plt.show()  # not required...</span>
</pre></div>
</div>
<figure class="align-default" id="id40">
<span id="fig-grid-weight"></span><img alt="../_images/grid-weight-9.png" src="../_images/grid-weight-9.png" />
<figcaption>
<p><span class="caption-number">Figure 12.5 </span><span class="caption-text">Distribution of weights for different genders and countries of birth</span><a class="headerlink" href="#id40" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<div class="proof proof-type-exercise" id="id41">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.11</span>
        
    </div><div class="proof-content">
<p>Pass <code class="docutils literal notranslate"><span class="pre">hue=&quot;bmicat&quot;</span></code> additionally to <strong class="command">seaborn.FacetGrid</strong>.</p>
</div></div><div class="admonition important">
<p class="admonition-title">Important</p>
<p>Grid plots can bear any kind of data visualisation we have discussed
so far (e.g., histograms, bar plots, scatterplots).</p>
</div>
<div class="proof proof-type-exercise" id="id42">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.12</span>
        
    </div><div class="proof-content">
<p>Draw a trellis plot with scatterplots of weight vs height
grouped by BMI category and gender.</p>
</div></div></section>
<section id="comparing-ecdfs-with-the-kolmogorovsmirnov-test">
<span id="sec-ks-test2"></span><h3><span class="section-number">12.2.6. </span>Comparing ECDFs with the Kolmogorov–Smirnov Test (*)<a class="headerlink" href="#comparing-ecdfs-with-the-kolmogorovsmirnov-test" title="Permalink to this heading"></a></h3>
<p><a class="reference internal" href="#fig-ecdf-weight-usborn"><span class="std std-numref">Figure 12.6</span></a> compares the empirical
cumulative distribution functions of the weight distributions
for US and non-US born participants.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">usborn</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">nhanes</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;usborn&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="p">:</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">ecdfplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">usborn</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;usborn&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id43">
<span id="fig-ecdf-weight-usborn"></span><img alt="../_images/ecdf-weight-usborn-11.png" src="../_images/ecdf-weight-usborn-11.png" />
<figcaption>
<p><span class="caption-number">Figure 12.6 </span><span class="caption-text">Empirical cumulative distribution functions of weight distributions for different birthplaces</span><a class="headerlink" href="#id43" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>We have used manual splitting of the <code class="docutils literal notranslate"><span class="pre">weight</span></code> column
into subgroups and then plotted the two ECDFs separately,
because a call to
<strong class="command">seaborn.ecdfplot</strong><code class="code docutils literal notranslate"><span class="pre">(data=nhanes,</span> <span class="pre">x=&quot;weight&quot;,</span> <span class="pre">hue=&quot;usborn&quot;)</span></code>
does not honour our wish to use alternating lines styles
(most likely due to a bug).</p>
<div style="margin-top: 1em"></div><p>A two-sample Kolmogorov–Smirnov test can be used
to check whether two ECDFs <span class="math notranslate nohighlight">\(\hat{F}_n'\)</span>
(e.g., the weight of the US-born participants)
and <span class="math notranslate nohighlight">\(\hat{F}_m''\)</span> (e.g., the weight of non-US-born persons)
are significantly different from each other:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left\{
\begin{array}{rll}
H_0: &amp; \hat{F}_n' = \hat{F}_n'' &amp; \text{(null hypothesis)}\\
H_1: &amp; \hat{F}_n' \neq \hat{F}_n'' &amp; \text{(two-sided alternative)} \\
\end{array}
\right.
\end{split}\]</div>
<p>The test statistic will be a variation of the one-sample setting
discussed in <a class="reference internal" href="230-distribution.html#sec-ks-test"><span class="std std-numref">Section 6.2.3</span></a>. Namely, let:</p>
<div class="math notranslate nohighlight">
\[\hat{D}_{n,m} = \sup_{t\in\mathbb{R}} | \hat{F}_n'(t) - \hat{F}_m''(t) |.\]</div>
<p>Computing the above is slightly trickier than
in the previous case<a class="footnote-reference brackets" href="#footintroalgs" id="id3">2</a>,
but luckily an appropriate procedure is already implemented
in <strong class="program">scipy.stats</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x12</span> <span class="o">=</span> <span class="n">nhanes</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;usborn&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">x12</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s2">&quot;yes&quot;</span><span class="p">]</span>  <span class="c1"># first sample</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">x12</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s2">&quot;no&quot;</span><span class="p">]</span>   <span class="c1"># second sample</span>
<span class="n">Dnm</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">ks_2samp</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">Dnm</span>
<span class="c1">## 0.22068075889911914</span>
</pre></div>
</div>
<p>Assuming significance level <span class="math notranslate nohighlight">\(\alpha=0.001\)</span>,
the critical value is approximately (for larger <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(m\)</span>)
equal to:</p>
<div class="math notranslate nohighlight">
\[
K_{n,m} = \sqrt{
-\frac{ \log(\alpha/2) (n+m) }{ 2nm }
}.
\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span><span class="o">+</span><span class="nb">len</span><span class="p">(</span><span class="n">x2</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">x2</span><span class="p">)))</span>
<span class="c1">## 0.04607410479813944</span>
</pre></div>
</div>
<p>As usual, we reject the null hypothesis
when <span class="math notranslate nohighlight">\(\hat{D}_{n,m}\ge K_{n,m}\)</span>, which is exactly the case here
(at significance level <span class="math notranslate nohighlight">\(0.1\%\)</span>).
In other words, weights of US- and non-US-born participants
differ significantly.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Frequentist hypothesis testing only takes into account the deviation
between distributions that is explainable due to sampling effects
(the assumed randomness of the data generation process).
For large sample sizes, even very small deviations<a class="footnote-reference brackets" href="#footerrdev" id="id4">3</a> will be
deemed <em>statistically significant</em>, but it does not mean that we should
consider them as <em>practically significant</em>.
For instance, if a very costly, environmentally unfriendly,
and generally inconvenient for everyone upgrade leads to a process’
improvement such that we reject the null hypothesis stating that two
distributions are equal, but it turns out that the gains are ca. 0.5%,
the good old common sense should be applied.</p>
</div>
<div class="proof proof-type-exercise" id="id44">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.13</span>
        
    </div><div class="proof-content">
<p>Compare between the ECDFs of weights
of men and women who are between 18 and 25 years old.
Determine whether they are significantly different.</p>
</div></div><div class="admonition important">
<p class="admonition-title">Important</p>
<p>Some statistical textbooks and many research papers in the social
sciences (amongst many others) employ the significance level
of <span class="math notranslate nohighlight">\(\alpha=5\%\)</span>, which is often criticised as too high<a class="footnote-reference brackets" href="#footpvalue" id="id5">4</a>.
Many stakeholders aggressively push towards
constant improvements in terms of inventing bigger, better, faster,
more efficient things. In this context, larger <span class="math notranslate nohighlight">\(\alpha\)</span> allows for
generating more <em>sensational</em> discoveries. This is because it considers
smaller differences as already significant.
This all adds to what we call the reproducibility crisis
in the empirical sciences.</p>
<p>We, on the other hand, claim that it is better to err on the side
of being cautious. This, in the long run, is more sustainable.</p>
</div>
</section>
<section id="comparing-quantiles">
<h3><span class="section-number">12.2.7. </span>Comparing Quantiles<a class="headerlink" href="#comparing-quantiles" title="Permalink to this heading"></a></h3>
<p>Plotting quantiles in two samples against each other
can also give us some further (informal) insight with regard to the
possible distributional differences. <a class="reference internal" href="#fig-qqplot2"><span class="std std-numref">Figure 12.7</span></a> depicts an example
Q-Q plot (see also the one-sample version in <a class="reference internal" href="230-distribution.html#sec-qqplot"><span class="std std-numref">Section 6.2.2</span></a>),
where we see that the distributions have similar shapes (points
more or less lie on a straight line), but they are shifted and/or
scaled (if they were, they would be on the identity line).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">nhanes</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">nhanes</span><span class="o">.</span><span class="n">usborn</span> <span class="o">==</span> <span class="s2">&quot;yes&quot;</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">nhanes</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">nhanes</span><span class="o">.</span><span class="n">usborn</span> <span class="o">==</span> <span class="s2">&quot;no&quot;</span><span class="p">]</span>
<span class="n">xd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">yd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">xd</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">yd</span><span class="p">):</span>  <span class="c1"># interpolate between quantiles in a longer sample</span>
    <span class="n">xd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">xd</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">yd</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">yd</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">yd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">yd</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">xd</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">xd</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xd</span><span class="p">,</span> <span class="n">yd</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axline</span><span class="p">((</span><span class="n">xd</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">xd</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="n">xd</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">xd</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span><span class="p">]),</span> <span class="n">slope</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>  <span class="c1"># identity line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample quantiles (weight; usborn=yes)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample quantiles (weight; usborn=no)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id45">
<span id="fig-qqplot2"></span><img alt="../_images/qqplot2-13.png" src="../_images/qqplot2-13.png" />
<figcaption>
<p><span class="caption-number">Figure 12.7 </span><span class="caption-text">A two-sample Q-Q plot</span><a class="headerlink" href="#id45" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>Notice that we interpolated between the quantiles in a larger sample
to match the length of the shorter vector.</p>
</section>
</section>
<section id="classification-tasks">
<h2><span class="section-number">12.3. </span>Classification Tasks<a class="headerlink" href="#classification-tasks" title="Permalink to this heading"></a></h2>
<p>Let us consider a small sample of white, rather sweet wines
from a much larger <a class="reference external" href="http://archive.ics.uci.edu/ml/datasets/Wine+Quality">wine quality</a> dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">wine_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
        <span class="s2">&quot;teaching-data/master/other/sweetwhitewine_train2.csv&quot;</span><span class="p">,</span>
        <span class="n">comment</span><span class="o">=</span><span class="s2">&quot;#&quot;</span><span class="p">)</span>
<span class="n">wine_train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<span class="c1">##      alcohol      sugar  bad</span>
<span class="c1">## 0  10.625271  10.340159    0</span>
<span class="c1">## 1   9.066111  18.593274    1</span>
<span class="c1">## 2  10.806395   6.206685    0</span>
<span class="c1">## 3  13.432876   2.739529    0</span>
<span class="c1">## 4   9.578162   3.053025    0</span>
</pre></div>
</div>
<p>We are given each wine’s alcohol and residual sugar content,
as well as a binary categorical variable stating whether a group of
sommeliers deem a given beverage quite bad (1) or not (0).
<a class="reference internal" href="#fig-wines"><span class="std std-numref">Figure 12.8</span></a> reveals
that subpar wines are rather low in… alcohol and, to some extent, sugar.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;alcohol&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;sugar&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">wine_train</span><span class="p">,</span>
    <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;bad&quot;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;bad&quot;</span><span class="p">,</span> <span class="n">markers</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;alcohol&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;sugar&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;bad&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id46">
<span id="fig-wines"></span><img alt="../_images/wines-15.png" src="../_images/wines-15.png" />
<figcaption>
<p><span class="caption-number">Figure 12.8 </span><span class="caption-text">Scatterplot for sugar vs alcohol content for white, rather sweet wines, and whether they are considered bad (1) or drinkable (0) by some experts</span><a class="headerlink" href="#id46" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>Someone answer the door! We have a delivery:
quite a few new wine bottles whose alcohol and sugar contents have,
luckily, been given on their respective labels.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">wine_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
        <span class="s2">&quot;teaching-data/master/other/sweetwhitewine_train2.csv&quot;</span><span class="p">,</span>
        <span class="n">comment</span><span class="o">=</span><span class="s2">&quot;#&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">wine_test</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<span class="c1">##      alcohol      sugar</span>
<span class="c1">## 0  10.625271  10.340159</span>
<span class="c1">## 1   9.066111  18.593274</span>
<span class="c1">## 2  10.806395   6.206685</span>
<span class="c1">## 3  13.432876   2.739529</span>
<span class="c1">## 4   9.578162   3.053025</span>
</pre></div>
</div>
<p>We would like to determine which of the wines from the test
set might be not-bad without asking an expert for their opinion.
In other words, we would like to exercise a <em>classification</em> task
(see, e.g., <span id="id6">[<a class="reference internal" href="999-bibliography.html#id53" title="C. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag, 2006. URL: https://www.microsoft.com/en-us/research/people/cmbishop/.">Bis06</a>, <a class="reference internal" href="999-bibliography.html#id52" title="T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer-Verlag, 2017. URL: https://hastie.su.domains/ElemStatLearn/.">HTF17</a>]</span>). More formally:</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Assume we are given a set of training points
<span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{n\times m}\)</span>
and the corresponding reference outputs
<span class="math notranslate nohighlight">\(\boldsymbol{y}\in\{L_1,L_2,\dots,L_l\}^n\)</span> in the form of a categorical
variable with <em>l</em> distinct levels.
The aim of a <em>classification</em> algorithm is to predict
what the outputs for each point from a possibly different
dataset <span class="math notranslate nohighlight">\(\mathbf{X}'\in\mathbb{R}^{n'\times m}\)</span>, i.e.,
<span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}}'\in\{L_1,L_2,\dots,L_l\}^{n'}\)</span>, might be.</p>
</div>
<p>In other words, we are asked to fill the gaps in a categorical
variable.
Recall that in a regression problem (<a class="reference internal" href="330-relationship.html#sec-regression"><span class="std std-numref">Section 9.2</span></a>),
the reference outputs were numerical.</p>
<div class="proof proof-type-exercise" id="id47">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.14</span>
        
    </div><div class="proof-content">
<p>Which of the following are instances of classification problems
and which are regression tasks?</p>
<ul class="simple">
<li><p>Detect email spam.</p></li>
<li><p>Predict a market stock price (good luck with that).</p></li>
<li><p>Assess credit risk.</p></li>
<li><p>Detect tumour tissues in medical images.</p></li>
<li><p>Predict time-to-recovery of cancer patients.</p></li>
<li><p>Recognise smiling faces on photographs (kind of creepy).</p></li>
<li><p>Detect unattended luggage in airport security camera footage.</p></li>
</ul>
<p>What kind of data should you gather to tackle them?</p>
</div></div><section id="k-nearest-neighbour-classification">
<span id="sec-knn-classification"></span><h3><span class="section-number">12.3.1. </span><em>K</em>-Nearest Neighbour Classification<a class="headerlink" href="#k-nearest-neighbour-classification" title="Permalink to this heading"></a></h3>
<p>One of the simplest approaches to classification –
good enough for such an introductory course –
is based on the information about a test point’s nearest neighbours
living in the training sample; compare <a class="reference internal" href="320-transform-matrix.html#sec-knn-vs-radius"><span class="std std-numref">Section 8.4.4</span></a>.</p>
<p>Fix <span class="math notranslate nohighlight">\(k\ge 1\)</span>.
Namely, to classify some <span class="math notranslate nohighlight">\(\boldsymbol{x}'\in\mathbb{R}^m\)</span>:</p>
<ol class="arabic">
<li><p>Find the indexes <span class="math notranslate nohighlight">\(N_k(\boldsymbol{x}')=\{i_1,\dots,i_k\}\)</span>
of the <span class="math notranslate nohighlight">\(k\)</span> points from
<span class="math notranslate nohighlight">\(\mathbf{X}\)</span> closest to <span class="math notranslate nohighlight">\(\boldsymbol{x}'\)</span>, i.e., ones that fulfil
for all <span class="math notranslate nohighlight">\(j\not\in\{i_1,\dots,i_k\}\)</span>:</p>
<div class="math notranslate nohighlight">
\[ \|\mathbf{x}_{i_1,\cdot}-\boldsymbol{x}'\|
    \le\dots\le
    \| \mathbf{x}_{i_k,\cdot} -\boldsymbol{x}' \|
    \le
    \| \mathbf{x}_{j,\cdot} -\boldsymbol{x}' \|.
    \]</div>
</li>
<li><p>Classify <span class="math notranslate nohighlight">\(\boldsymbol{x}'\)</span>
as <span class="math notranslate nohighlight">\(\hat{y}'=\mathrm{mode}(y_{i_1},\dots,y_{i_k})\)</span>,
i.e., assign it the label that most frequently occurs
amongst its <span class="math notranslate nohighlight">\(k\)</span> nearest neighbours.
If a mode is nonunique, resolve the ties, for example, at random.</p></li>
</ol>
<p>It is thus a similar algorithm to
<em>k</em>-nearest neighbour regression (<a class="reference internal" href="330-relationship.html#sec-knn-regression"><span class="std std-numref">Section 9.2.1</span></a>).
We only replaced the <em>quantitative</em> mean with the <em>qualitative</em> mode.</p>
<p>This is a variation on the theme: if you don’t know what to do
in a given situation, try to mimic what most of the other people around you
are doing. Or, if you don’t know what to think about a particular wine,
but amongst the 5 similar ones (in terms of alcohol and sugar content)
three were said to be awful, say that you don’t like it because it’s
not sweet enough. Thanks to this, others will take you for a very
refined wine taster.</p>
<p>Let us apply a 5-nearest neighbour classifier on the standardised version
of the dataset: as we are about to use a technique based on pairwise
distances, it would be best if the variables were on the same scale.
Thus, we first compute the z-scores for the training set:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">wine_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;alcohol&quot;</span><span class="p">,</span> <span class="s2">&quot;sugar&quot;</span><span class="p">]])</span>
<span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">sds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">Z_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span><span class="o">-</span><span class="n">means</span><span class="p">)</span><span class="o">/</span><span class="n">sds</span>
</pre></div>
</div>
<p>Then, we determine the z-scores for the test set:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Z_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">wine_test</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;alcohol&quot;</span><span class="p">,</span> <span class="s2">&quot;sugar&quot;</span><span class="p">]])</span><span class="o">-</span><span class="n">means</span><span class="p">)</span><span class="o">/</span><span class="n">sds</span>
</pre></div>
</div>
<p>Let us stress that we referred to the aggregates computed
for the training set. This is a good example of a situation where
we cannot simply use a built-in method from <strong class="command">pandas</strong>.
Instead, we apply what we have learned about <strong class="command">numpy</strong>.</p>
<p>To make the predictions, we will use the following function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">knn_class</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">nnis</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">spatial</span><span class="o">.</span><span class="n">KDTree</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">k</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">nnls</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">nnis</span><span class="p">]</span>  <span class="c1"># same as: y_train[nnis.reshape(-1)].reshape(-1, k)</span>
    <span class="k">return</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="n">nnls</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>First, we fetched the indexes of each test point’s nearest neighbours
(amongst the points in the training set).
Then, we read their corresponding labels; they are stored
in a matrix with <span class="math notranslate nohighlight">\(k\)</span> columns.
Finally, we computed the modes in each row.
As a consequence, we have each point in the test set
classified.</p>
<p>And now:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">wine_train</span><span class="o">.</span><span class="n">bad</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">knn_class</span><span class="p">(</span><span class="n">Z_test</span><span class="p">,</span> <span class="n">Z_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
<span class="n">y_pred</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>  <span class="c1"># preview</span>
<span class="c1">## array([0, 1, 0, 0, 1, 0, 0, 0, 0, 0])</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unfortunately, <strong class="command">scipy.stats.mode</strong> does not resolve the possible
ties at random. Nevertheless, in our case, <span class="math notranslate nohighlight">\(k\)</span> is odd and the number
of possible classes is <span class="math notranslate nohighlight">\(l=2\)</span>. In this setting, the mode is always unique.</p>
</div>
<p><a class="reference internal" href="#fig-knn-class"><span class="std std-numref">Figure 12.9</span></a> shows how nearest neighbour classification
categorises different regions of a section of the two-dimensional
plane. The greater the <span class="math notranslate nohighlight">\(k\)</span>, the smoother the decision boundaries.
Naturally, in regions corresponding to few training points, we do not
expect the classification accuracy to be good enough<a class="footnote-reference brackets" href="#football" id="id7">5</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Z_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">Z_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Z_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">Z_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">xg1</span><span class="p">,</span> <span class="n">xg2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
<span class="n">Xg12</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">xg1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">xg2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
<span class="n">ks</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">25</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ks</span><span class="p">)):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ks</span><span class="p">),</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">yg12</span> <span class="o">=</span> <span class="n">knn_class</span><span class="p">(</span><span class="n">Xg12</span><span class="p">,</span> <span class="n">Z_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">ks</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Z_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Z_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">c</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Z_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Z_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">c</span><span class="o">=</span><span class="s2">&quot;#DF536B&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;v&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">yg12</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x2</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)),</span>
        <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gist_heat&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;$k=</span><span class="si">{</span><span class="n">ks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">$&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;alcohol&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;sugar&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id48">
<span id="fig-knn-class"></span><img alt="../_images/knn-class-17.png" src="../_images/knn-class-17.png" />
<figcaption>
<p><span class="caption-number">Figure 12.9 </span><span class="caption-text"><em>k</em>-nearest neighbour classification of a whole, dense, two-dimensional grid of points for different <span class="math notranslate nohighlight">\(k\)</span></span><a class="headerlink" href="#id48" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<div class="proof proof-type-example" id="id49">

    <div class="proof-title">
        <span class="proof-type">Example 12.15</span>
        
    </div><div class="proof-content">
<p>(*) The same with the <strong class="program">scikit-learn</strong> package:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sklearn.neighbors</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">neighbors</span><span class="o">.</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Z_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred2</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Z_test</span><span class="p">)</span>
</pre></div>
</div>
<p>We can verify that the results are identical to the ones above
by calling:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">y_pred2</span> <span class="o">==</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="c1">## True</span>
</pre></div>
</div>
</div></div></section>
<section id="assessing-the-quality-of-predictions">
<span id="sec-classifier-metrics"></span><h3><span class="section-number">12.3.2. </span>Assessing the Quality of Predictions<a class="headerlink" href="#assessing-the-quality-of-predictions" title="Permalink to this heading"></a></h3>
<p>It is time to reveal the truth: our test wines, it turns out,
have already been assessed by some experts.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
        <span class="s2">&quot;teaching-data/master/other/sweetwhitewine_train2.csv&quot;</span><span class="p">,</span>
        <span class="n">comment</span><span class="o">=</span><span class="s2">&quot;#&quot;</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">bad</span><span class="p">)</span>
<span class="n">y_test</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>  <span class="c1"># preview</span>
<span class="c1">## array([0, 1, 0, 0, 0, 1, 0, 0, 0, 1])</span>
</pre></div>
</div>
<div style="margin-top: 1em"></div><p>The <em>accuracy</em> score is the most straightforward measure of the similarity
between these true labels (denoted <span class="math notranslate nohighlight">\(\boldsymbol{y}'=(y_1',\dots,y_{n'}')\)</span>)
and the ones predicted by the classifier
(denoted <span class="math notranslate nohighlight">\(\hat{\boldsymbol{y}}'=(\hat{y}_1',\dots,\hat{y}_{n'}')\)</span>).
It is defined as a ratio between the correctly classified instances
and all the instances:</p>
<div class="math notranslate nohighlight">
\[
\text{Accuracy}(\boldsymbol{y}', \hat{\boldsymbol{y}}') =
\frac{\sum_{i=1}^{n'} \mathbf{1}(y_i' = \hat{y}_i')}{n'},
\]</div>
<p>where the <em>indicator function</em>
<span class="math notranslate nohighlight">\(\mathbf{1}(y_i' = \hat{y}_i')=1\)</span> if and only if
<span class="math notranslate nohighlight">\(y_i' = \hat{y}_i'\)</span> and <span class="math notranslate nohighlight">\(0\)</span> otherwise.</p>
<p>Computing the above for our test sample gives:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_test</span> <span class="o">==</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="c1">## 0.788</span>
</pre></div>
</div>
<p>Thus, 79% of the wines were correctly classified with regard to their
true quality. Before we get too enthusiastic,
let us note that our dataset is slightly <em>imbalanced</em>
in terms of the distribution of label counts:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>  <span class="c1"># contingency table</span>
<span class="c1">## 0    639</span>
<span class="c1">## 1    361</span>
<span class="c1">## dtype: int64</span>
</pre></div>
</div>
<p>It turns out that the majority of the wines (639 out of 1,000)
in our sample are <em>truly</em> good.
Notice that a dummy classifier which labels <em>all</em> the wines as great
would have accuracy of ca. 64%. Our <em>k</em>-nearest neighbour approach
to wine quality assessment is not that usable after all.</p>
<div style="margin-top: 1em"></div><p>It is therefore always beneficial to analyse the corresponding
<em>confusion matrix</em>, which is a two-way contingency table
summarising the correct decisions and errors we make.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="nb">dict</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="o">=</span><span class="n">y_test</span><span class="p">)</span>
<span class="p">)</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">fill_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">C</span>
<span class="c1">## y_test    0    1</span>
<span class="c1">## y_pred          </span>
<span class="c1">## 0       548  121</span>
<span class="c1">## 1        91  240</span>
</pre></div>
</div>
<div style="margin-top: 1em"></div><p>In the binary classification case (<span class="math notranslate nohighlight">\(l=2\)</span>) such as this one,
its entries are usually referred to as (see also the table below):</p>
<ul class="simple">
<li><p>TN – the number of cases where the true <span class="math notranslate nohighlight">\(y_i'=0\)</span> and
the predicted <span class="math notranslate nohighlight">\(\hat{y}_i'=0\)</span> (true negative),</p></li>
<li><p>TP – the number of instances such that the true <span class="math notranslate nohighlight">\(y_i'=1\)</span> and
the predicted <span class="math notranslate nohighlight">\(\hat{y}_i'=1\)</span> (true positive),</p></li>
<li><p>FN – how many times the true <span class="math notranslate nohighlight">\(y_i'=1\)</span> but
the predicted <span class="math notranslate nohighlight">\(\hat{y}_i'=0\)</span> (false negative),</p></li>
<li><p>FN – how many times the true <span class="math notranslate nohighlight">\(y_i'=0\)</span> but the predicted <span class="math notranslate nohighlight">\(\hat{y}_i'=1\)</span>
(false positive).</p></li>
</ul>
<p>The terms <em>positive</em> and <em>negative</em> refer to
the output predicted by a classifier, i.e., they indicate whether some
<span class="math notranslate nohighlight">\(\hat{y}_i'\)</span> is equal to 1 and 0, respectively.</p>
<table class="colwidths-auto docutils align-default" id="id50">
<caption><span class="caption-number">Table 12.1 </span><span class="caption-text">The different cases of true vs predicted labels in a binary classification task <span class="math notranslate nohighlight">\((l=2)\)</span></span><a class="headerlink" href="#id50" title="Permalink to this table"></a></caption>
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(y_i'=0\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(y_i'=1\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\hat{y}_i'=0\)</span></p></td>
<td><p><strong>True Negative</strong></p></td>
<td><p>False Negative (Type II error)</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(\hat{y}_i'=1\)</span></p></td>
<td><p>False Positive (Type I error)</p></td>
<td><p><strong>True Positive</strong></p></td>
</tr>
</tbody>
</table>
<p>Ideally, the number of false positives and false negatives
should be as low as possible. The accuracy score only takes the raw
number of true negatives (TN) and true positives (TP)
into account:</p>
<div class="math notranslate nohighlight">
\[
\text{Accuracy}(\boldsymbol{y}', \hat{\boldsymbol{y}}') =
\frac{\text{TN}+\text{TP}}{\text{TN}+\text{TP}+\text{FN}+\text{FP}}.
\]</div>
<p>Consequently, it might not be a good metric in imbalanced
classification problems.</p>
<p>There are, fortunately, some more meaningful measures in the case where
class 1 is less prevalent and where mispredicting it is considered
more hazardous than making an inaccurate prediction with respect to class 0.
This is because most will agree that it is better to be surprised
by a vino mislabelled as bad, than be disappointed with
a highly recommended product where we have already
built some expectations around it.
Further, not getting diagnosed as having COVID-19 where we are genuinely
sick can be more dangerous for the people around us than
being asked to stay at home with nothing but a headache.</p>
<div style="margin-top: 1em"></div><p><em>Precision</em> answers the question: If the classifier outputs 1,
what is the probability that this is indeed true?</p>
<div class="math notranslate nohighlight">
\[
\text{Precision}(\boldsymbol{y}', \hat{\boldsymbol{y}}') =
\frac{\text{TP}}{\text{TP}+\text{FP}}
=
\frac{\sum_{i=1}^{n'} y_i' \hat{y}_i'}{\sum_{i=1}^{n'} \hat{y}_i'}
.
\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>  <span class="c1"># convert to matrix</span>
<span class="n">C</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">C</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># precision</span>
<span class="c1">## 0.7250755287009063</span>
<span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_test</span><span class="o">*</span><span class="n">y_pred</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>  <span class="c1"># equivalently</span>
<span class="c1">## 0.7250755287009063</span>
</pre></div>
</div>
<p>When a classifier labels a vino as bad, in 73% of cases
it is veritably undrinkable.</p>
<div style="margin-top: 1em"></div><p><em>Recall</em> (sensitivity, hit rate, or true positive rate)
addresses the question:
If the true class is 1, what is the probability that the classifier
will detect it?</p>
<div class="math notranslate nohighlight">
\[
\text{Recall}(\boldsymbol{y}', \hat{\boldsymbol{y}}') =
\frac{\text{TP}}{\text{TP}+\text{FN}}
=
\frac{\sum_{i=1}^{n'} y_i' \hat{y}_i'}{\sum_{i=1}^{n'} {y}_i'}.
\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">C</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">C</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># recall</span>
<span class="c1">## 0.6648199445983379</span>
<span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_test</span><span class="o">*</span><span class="n">y_pred</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>  <span class="c1"># equivalently</span>
<span class="c1">## 0.6648199445983379</span>
</pre></div>
</div>
<p>Only 66% of the really bad wines will be filtered out
by the classifier.</p>
<div style="margin-top: 1em"></div><p><em>F-measure</em> (or <span class="math notranslate nohighlight">\(F_1\)</span>-measure), is the harmonic<a class="footnote-reference brackets" href="#foothar" id="id8">6</a> mean of
precision and recall in the case where we would rather have them
aggregated into a single number:</p>
<div class="math notranslate nohighlight">
\[
\text{F}(\boldsymbol{y}', \hat{\boldsymbol{y}}') = \frac{1}{
    \frac{
        \frac{1}{\text{Precision}}+\frac{1}{\text{Recall}}
    }{2}
}
=
\left(
\frac{1}{2}
\left(
\text{Precision}^{-1}+\text{Recall}^{-1}
\right)
\right)^{-1}
=
\frac{\text{TP}}{\text{TP} + \frac{\text{FP} + \text{FN}}{2}}.
\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">C</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mf">0.5</span><span class="o">*</span><span class="n">C</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mf">0.5</span><span class="o">*</span><span class="n">C</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># F</span>
<span class="c1">## 0.6936416184971098</span>
</pre></div>
</div>
<p>Overall, we can conclude that our classifier is rather weak.</p>
<div class="proof proof-type-exercise" id="id51">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.16</span>
        
    </div><div class="proof-content">
<p>Would you use precision or recall in each of the following settings?</p>
<ul class="simple">
<li><p>Medical diagnosis,</p></li>
<li><p>medical screening,</p></li>
<li><p>suggestions of potential matches in a dating app,</p></li>
<li><p>plagiarism detection,</p></li>
<li><p>wine recommendation.</p></li>
</ul>
</div></div></section>
<section id="splitting-into-training-and-test-sets">
<span id="sec-train-test-split"></span><h3><span class="section-number">12.3.3. </span>Splitting into Training and Test Sets<a class="headerlink" href="#splitting-into-training-and-test-sets" title="Permalink to this heading"></a></h3>
<p>The training set was used as a source of knowledge about our
problem domain. The <em>k</em>-nearest neighbour classifier is technically
<em>model-free</em>. As a consequence, to generate a new prediction, we need
to be able to query all the points in the database every time.</p>
<p>Nonetheless, most statistical/machine learning algorithms, by construction,
generalise the patterns discovered in the dataset in the form
of mathematical functions (oftentimes, very complicated ones),
that are fitted by minimising some error metric.
Linear regression analysis by means of the least squares approximation
uses exactly this kind of approach.
Logistic regression for a binary response variable
would be a conceptually similar classifier, but it is beyond
our introductory course.</p>
<p>Either way, we used a separate <em>test set</em> to verify the quality
of our classifier on so-far <em>unobserved</em> data, i.e., its <em>predictive</em>
capabilities. We do not want our model to fit to the
training data too closely. This could lead to its being completely useless
when filling the gaps between the points it was exposed to.
This is like being a student who
can only repeat what the teacher says, and when faced with a slightly
different real-world problem, they panic and say complete gibberish.</p>
<p>In the above example, the training and test sets were created by yours truly.
Still, normally, it is the data scientist who splits a single data frame
into two parts themself; see <a class="reference internal" href="410-data-frame.html#sec-random-sampling"><span class="std std-numref">Section 10.5.3</span></a>.
This way, they can <em>mimic</em> the situation
where some <em>test</em> observations become available after the learning
phase is complete.</p>
</section>
<section id="validating-many-models-parameter-selection">
<span id="sec-model-validation"></span><h3><span class="section-number">12.3.4. </span>Validating Many Models (Parameter Selection) (*)<a class="headerlink" href="#validating-many-models-parameter-selection" title="Permalink to this heading"></a></h3>
<p>In statistical modelling, there usually are many
<em>hyperparameters</em> that should be tweaked. For example:</p>
<ul class="simple">
<li><p>which independent variables should be used for model building,</p></li>
<li><p>how they should be preprocessed; e.g., which of them should be
standardised,</p></li>
<li><p>if an algorithm has some tunable parameters, what is the best combination
thereof; for instance, which <span class="math notranslate nohighlight">\(k\)</span> should we use in the <em>k</em>-nearest
neighbours search.</p></li>
</ul>
<p>At initial stages of data analysis, we usually tune them up by trial and
error. Later, but this is already beyond the scope of this introductory
course, we are used to exploring all the possible combinations thereof
(exhaustive grid search) or making use of some local search-based
heuristics (e.g., greedy optimisers such as hill climbing).</p>
<p>These always involve verifying the performance of <em>many</em> different classifiers,
for example, 1-, 3-, 9, and 15-nearest neighbours-based ones.
For each of them, we need to compute separate quality metrics,
e.g., F-measures. Then, the classifier which yields the highest
score is picked as the best.
Unfortunately, if we do it recklessly,
this can lead to <em>overfitting</em>, this time with respect to the test set.
The obtained metrics might be too optimistic and can poorly reflect
the real performance of the solution on future data.</p>
<p>Assuming that our dataset carries a decent number of observations,
to overcome this problem, we can perform a random
<em>training/validation/test split</em>:</p>
<ul class="simple">
<li><p><em>training sample</em>  (e.g., 60% of randomly chosen rows) –
for model construction,</p></li>
<li><p><em>validation sample</em> (e.g., 20%) –
used to tune the hyperparameters of many classifiers and to choose
the best one,</p></li>
<li><p><em>test (hold-out) sample</em> (e.g., the remaining 20%) – used to assess
the goodness of fit of the best classifier.</p></li>
</ul>
<p>This common-sense approach is not limited to classification.
We can validate different regression
models in the same way.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>We would like  to obtain a good estimate of a classifier’s performance on
previously unobserved data. For this reason, the test (hold-out) sample
must neither be used in the training nor the validation phase.</p>
</div>
<div class="proof proof-type-exercise" id="id52">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.17</span>
        
    </div><div class="proof-content">
<p>Determine the <em>best</em> parameter setting
for the <em>k</em>-nearest neighbour classification of the <code class="docutils literal notranslate"><span class="pre">color</span></code> variable
based on standardised versions of
some physicochemical features (chosen columns) of wines
in the <a class="reference external" href="https://github.com/gagolews/teaching-data/raw/master/other/wine_quality_all.csv"><code class="docutils literal notranslate"><span class="pre">wine_quality_all</span></code></a> dataset.
Create a 60/20/20% dataset split.
For each <span class="math notranslate nohighlight">\(k=1, 3, 5, 7, 9\)</span>, compute the corresponding
F-measure on the validation test.
Evaluate the quality of the best classifier on the test set.</p>
</div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>(*)
Instead of a training/validation/test split,
we can use various <em>cross-validation</em> techniques, especially
on smaller datasets.
For instance, in a <em>5-fold cross-validation</em>, we split the original
training set randomly into five disjoint parts: <span class="math notranslate nohighlight">\(A, B, C, D, E\)</span>
(more or less of the same size). We use each
combination of four chunks as training sets and the remaining part
as the validation set, for which we generate the predictions and then
compute, say, the F-measure:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>training set</p></th>
<th class="head"><p>validation set</p></th>
<th class="head"><p>F-measure</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(B\cup C\cup D\cup E\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(A\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(F_A\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(A\cup C\cup D\cup E\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(B\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(F_B\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(A\cup B\cup D\cup E\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(C\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(F_C\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(A\cup B\cup C\cup E\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(D\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(F_D\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(A\cup B\cup C\cup D\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(E\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(F_E\)</span></p></td>
</tr>
</tbody>
</table>
<p>In the end, we can determine the average F-measure,
<span class="math notranslate nohighlight">\((F_A+F_B+F_C+F_D+F_E)/5\)</span>, as a basis for assessing different
classifiers’ quality.</p>
<p>Once the best classifier is chosen, we can use the whole
training sample to fit the final model and then consider the separate
test sample to assess its quality.</p>
<p>Furthermore, for highly imbalanced labels, some form of stratified
sampling might be necessary.
Such problems are typically explored in more advanced courses in
statistical learning.</p>
</div>
<div class="proof proof-type-exercise" id="id53">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.18</span>
        
    </div><div class="proof-content">
<p>(**)
Redo the above exercise (assessing the wine colour classifiers),
but this time maximise the F-measure
obtained by a 5-fold cross-validation.</p>
</div></div></section>
</section>
<section id="clustering-tasks">
<span id="sec-kmeans"></span><h2><span class="section-number">12.4. </span>Clustering Tasks<a class="headerlink" href="#clustering-tasks" title="Permalink to this heading"></a></h2>
<p>So far, we have been implicitly assuming that either
each dataset comes from a single homogeneous distribution,
or we have a categorical variable that naturally defines the
groups that we can split the dataset into.
Nevertheless, it might be the case that we are given a sample coming from
a distribution mixture, where some subsets behave differently, but a
grouping variable has not been provided at all (e.g., we have height
and weight data but no information about the subjects’ sexes).</p>
<p><em>Clustering</em> (also known as segmentation or quantisation;
see, e.g., <span id="id9">[<a class="reference internal" href="999-bibliography.html#id106" title="S.T. Wierzchoń and M.A. Kłopotek. Modern Algorithms for Cluster Analysis. Springer, 2018. doi:10.1007/978-3-319-69308-8.">WK18</a>]</span>) methods
can be used to partition a dataset
into groups based only on the spatial structure of the points’
relative densities.
In the <em>k</em>-means method, which we discuss below,
the cluster structure is determined based on
the points’ proximity to <span class="math notranslate nohighlight">\(k\)</span> carefully chosen group centroids;
compare <a class="reference internal" href="320-transform-matrix.html#sec-centroid"><span class="std std-numref">Section 8.4.2</span></a>.</p>
<section id="k-means-method">
<h3><span class="section-number">12.4.1. </span><em>K</em>-Means Method<a class="headerlink" href="#k-means-method" title="Permalink to this heading"></a></h3>
<p>Fix <span class="math notranslate nohighlight">\(k \ge 2\)</span>.
In the <em>k</em>-means method<a class="footnote-reference brackets" href="#footkmeans" id="id10">7</a>, we seek <span class="math notranslate nohighlight">\(k\)</span> pivot points,
<span class="math notranslate nohighlight">\(\boldsymbol{c}_1, \boldsymbol{c}_2,\dots,\boldsymbol{c}_k\in\mathbb{R}^m\)</span>,
such that the sum of squared distances between the input points
in <span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{n\times m}\)</span>
and their closest pivots is minimised:</p>
<div class="math notranslate nohighlight">
\[
\text{minimise}\ \sum_{i=1}^n
\min\left\{
\| \mathbf{x}_{i,\cdot} - \boldsymbol{c}_{1} \|^2,
\| \mathbf{x}_{i,\cdot} - \boldsymbol{c}_{2} \|^2,
\dots,
\| \mathbf{x}_{i,\cdot} - \boldsymbol{c}_{k} \|^2
\right\}
\qquad\text{w.r.t. }{\boldsymbol{c}_1, \boldsymbol{c}_2,\dots,\boldsymbol{c}_k}.
\]</div>
<p>Let us introduce the <em>label vector</em> <span class="math notranslate nohighlight">\(\boldsymbol{l}\)</span> such that:</p>
<div class="math notranslate nohighlight">
\[
l_i = \mathrm{arg}\min_{j} \| \mathbf{x}_{i,\cdot} - \boldsymbol{c}_{j} \|^2,
\]</div>
<p>i.e., it is the index of the pivot closest to <span class="math notranslate nohighlight">\(\mathbf{x}_{i,\cdot}\)</span>.</p>
<p>We will consider all the points <span class="math notranslate nohighlight">\(\mathbf{x}_{i,\cdot}\)</span>
with <span class="math notranslate nohighlight">\(i\)</span> such that <span class="math notranslate nohighlight">\(l_i=j\)</span> as belonging to the same, <span class="math notranslate nohighlight">\(j\)</span>-th,
<em>cluster</em> (point group). This way <span class="math notranslate nohighlight">\(\boldsymbol{l}\)</span> defines
a <em>partition</em> of the original dataset
into <span class="math notranslate nohighlight">\(k\)</span> nonempty, mutually disjoint subsets.</p>
<p>Now, the above optimisation task can be equivalently rewritten as:</p>
<div class="math notranslate nohighlight">
\[
\text{minimise}\ \sum_{i=1}^n \| \mathbf{x}_{i,\cdot} - \boldsymbol{c}_{l_i} \|^2
\qquad\text{w.r.t. }{\boldsymbol{c}_1, \boldsymbol{c}_2,\dots,\boldsymbol{c}_k}.
\]</div>
<p>And this is why we refer to the above objective function
as the (total) <em>within-cluster sum of squares</em> (WCSS).
This problem looks easier, but let us not be tricked;
<span class="math notranslate nohighlight">\(l_i\)</span>s depend on <span class="math notranslate nohighlight">\(\boldsymbol{c}_j\)</span>s. They vary <em>together</em>.
We have just made it less explicit.</p>
<p>It can be shown that given a fixed label vector
<span class="math notranslate nohighlight">\(\boldsymbol{l}\)</span> representing a partition,
<span class="math notranslate nohighlight">\(\boldsymbol{c}_j\)</span> must be the centroid (<a class="reference internal" href="320-transform-matrix.html#sec-centroid"><span class="std std-numref">Section 8.4.2</span></a>)
of the points assigned thereto:</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{c}_j = \frac{1}{n_j} \sum_{i: l_i=j} \mathbf{x}_{i,\cdot},
\]</div>
<p>where <span class="math notranslate nohighlight">\(n_j=|\{i: l_i=j\}|\)</span> gives the number of
<span class="math notranslate nohighlight">\(i\)</span>s such that <span class="math notranslate nohighlight">\(l_i=j\)</span>, i.e., the size of the <span class="math notranslate nohighlight">\(j\)</span>-th cluster.</p>
<div style="margin-top: 1em"></div><p>Here is an example dataset (see below for a scatterplot):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching-data/master/marek/blobs1.txt&quot;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>We can call <strong class="command">scipy.cluster.vq.kmeans2</strong> to find <span class="math notranslate nohighlight">\(k=2\)</span>
clusters:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.cluster.vq</span>
<span class="n">C</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">vq</span><span class="o">.</span><span class="n">kmeans2</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>The discovered cluster centres are stored in a matrix
with <span class="math notranslate nohighlight">\(k\)</span> rows and <span class="math notranslate nohighlight">\(m\)</span> columns, i.e., the <span class="math notranslate nohighlight">\(j\)</span>-th row
gives <span class="math notranslate nohighlight">\(\mathbf{c}_j\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">C</span>
<span class="c1">## array([[ 0.99622971,  1.052801  ],</span>
<span class="c1">##        [-0.90041365, -1.08411794]])</span>
</pre></div>
</div>
<p>The label vector is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">l</span>
<span class="c1">## array([1, 1, 1, ..., 0, 0, 0], dtype=int32)</span>
</pre></div>
</div>
<p>As usual in Python, indexing starts at 0. So for <span class="math notranslate nohighlight">\(k=2\)</span>
we only obtain the labels 0 and 1.</p>
<p><a class="reference internal" href="#fig-two-blobs-clusters"><span class="std std-numref">Figure 12.10</span></a>
depicts the two clusters together with the cluster centroids.
We use <code class="docutils literal notranslate"><span class="pre">l</span></code> as a colour selector
in <code class="docutils literal notranslate"><span class="pre">my_colours[l]</span></code> (this is a clever instance of the integer vector-based
indexing). It seems that we correctly discovered the
very natural partitioning of this dataset into two clusters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="s2">&quot;#DF536B&quot;</span><span class="p">])[</span><span class="n">l</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">C</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">C</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;yX&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id54">
<span id="fig-two-blobs-clusters"></span><img alt="../_images/two-blobs-clusters-19.png" src="../_images/two-blobs-clusters-19.png" />
<figcaption>
<p><span class="caption-number">Figure 12.10 </span><span class="caption-text">Two clusters discovered by the <em>k</em>-means method; cluster centroids are marked separately</span><a class="headerlink" href="#id54" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>Here are the cluster sizes:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>  <span class="c1"># or, e.g., pd.Series(l).value_counts()</span>
<span class="c1">## array([1017, 1039])</span>
</pre></div>
</div>
<p>The label vector <code class="docutils literal notranslate"><span class="pre">l</span></code> can be added as a new column
in the dataset. Here is a preview:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Xl</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">X1</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X2</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">l</span><span class="o">=</span><span class="n">l</span><span class="p">))</span>
<span class="n">Xl</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>  <span class="c1"># some randomly chosen rows</span>
<span class="c1">##             X1        X2  l</span>
<span class="c1">## 184  -0.973736 -0.417269  1</span>
<span class="c1">## 1724  1.432034  1.392533  0</span>
<span class="c1">## 251  -2.407422 -0.302862  1</span>
<span class="c1">## 1121  2.158669 -0.000564  0</span>
<span class="c1">## 1486  2.060772  2.672565  0</span>
</pre></div>
</div>
<p>We can now enjoy all the techniques for processing
data in groups that we have discussed so far.
In particular, computing the columnwise means
gives nothing else than the above cluster centroids:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Xl</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;l&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="c1">##          X1        X2</span>
<span class="c1">## l                    </span>
<span class="c1">## 0  0.996230  1.052801</span>
<span class="c1">## 1 -0.900414 -1.084118</span>
</pre></div>
</div>
<p>The label vector <code class="docutils literal notranslate"><span class="pre">l</span></code> can be recreated by computing
the distances between all the points and the centroids
and then picking the indexes of the closest pivots:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">l_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">scipy</span><span class="o">.</span><span class="n">spatial</span><span class="o">.</span><span class="n">distance</span><span class="o">.</span><span class="n">cdist</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">l_test</span> <span class="o">==</span> <span class="n">l</span><span class="p">)</span>  <span class="c1"># verify they are identical</span>
<span class="c1">## True</span>
</pre></div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>By construction<a class="footnote-reference brackets" href="#footvoronoi" id="id11">8</a>,
the <em>k</em>-means method can only detect clusters of convex shapes
(such as Gaussian blobs).</p>
</div>
<div class="proof proof-type-exercise" id="id55">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.19</span>
        
    </div><div class="proof-content">
<p>Perform the clustering of the
<a class="reference external" href="https://github.com/gagolews/teaching-data/raw/master/clustering/wut_isolation.csv"><code class="docutils literal notranslate"><span class="pre">wut_isolation</span></code></a>
dataset and notice how nonsensical, geometrically speaking,
the returned clusters are.</p>
</div></div><div class="proof proof-type-exercise" id="id56">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.20</span>
        
    </div><div class="proof-content">
<p>Determine a clustering of the
<a class="reference external" href="https://github.com/gagolews/teaching-data/raw/master/clustering/wut_twosplashes.csv"><code class="docutils literal notranslate"><span class="pre">wut_twosplashes</span></code></a>
dataset and display the results on a scatterplot.
Compare them with those obtained on the standardised
version of the dataset. Recall what we said about the Euclidean distance
and its perception being disturbed when a plot’s aspect ratio is not 1:1.</p>
</div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>(*)
An even simpler classifier than the <em>k</em>-nearest neighbours one
described above builds upon the concept of the nearest centroids.
Namely, it first determines the centroids (componentwise
arithmetic means) of the points in each class. Then, a new point
(from the test set) is assigned to the class whose centroid
is the closest thereto.
The implementation of such a classifier is left as a rather
straightforward exercise to the reader.
As an application, we recommend using it
to extrapolate the results generated by the <em>k</em>-means method
(for different <span class="math notranslate nohighlight">\(k\)</span>s) to previously unobserved data,
e.g., all points on a dense equidistant grid.</p>
</div>
</section>
<section id="solving-k-means-is-hard">
<h3><span class="section-number">12.4.2. </span>Solving <em>K</em>-means Is Hard<a class="headerlink" href="#solving-k-means-is-hard" title="Permalink to this heading"></a></h3>
<p>Unfortunately, the <em>k</em>-means method – the identification
of label vectors/cluster centres that minimise the total within-cluster
sum of squares – relies on solving a computationally hard combinatorial
optimisation problem (e.g., <span id="id12">[<a class="reference internal" href="999-bibliography.html#id132" title="J. Lee. A First Course in Combinatorial Optimisation. Cambridge University Press, 2011.">Lee11</a>]</span>). In other words, the search
for the <em>truly</em> (i.e., globally) optimal solution takes,
for larger <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(k\)</span>, an impractically long time.</p>
<p>As a consequence, we must rely on some approximate algorithms
which all have one drawback in common. Namely, whatever they return
can be <em>suboptimal</em>. Hence, they can constitute a
possibly meaningless solution.</p>
<p>The documentation of <strong class="command">scipy.cluster.vq.kmeans2</strong> is of course
honest about it. It states that the method <em>attempts to minimise the
Euclidean distance between observations and centroids</em>.
Further, <strong class="command">sklearn.cluster.KMeans</strong>, implementing a similar algorithm,
mentions that the procedure <em>is very fast […], but it falls in local minima.
That is why it can be useful to restart it several times.</em></p>
<p>To understand what it all means, it will be very educational
to study this issue in more detail.
This is because the discussed approach
to clustering is not the only hard problem in data science
(selecting an optimal set of independent variables with respect
to AIC or BIC in linear regression is another example).</p>
</section>
<section id="lloyd-s-algorithm">
<h3><span class="section-number">12.4.3. </span>Lloyd’s Algorithm<a class="headerlink" href="#lloyd-s-algorithm" title="Permalink to this heading"></a></h3>
<p>Technically, there is no such thing as <em>the</em> <em>k</em>-means <em>algorithm</em>.
There are many procedures, based on numerous different heuristics,
that attempt to solve the <em>k</em>-means <em>problem</em>. Unfortunately,
neither of them is perfect. This is not possible.</p>
<p>Perhaps the most widely known and easiest to understand
method is traditionally attributed to Lloyd <span id="id13">[<a class="reference internal" href="999-bibliography.html#id131" title="S.P. Lloyd. Least squares quantization in PCM. IEEE Transactions on Information Theory, 28:128–137, 1957 (1982). Originally a 1957 Bell Telephone Laboratories Research Report; republished in 1982. doi:10.1109/TIT.1982.1056489.">Llo2)</a>]</span>.
It is based on the fixed-point iteration and.
For a given <span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{n\times m}\)</span> and <span class="math notranslate nohighlight">\(k \ge 2\)</span>:</p>
<ol class="arabic">
<li><p>Pick initial cluster centres
<span class="math notranslate nohighlight">\(\boldsymbol{c}_{1}, \dots, \boldsymbol{c}_{k}\)</span>, for example,
randomly.</p></li>
<li><p>For each point in the dataset, <span class="math notranslate nohighlight">\(\mathbf{x}_{i,\cdot}\)</span>,
determine the index of its closest centre <span class="math notranslate nohighlight">\(l_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    l_i = \mathrm{arg}\min_{j} \| \mathbf{x}_{i,\cdot} - \boldsymbol{c}_{j} \|^2.
    \]</div>
</li>
<li><p>Compute the centroids of the clusters defined by the label vector
<span class="math notranslate nohighlight">\(\boldsymbol{l}\)</span>, i.e., for every <span class="math notranslate nohighlight">\(j=1,2,\dots,k\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{c}_j = \frac{1}{n_j} \sum_{i: l_i=j} \mathbf{x}_{i,\cdot},
    \]</div>
<p>where <span class="math notranslate nohighlight">\(n_j=|\{i: l_i=j\}|\)</span> gives the size of the <span class="math notranslate nohighlight">\(j\)</span>-th cluster.</p>
</li>
<li><p>If the objective function (total within-cluster sum of squares)
has not changed significantly since the last
iteration (say, the absolute value of the difference
between the last and the current loss is less than <span class="math notranslate nohighlight">\(10^{-9}\)</span>),
then stop and return the current
<span class="math notranslate nohighlight">\(\boldsymbol{c}_1,\dots,\boldsymbol{c}_k\)</span> as the result.
Otherwise, go to Step 2.</p></li>
</ol>
<div class="proof proof-type-exercise" id="id57">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.21</span>
        
    </div><div class="proof-content">
<p>(*) Implement the Lloyd algorithm in the form of a function
<strong class="command">kmeans</strong><code class="code docutils literal notranslate"><span class="pre">(X,</span> <span class="pre">C)</span></code>, where <code class="docutils literal notranslate"><span class="pre">X</span></code> is the data matrix (<em>n</em>-by-<em>m</em>)
and where the rows in <code class="docutils literal notranslate"><span class="pre">C</span></code>, being a <em>k</em>-by-<em>m</em> matrix,
give the initial cluster centres.</p>
</div></div></section>
<section id="local-minima">
<span id="sec-local-minima"></span><h3><span class="section-number">12.4.4. </span>Local Minima<a class="headerlink" href="#local-minima" title="Permalink to this heading"></a></h3>
<p>The way the above algorithm is constructed implies what follows.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Lloyd’s method guarantees that the centres
<span class="math notranslate nohighlight">\(\boldsymbol{c}_1,\dots,\boldsymbol{c}_k\)</span> it returns cannot be
significantly improved any further by repeating Steps 2 and 3
of the algorithm.
Still, it does not necessarily
mean that they yield the <em>globally</em> optimal (the best possible) WCSS.
We might as well get stuck in a <em>local</em> minimum, where there is no
better positioning thereof in the <em>neighbourhoods</em> of the current
cluster centres; compare <a class="reference internal" href="#fig-many-local-minima"><span class="std std-numref">Figure 12.11</span></a>. Yet, had we
looked beyond them, we could have found a superior solution.</p>
</div>
<figure class="align-default" id="id58">
<span id="fig-many-local-minima"></span><img alt="../_images/many-local-minima-21.png" src="../_images/many-local-minima-21.png" />
<figcaption>
<p><span class="caption-number">Figure 12.11 </span><span class="caption-text">An example function (of only one variable; our problem is much higher-dimensional) with many local minima; how can we be sure there is no better minimum outside of the depicted interval?</span><a class="headerlink" href="#id58" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>A variant of the Lloyd method is implemented
in <strong class="command">scipy.cluster.vq.kmeans2</strong>, where the initial cluster
centres are picked at random. Let us test its behaviour
by analysing three chosen country-wise categories
from the 2016 <a class="reference external" href="https://ssi.wi.th-koeln.de/">Sustainable Society Indices</a>
dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ssi</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching-data/master/marek/ssi_2016_categories.csv&quot;</span><span class="p">,</span>
    <span class="n">comment</span><span class="o">=</span><span class="s2">&quot;#&quot;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">ssi</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;Country&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span>
    <span class="p">[</span><span class="s2">&quot;PersonalDevelopmentAndHealth&quot;</span><span class="p">,</span> <span class="s2">&quot;WellBalancedSociety&quot;</span><span class="p">,</span> <span class="s2">&quot;Economy&quot;</span><span class="p">]</span>
<span class="p">]</span><span class="o">.</span><span class="n">rename</span><span class="p">({</span>
        <span class="s2">&quot;PersonalDevelopmentAndHealth&quot;</span><span class="p">:</span> <span class="s2">&quot;Health&quot;</span><span class="p">,</span>
        <span class="s2">&quot;WellBalancedSociety&quot;</span><span class="p">:</span> <span class="s2">&quot;Balance&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Economy&quot;</span><span class="p">:</span> <span class="s2">&quot;Economy&quot;</span>
    <span class="p">},</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># rename columns</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[[</span><span class="s2">&quot;Australia&quot;</span><span class="p">,</span> <span class="s2">&quot;Germany&quot;</span><span class="p">,</span> <span class="s2">&quot;Poland&quot;</span><span class="p">,</span> <span class="s2">&quot;United States&quot;</span><span class="p">],</span> <span class="p">:]</span>  <span class="c1"># preview</span>
<span class="c1">##                  Health   Balance   Economy</span>
<span class="c1">## Country                                    </span>
<span class="c1">## Australia      8.590927  6.105539  7.593052</span>
<span class="c1">## Germany        8.629024  8.036620  5.575906</span>
<span class="c1">## Poland         8.265950  7.331700  5.989513</span>
<span class="c1">## United States  8.357395  5.069076  3.756943</span>
</pre></div>
</div>
<p>It is a three-dimensional dataset, where each point (row) corresponds
to a different country. Let us find a partition into <span class="math notranslate nohighlight">\(k=3\)</span> clusters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>  <span class="c1"># reproducibility matters</span>
<span class="n">C1</span><span class="p">,</span> <span class="n">l1</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">vq</span><span class="o">.</span><span class="n">kmeans2</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
<span class="n">C1</span>
<span class="c1">## array([[7.99945084, 6.50033648, 4.36537659],</span>
<span class="c1">##        [7.6370645 , 4.54396676, 6.89893746],</span>
<span class="c1">##        [6.24317074, 3.17968018, 3.60779268]])</span>
</pre></div>
</div>
<p>The objective function (total within-cluster sum of squares)
at the returned cluster centres is equal to:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.spatial.distance</span>
<span class="k">def</span> <span class="nf">get_wcss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">C</span><span class="p">):</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">spatial</span><span class="o">.</span><span class="n">distance</span><span class="o">.</span><span class="n">cdist</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="n">get_wcss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">C1</span><span class="p">)</span>
<span class="c1">## 446.5221283436733</span>
</pre></div>
</div>
<p>Is it good or not necessarily? We are unable to tell.
What we can do, however, is to run the algorithm again,
this time from a different starting point:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>  <span class="c1"># different seed - different initial centres</span>
<span class="n">C2</span><span class="p">,</span> <span class="n">l2</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">vq</span><span class="o">.</span><span class="n">kmeans2</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
<span class="n">C2</span>
<span class="c1">## array([[7.80779013, 5.19409177, 6.97790733],</span>
<span class="c1">##        [6.31794579, 3.12048584, 3.84519706],</span>
<span class="c1">##        [7.92606993, 6.35691349, 3.91202972]])</span>
<span class="n">get_wcss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">C2</span><span class="p">)</span>
<span class="c1">## 437.51120966832775</span>
</pre></div>
</div>
<p>It is a better solution (we are lucky;
it might as well have been worse). But is it the best possible?
Again, we cannot tell, alone in the dark.</p>
<p>Does a potential suboptimality affect the way the data points are grouped?
It is indeed the case here. Let us look at the contingency
table for the two label vectors:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">l1</span><span class="o">=</span><span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="n">l2</span><span class="p">))</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">fill_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1">## l2   0   1   2</span>
<span class="c1">## l1            </span>
<span class="c1">## 0    8   0  43</span>
<span class="c1">## 1   39   6   0</span>
<span class="c1">## 2    0  57   1</span>
</pre></div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Clusters are essentially unordered.
The label vector <span class="math notranslate nohighlight">\((1, 1, 2, 2, 1, 3)\)</span> represents
the same clustering as the label vectors <span class="math notranslate nohighlight">\((3, 3, 2, 2, 3, 1)\)</span>
and <span class="math notranslate nohighlight">\((2, 2, 3, 3, 2, 1)\)</span>.</p>
</div>
<p>By looking at the contingency table, we see
that clusters 0, 1, and 2 in <code class="docutils literal notranslate"><span class="pre">l1</span></code> correspond, respectively,
to clusters 2, 0, and 1 in <code class="docutils literal notranslate"><span class="pre">l2</span></code> (via a kind of majority voting).
We can relabel the elements in <code class="docutils literal notranslate"><span class="pre">l1</span></code> to get a more readable
result:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">l1p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])[</span><span class="n">l1</span><span class="p">]</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">l1p</span><span class="o">=</span><span class="n">l1p</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="n">l2</span><span class="p">))</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">fill_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1">## l2    0   1   2</span>
<span class="c1">## l1p            </span>
<span class="c1">## 0    39   6   0</span>
<span class="c1">## 1     0  57   1</span>
<span class="c1">## 2     8   0  43</span>
</pre></div>
</div>
<p>Much better. It turns out that 8+6+1 countries
are categorised differently.
We would definitely not want to initiate any diplomatic crisis because
of our not knowing that the above algorithm might return
suboptimal solutions.</p>
<div class="proof proof-type-exercise" id="id59">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.22</span>
        
    </div><div class="proof-content">
<p>(*) Determine which countries are affected.</p>
</div></div></section>
<section id="random-restarts">
<h3><span class="section-number">12.4.5. </span>Random Restarts<a class="headerlink" href="#random-restarts" title="Permalink to this heading"></a></h3>
<p>There will never be any guarantees, but we can increase
the probability of generating a satisfactory solution by simply
restarting the method multiple times from many randomly chosen points
and picking the best<a class="footnote-reference brackets" href="#footkmeansdoesntmatter" id="id14">9</a> solution
(the one with the smallest WCSS)
identified as the result.</p>
<p>Let us make 1,000 such <em>restarts</em>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">wcss</span><span class="p">,</span> <span class="n">Cs</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">C</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">vq</span><span class="o">.</span><span class="n">kmeans2</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
    <span class="n">Cs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
    <span class="n">wcss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">get_wcss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span>
</pre></div>
</div>
<p>The best of the local minima (no guarantee that it is the global one, again)
is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">wcss</span><span class="p">)</span>
<span class="c1">## 437.51120966832775</span>
</pre></div>
</div>
<p>It corresponds to the cluster centres:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Cs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">wcss</span><span class="p">)]</span>
<span class="c1">## array([[7.80779013, 5.19409177, 6.97790733],</span>
<span class="c1">##        [7.92606993, 6.35691349, 3.91202972],</span>
<span class="c1">##        [6.31794579, 3.12048584, 3.84519706]])</span>
</pre></div>
</div>
<p>They are the same as <code class="docutils literal notranslate"><span class="pre">C2</span></code> above (up to a permutation of
labels). We were lucky<a class="footnote-reference brackets" href="#footlucky" id="id15">10</a>, after all.</p>
<p>It is very educational to look at the distribution of the
objective function at the identified local minima to see
that, proportionally, in the case of this dataset
it is not rare to end up in a quite bad solution;
see <a class="reference internal" href="#fig-wcss-local-minima"><span class="std std-numref">Figure 12.12</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">wcss</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id60">
<span id="fig-wcss-local-minima"></span><img alt="../_images/wcss-local-minima-23.png" src="../_images/wcss-local-minima-23.png" />
<figcaption>
<p><span class="caption-number">Figure 12.12 </span><span class="caption-text">Within-cluster sum of squares at the results returned by different runs of the <em>k</em>-means algorithm; sometimes we might be very unlucky</span><a class="headerlink" href="#id60" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>Also, <a class="reference internal" href="#fig-kmeans-suboptimal-centres"><span class="std std-numref">Figure 12.13</span></a> depicts
all the cluster centres to which the algorithm converged.
We see that we should not be trusting the results
generated by a single run of a heuristic solver to the <em>k</em>-means problem.</p>
<figure class="align-default" id="id61">
<span id="fig-kmeans-suboptimal-centres"></span><img alt="../_images/kmeans-suboptimal-centres-25.png" src="../_images/kmeans-suboptimal-centres-25.png" />
<figcaption>
<p><span class="caption-number">Figure 12.13 </span><span class="caption-text">Traces of different cluster centres our k-means algorithm converged to; some are definitely not optimal, and therefore the method must be restarted a few times to increase the likelihood of pinpointing the true solution</span><a class="headerlink" href="#id61" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<div class="proof proof-type-example" id="id62">

    <div class="proof-title">
        <span class="proof-type">Example 12.23</span>
        
    </div><div class="proof-content">
<p>(*)
The <strong class="program">scikit-learn</strong> package implements an algorithm that is similar
to the Lloyd’s one. The method is equipped with
the <code class="docutils literal notranslate"><span class="pre">n_init</span></code> parameter (which defaults to 10) which automatically
applies the aforementioned restarting.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sklearn.cluster</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">km</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">KMeans</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>  <span class="c1"># KMeans(k, n_init=10)</span>
<span class="n">km</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="c1">## KMeans(n_clusters=3)</span>
<span class="n">km</span><span class="o">.</span><span class="n">inertia_</span>  <span class="c1"># WCSS – not optimal!</span>
<span class="c1">## 437.5467188958928</span>
</pre></div>
</div>
<p>Still, there are no guarantees: the solution is suboptimal too.
As an exercise, pass <code class="docutils literal notranslate"><span class="pre">n_init=100</span></code>, <code class="docutils literal notranslate"><span class="pre">n_init=1000</span></code>,
and <code class="docutils literal notranslate"><span class="pre">n_init=10000</span></code> and determine the returned WCSS.</p>
</div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is theoretically possible that a developer from the
<strong class="program">scikit-learn</strong> team, when they see the above result,
will make a tweak in the algorithm so that after an update to the package,
the returned minimum will be better.
This cannot be deemed a bug fix, though, as there are no bugs here.
Improving the behaviour of the method in this example
will lead to its degradation in others.
There is no free lunch in optimisation.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some datasets are more well-behaving than others.
The <em>k</em>-means method is <em>overall</em> quite usable,
but we must always be cautious.</p>
<p>We recommend always performing at least 100 random restarts.
Also, if a report from data analysis does not say anything about
the number of tries performed, we should assume that the results
are gibberish<a class="footnote-reference brackets" href="#footr" id="id16">11</a>. People will complain about our being
a pain, but we know better; compare Rule#9.</p>
</div>
<div class="proof proof-type-exercise" id="id63">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.24</span>
        
    </div><div class="proof-content">
<p>Run the <em>k</em>-means method, <span class="math notranslate nohighlight">\(k=8\)</span>, on the
<a class="reference external" href="https://github.com/gagolews/teaching-data/raw/master/clustering/sipu_unbalance.csv"><code class="docutils literal notranslate"><span class="pre">sipu_unbalance</span></code></a>
dataset from many random sets of cluster centres.
Note the value of the total within-cluster sum
of squares. Also, plot the cluster centres discovered. Do they make sense?
Compare these to the case where you start the method from the
following cluster centres which are close to the global minimum.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{C} = \left[
\begin{array}{cc}
   -15  &amp; 5    \\
   -12  &amp; 10   \\
   -10  &amp; 5    \\
    15  &amp; 0    \\
    15  &amp; 10   \\
    20  &amp; 5    \\
    25  &amp; 0    \\
    25  &amp; 10   \\
\end{array}
\right].
\end{split}\]</div>
</div></div></section>
</section>
<section id="further-reading">
<h2><span class="section-number">12.5. </span>Further Reading<a class="headerlink" href="#further-reading" title="Permalink to this heading"></a></h2>
<p>An overall good introduction to classification is <span id="id17">[<a class="reference internal" href="999-bibliography.html#id52" title="T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer-Verlag, 2017. URL: https://hastie.su.domains/ElemStatLearn/.">HTF17</a>]</span>
and <span id="id18">[<a class="reference internal" href="999-bibliography.html#id53" title="C. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag, 2006. URL: https://www.microsoft.com/en-us/research/people/cmbishop/.">Bis06</a>]</span>.
Nevertheless, as we said earlier, we recommend going through
a solid course in matrix algebra and mathematical statistics
first, e.g., <span id="id19">[<a class="reference internal" href="999-bibliography.html#id74" title="M.P. Deisenroth, A.A. Faisal, and C.S. Ong. Mathematics for Machine Learning. Cambridge University Press, 2020. URL: https://mml-book.github.io/.">DFO20</a>, <a class="reference internal" href="999-bibliography.html#id133" title="J.E. Gentle. Matrix Algebra: Theory, Computations and Applications in Statistics. Springer, 2017.">Gen17</a>]</span>
and <span id="id20">[<a class="reference internal" href="999-bibliography.html#id44" title="F.M. Dekking, C. Kraaikamp, H.P. Lopuhaä, and L.E. Meester. A Modern Introduction to Probability and Statistics: Understanding Why and How. Springer, 2005.">DKLM05</a>, <a class="reference internal" href="999-bibliography.html#id47" title="J.E. Gentle. Computational Statistics. Springer-Verlag, 2009.">Gen09</a>, <a class="reference internal" href="999-bibliography.html#id42" title="J.E. Gentle. Theory of Statistics. book draft, 2020. URL: https://mason.gmu.edu/~jgentle/books/MathStat.pdf.">Gen20</a>]</span>.
For advanced theoretical (probabilistic, information-theoretic) results,
see, e.g., <span id="id21">[<a class="reference internal" href="999-bibliography.html#id118" title="A. Blum, J. Hopcroft, and R. Kannan. Foundations of Data Science. Cambridge University Press, 2020. URL: https://www.cs.cornell.edu/jeh/book.pdf.">BHK20</a>, <a class="reference internal" href="999-bibliography.html#id78" title="L. Devroye, L. Györfi, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer, 1996. doi:10.1007/978-1-4612-0711-5.">DGL96</a>]</span>.</p>
<p>Hierarchical clustering algorithms
(see, e.g., <span id="id22">[<a class="reference internal" href="999-bibliography.html#id84" title="M. Gagolewski. genieclust: Fast and robust hierarchical clustering. SoftwareX, 15:100722, 2021. URL: https://genieclust.gagolewski.com, doi:10.1016/j.softx.2021.100722.">Gag21</a>, <a class="reference internal" href="999-bibliography.html#id112" title="D. Müllner. Modern hierarchical, agglomerative clustering algorithms. 2011. arXiv:1109.2378 [stat.ML]. URL: https://arxiv.org/abs/1109.2378v1.">Mul11</a>]</span>)
are also noteworthy, because they do not require
asking for a fixed number of clusters.
Furthermore, density-based algorithms (DBSCAN and its variants)
<span id="id23">[<a class="reference internal" href="999-bibliography.html#id86" title="R.J.G.B. Campello, D. Moulavi, A. Zimek, and J. Sander. Hierarchical density estimates for data clustering, visualization, and outlier detection. ACM Transactions on Knowledge Discovery from Data, 10(1):5:1–5:51, 2015. doi:10.1145/2733381.">CMZS15</a>, <a class="reference internal" href="999-bibliography.html#id87" title="M. Ester, H.P. Kriegel, J. Sander, and X. Xu. A density-based algorithm for discovering clusters in large spatial databases with noise. In Proc. KDD'96, 226–231. 1996.">EKSX96</a>, <a class="reference internal" href="999-bibliography.html#id88" title="R.F. Ling. A probability theory of cluster analysis. Journal of the American Statistical Association, 68(341):159–164, 1973. doi:10.1080/01621459.1973.10481356.">Lin73</a>]</span>
utilise the notion of fixed-radius search that we have introduced
in <a class="reference internal" href="320-transform-matrix.html#sec-knn-vs-radius"><span class="std std-numref">Section 8.4.4</span></a>.</p>
<p>There are quite a few ways that aim to assess the quality of clustering
results, but their meaningfulness is somewhat limited;
see <span id="id24">[<a class="reference internal" href="999-bibliography.html#id7" title="M. Gagolewski, M. Bartoszuk, and A. Cena. Are cluster validity measures (in)valid? Information Sciences, 581:620–636, 2021. doi:10.1016/j.ins.2021.10.004.">GBC21</a>]</span> for discussion.</p>
</section>
<section id="exercises">
<h2><span class="section-number">12.6. </span>Exercises<a class="headerlink" href="#exercises" title="Permalink to this heading"></a></h2>
<div class="proof proof-type-exercise" id="id64">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.25</span>
        
    </div><div class="proof-content">
<p>Name the data type of the objects that the <strong class="command">DataFrame.groupby</strong>
method returns.</p>
</div></div><div class="proof proof-type-exercise" id="id65">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.26</span>
        
    </div><div class="proof-content">
<p>What is the relationship between the <code class="docutils literal notranslate"><span class="pre">GroupBy</span></code>, <code class="docutils literal notranslate"><span class="pre">DataFrameGroupBy</span></code>,
and <code class="docutils literal notranslate"><span class="pre">SeriesGroupBy</span></code> classes?</p>
</div></div><div class="proof proof-type-exercise" id="id66">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.27</span>
        
    </div><div class="proof-content">
<p>What are relative z-scores and how can we compute them?</p>
</div></div><div class="proof proof-type-exercise" id="id67">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.28</span>
        
    </div><div class="proof-content">
<p>Why and when the accuracy score might not be the best way to quantify a
classifier’s performance?</p>
</div></div><div class="proof proof-type-exercise" id="id68">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.29</span>
        
    </div><div class="proof-content">
<p>What is the difference between recall and precision, both in terms
of how they are defined and where they are the most useful?</p>
</div></div><div class="proof proof-type-exercise" id="id69">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.30</span>
        
    </div><div class="proof-content">
<p>Explain how the <em>k</em>-nearest neighbour classification and regression
algorithms work. Why do we say that they are model-free?</p>
</div></div><div class="proof proof-type-exercise" id="id70">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.31</span>
        
    </div><div class="proof-content">
<p>In the context of <em>k</em>-nearest neighbour classification,
why it might be important to resolve the
potential ties at random when computing the mode of the neighbours’ labels?</p>
</div></div><div class="proof proof-type-exercise" id="id71">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.32</span>
        
    </div><div class="proof-content">
<p>What is the purpose of a training/test and a training/validation/test set
split?</p>
</div></div><div class="proof proof-type-exercise" id="id72">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.33</span>
        
    </div><div class="proof-content">
<p>Give the formula for the total within-cluster sum of squares.</p>
</div></div><div class="proof proof-type-exercise" id="id73">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.34</span>
        
    </div><div class="proof-content">
<p>Are there any cluster shapes that cannot be detected by the <em>k</em>-means
method?</p>
</div></div><div class="proof proof-type-exercise" id="id74">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.35</span>
        
    </div><div class="proof-content">
<p>Why do we say that solving the <em>k</em>-means problem is hard?</p>
</div></div><div class="proof proof-type-exercise" id="id75">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.36</span>
        
    </div><div class="proof-content">
<p>Why restarting Lloyd’s algorithm many times is necessary?
Why are reports from data analysis that do not mention the number
of restarts not trustworthy?</p>
</div></div><hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="footoop"><span class="brackets"><a class="fn-backref" href="#id2">1</a></span></dt>
<dd><p>(*) In this example, we called <strong class="command">pandas.GroupBy.mean</strong>.
Note that it has slightly different functionality
from <strong class="command">pandas.DataFrame.mean</strong> and <strong class="command">pandas.Series.mean</strong>,
which all needed to be implemented separately so that we can
use them in complex operation chains.
Still, they all call the underlying <strong class="command">numpy.mean</strong> function.
Object-oriented programming has its pros (more expressive syntax)
and cons (sometimes more redundancy in the API design).</p>
</dd>
<dt class="label" id="footintroalgs"><span class="brackets"><a class="fn-backref" href="#id3">2</a></span></dt>
<dd><p>Remember that this is an introductory course,
and we are still being very generous here.
We encourage the readers to upskill themselves (later, of course)
not only in mathematics, but also in programming
(e.g., algorithms and data structures).</p>
</dd>
<dt class="label" id="footerrdev"><span class="brackets"><a class="fn-backref" href="#id4">3</a></span></dt>
<dd><p>Including those that are merely due to round-off errors.</p>
</dd>
<dt class="label" id="footpvalue"><span class="brackets"><a class="fn-backref" href="#id5">4</a></span></dt>
<dd><p>For similar reasons, we do not introduce the notion
of p-values. Most practitioners tend to misunderstand them anyway.</p>
</dd>
<dt class="label" id="football"><span class="brackets"><a class="fn-backref" href="#id7">5</a></span></dt>
<dd><p>(*) As an exercise, we could implement a fixed-radius
classifier; compare <a class="reference internal" href="320-transform-matrix.html#sec-knn-vs-radius"><span class="std std-numref">Section 8.4.4</span></a>.
In sparsely populated regions, the decision might be
“unknown”.</p>
</dd>
<dt class="label" id="foothar"><span class="brackets"><a class="fn-backref" href="#id8">6</a></span></dt>
<dd><p>(*) For any vector of nonnegative values,
its minimum <span class="math notranslate nohighlight">\(\le\)</span> its harmonic mean <span class="math notranslate nohighlight">\(\le\)</span> its arithmetic mean.</p>
</dd>
<dt class="label" id="footkmeans"><span class="brackets"><a class="fn-backref" href="#id10">7</a></span></dt>
<dd><p>We do not have to denote the number of clusters
with <span class="math notranslate nohighlight">\(k\)</span>: we could be speaking about the 2-means, 3-means,
<em>l</em>-means, or <em>ü</em>-means method too. Nevertheless,
some mainstream practitioners consider <em>k</em>-means as a kind of a
brand name, let us thus refrain from adding to their confusion.
Interestingly, another widely known algorithm
is called fuzzy (weighted) <em>c</em>-means <span id="id25">[<a class="reference internal" href="999-bibliography.html#id90" title="J.C. Bezdek, R. Ehrlich, and W. Full. FCM: The fuzzy c-means clustering algorithm. Computer and Geosciences, 10(2–3):191–203, 1984. doi:10.1016/0098-3004(84)90020-7.">BEF84</a>]</span>.</p>
</dd>
<dt class="label" id="footvoronoi"><span class="brackets"><a class="fn-backref" href="#id11">8</a></span></dt>
<dd><p>(*) And its relation to Voronoi diagrams.</p>
</dd>
<dt class="label" id="footkmeansdoesntmatter"><span class="brackets"><a class="fn-backref" href="#id14">9</a></span></dt>
<dd><p>If we have many different heuristics,
each aiming to approximate
a solution to the <em>k</em>-means problem, from the practical
point of view it does not really matter which one returns
the best solution – they are merely our tools to achieve
a higher goal. Ideally, we should run all of them
many times and get the result that corresponds to the smallest
WCSS. It is crucial to <em>do our best</em> to
find the optimal set of cluster centres – the
more approaches we test, the better the chance of success.</p>
</dd>
<dt class="label" id="footlucky"><span class="brackets"><a class="fn-backref" href="#id15">10</a></span></dt>
<dd><p>Mind who is the benevolent dictator
of the pseudorandom number generator’s seed.</p>
</dd>
<dt class="label" id="footr"><span class="brackets"><a class="fn-backref" href="#id16">11</a></span></dt>
<dd><p>For instance, R’s <strong class="command">stats::kmeans</strong>
automatically uses <code class="docutils literal notranslate"><span class="pre">nstart=1</span></code>. It is not rare, unfortunately,
that data analysts only stick with the default arguments.</p>
</dd>
</dl>
</section>
</section>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="440-sql.html" class="btn btn-neutral float-right" title="13. Accessing Databases" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="420-categorical.html" class="btn btn-neutral float-left" title="11. Handling Categorical Data" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        Copyright &#169; 2022 by <a href="https://www.gagolewski.com">Marek Gagolewski</a>. Some rights reserved. Licensed under <a href='https://creativecommons.org/licenses/by-nc-nd/4.0/'>CC BY-NC-ND 4.0</a>.

    Built with <a href="https://sphinx-doc.org/">Sphinx</a>
    and a customised <a href="https://github.com/rtfd/sphinx_rtd_theme">rtd</a>
    theme.
      <span class="lastupdated">
        Last updated on 2022-08-22T12:32:01+1000.
      </span>


    This site will never display any ads: it is a non-profit project.
    It does not collect any data.

    </p>
  </div> 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>