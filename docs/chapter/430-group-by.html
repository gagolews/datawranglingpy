<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta content="Minimalist Data Wrangling with Python" name="citation_title" />
<meta content="Marek Gagolewski" name="citation_author" />
<meta content="2024" name="citation_date" />
<meta content="2024" name="citation_publication_date" />
<meta content="https://datawranglingpy.gagolewski.com/datawranglingpy.pdf" name="citation_pdf_url" />
<meta content="https://datawranglingpy.gagolewski.com/" name="citation_public_url" />
<meta content="10.5281/zenodo.6451068" name="citation_doi" />
<meta content="Minimalist Data Wrangling with Python is envisaged as a student's first introduction to data science, providing a high-level overview as well as discussing key concepts in detail. We explore methods for cleaning data gathered from different sources, transforming, selecting, and extracting features, performing exploratory data analysis and dimensionality reduction, identifying naturally occurring data clusters, modelling patterns in data, comparing data between groups, and reporting the results. This textbook is a non-profit project. Its online and PDF versions are freely available at https://datawranglingpy.gagolewski.com/." name="citation_abstract" />
<meta content="summary" name="twitter:card" />
<meta content="Minimalist Data Wrangling with Python" name="twitter:title" />
<meta content="Minimalist Data Wrangling with Python" name="og:title" />
<meta content="Minimalist Data Wrangling with Python is envisaged as a student's first introduction to data science, providing a high-level overview as well as discussing key concepts in detail. We explore methods for cleaning data gathered from different sources, transforming, selecting, and extracting features, performing exploratory data analysis and dimensionality reduction, identifying naturally occurring data clusters, modelling patterns in data, comparing data between groups, and reporting the results. This textbook is a non-profit project. Its online and PDF versions are freely available at https://datawranglingpy.gagolewski.com/." name="twitter:description" />
<meta content="Minimalist Data Wrangling with Python is envisaged as a student's first introduction to data science, providing a high-level overview as well as discussing key concepts in detail. We explore methods for cleaning data gathered from different sources, transforming, selecting, and extracting features, performing exploratory data analysis and dimensionality reduction, identifying naturally occurring data clusters, modelling patterns in data, comparing data between groups, and reporting the results. This textbook is a non-profit project. Its online and PDF versions are freely available at https://datawranglingpy.gagolewski.com/." name="og:description" />
<meta content="gagolews/datawranglingpy" name="og:site_name" />
<meta content="https://datawranglingpy.gagolewski.com/" name="og:url" />
<meta content="https://datawranglingpy.gagolewski.com/_images/cover.png" name="twitter:image" />
<meta content="https://datawranglingpy.gagolewski.com/_images/cover.png" name="og:image" />
<meta content="https://datawranglingpy.gagolewski.com/" name="DC.identifier" />
<meta content="Marek Gagolewski" name="DC.publisher" />
<meta content="INDEX,FOLLOW" name="robots" />
<meta content="book" name="og:type" />
<meta content="9780645571912" name="og:book:isbn" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="13. Accessing databases" href="440-sql.html" /><link rel="prev" title="11. Handling categorical data" href="420-categorical.html" />
        <link rel="canonical" href="https://datawranglingpy.gagolewski.com/chapter/430-group-by.html" />

    <link rel="shortcut icon" href="https://www.gagolewski.com/_static/img/datawranglingpy.png"/><!-- Generated with Sphinx 7.2.6 and Furo 2023.09.10 -->
        <title>12. Processing data in groups - Minimalist Data Wrangling with Python</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=135e06be" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" />
    <link rel="stylesheet" type="text/css" href="../_static/katex-math.css?v=91adb8b6" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=36a5483c" />
    <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=1e0fcfc0" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  --admonition-font-size: 95%;
  --admonition-title-font-size: 95%;
  --color-brand-primary: red;
  --color-brand-content: #CC3333;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --admonition-font-size: 95%;
  --admonition-title-font-size: 95%;
  --color-brand-primary: #ff2b53;
  --color-brand-content: #dd3333;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --admonition-font-size: 95%;
  --admonition-title-font-size: 95%;
  --color-brand-primary: #ff2b53;
  --color-brand-content: #dd3333;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">Minimalist DataÂ Wrangling withÂ Python</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto colour theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky">
<div class="sidebar-logo-container">
  <a class="sidebar-brand" href="../index.html"><img class="sidebar-logo" src="https://www.gagolewski.com/_static/img/datawranglingpy.png" alt="Logo"/></a>
</div>

<span class="sidebar-brand-text">
<a class="sidebar-brand" href="../index.html">Minimalist DataÂ Wrangling withÂ Python</a>
</span>
<div class="sidebar-brand">
An open-access textbook<br />
byÂ <a href='https://www.gagolewski.com/' style="display: contents">MarekÂ Gagolewski</a><br />
v1.0.3.9107
</div>
<form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">About</a></li>
<li class="toctree-l1"><a class="reference external" href="https://www.gagolewski.com/">Author</a></li>
<li class="toctree-l1"><a class="reference external" href="https://datawranglingpy.gagolewski.com/datawranglingpy.pdf">This book in PDF</a></li>
<li class="toctree-l1"><a class="reference internal" href="../order-paper-copy.html">Order a paper copy</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/gagolews/datawranglingpy">Report bugs or typos (GitHub)</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/gagolews/teaching-data">Datasets</a></li>
<li class="toctree-l1"><a class="reference external" href="https://deepr.gagolewski.com/">Deep R programming</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Start here</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="000-preface.html">Preface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introducing Python</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="110-setup.html">1. Getting started with Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="120-scalar.html">2. Scalar types and control structures in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="130-sequential.html">3. Sequential and other types in Python</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Unidimensional data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="210-vector.html">4. Unidimensional numeric data and their empirical distribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="220-transform-vector.html">5. Processing unidimensional data</a></li>
<li class="toctree-l1"><a class="reference internal" href="230-distribution.html">6. Continuous probability distributions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multidimensional data</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="310-matrix.html">7. From uni- to multidimensional numeric data</a></li>
<li class="toctree-l1"><a class="reference internal" href="320-transform-matrix.html">8. Processing multidimensional data</a></li>
<li class="toctree-l1"><a class="reference internal" href="330-relationship.html">9. Exploring relationships between variables</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Heterogeneous data</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="410-data-frame.html">10. Introducing data frames</a></li>
<li class="toctree-l1"><a class="reference internal" href="420-categorical.html">11. Handling categorical data</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">12. Processing data in groups</a></li>
<li class="toctree-l1"><a class="reference internal" href="440-sql.html">13. Accessing databases</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other data types</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="510-text.html">14. Text data</a></li>
<li class="toctree-l1"><a class="reference internal" href="520-missingness.html">15. Missing, censored, and questionable data</a></li>
<li class="toctree-l1"><a class="reference internal" href="530-time-series.html">16. Time series</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Appendix</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="998-changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="999-bibliography.html">References</a></li>
</ul>

</div></div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="edit-this-page">
  <a class="muted-link" href="https://github.com/gagolews/datawranglingpy" title="Edit this page">
    <svg aria-hidden="true" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <path d="M4 20h4l10.5 -10.5a1.5 1.5 0 0 0 -4 -4l-10.5 10.5v4" />
      <line x1="13.5" y1="6.5" x2="17.5" y2="10.5" />
    </svg>
    <span class="visually-hidden">Edit this page</span>
  </a>
</div><div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto colour theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section class="tex2jax_ignore mathjax_ignore" id="processing-data-in-groups">
<span id="chap-group-by"></span><h1><span class="section-number">12. </span>Processing data in groups<a class="headerlink" href="#processing-data-in-groups" title="Link to this heading">#</a></h1>
<blockquote>
<div><p><em>This open-access textbook
is, and will remain, freely available for everyoneâs enjoyment
(also in <a class="reference external" href="https://datawranglingpy.gagolewski.com/datawranglingpy.pdf">PDF</a>;
a paper copy can also be <a class="reference internal" href="../order-paper-copy.html"><span class="doc std std-doc">ordered</span></a>).
It is a non-profit project. Although available online, it is a whole course,
and should be read from the beginning to the end.
Refer to the <a class="reference internal" href="000-preface.html#chap-preface"><span class="std std-ref">Preface</span></a> for general introductory remarks. Any
<a class="reference external" href="https://github.com/gagolews/datawranglingpy">bug/typo reports/fixes</a>
are appreciated. Make sure to check out
<a class="reference external" href="https://deepr.gagolewski.com/"><em>Deep R Programming</em></a>
<span id="id1">[<a class="reference internal" href="999-bibliography.html#id2">35</a>]</span> too.</em></p>
</div></blockquote>
<p>Consider another subset of the US Centres for Disease Control
and Prevention National Health and Nutrition Examination Survey,
this time carrying some body measures
(<a class="reference external" href="https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/P_BMX.htm">P_BMX</a>)
together with demographics
(<a class="reference external" href="https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/P_DEMO.htm">P_DEMO</a>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching-data/master/marek/nhanes_p_demo_bmx_2020.csv&quot;</span><span class="p">,</span>
    <span class="n">comment</span><span class="o">=</span><span class="s2">&quot;#&quot;</span><span class="p">)</span>
<span class="n">nhanes</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">nhanes</span>
    <span class="o">.</span><span class="n">loc</span><span class="p">[</span>
        <span class="p">(</span><span class="n">nhanes</span><span class="o">.</span><span class="n">DMDBORN4</span> <span class="o">&lt;=</span> <span class="mi">2</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">nhanes</span><span class="o">.</span><span class="n">RIDAGEYR</span> <span class="o">&gt;=</span> <span class="mi">18</span><span class="p">),</span>
        <span class="p">[</span><span class="s2">&quot;RIDAGEYR&quot;</span><span class="p">,</span> <span class="s2">&quot;BMXWT&quot;</span><span class="p">,</span> <span class="s2">&quot;BMXHT&quot;</span><span class="p">,</span> <span class="s2">&quot;BMXBMI&quot;</span><span class="p">,</span> <span class="s2">&quot;RIAGENDR&quot;</span><span class="p">,</span> <span class="s2">&quot;DMDBORN4&quot;</span><span class="p">]</span>
    <span class="p">]</span>  <span class="c1"># age &gt;= 18 and only US and non-US born</span>
    <span class="o">.</span><span class="n">rename</span><span class="p">({</span>
        <span class="s2">&quot;RIDAGEYR&quot;</span><span class="p">:</span> <span class="s2">&quot;age&quot;</span><span class="p">,</span>
        <span class="s2">&quot;BMXWT&quot;</span><span class="p">:</span> <span class="s2">&quot;weight&quot;</span><span class="p">,</span>
        <span class="s2">&quot;BMXHT&quot;</span><span class="p">:</span> <span class="s2">&quot;height&quot;</span><span class="p">,</span>
        <span class="s2">&quot;BMXBMI&quot;</span><span class="p">:</span> <span class="s2">&quot;bmival&quot;</span><span class="p">,</span>
        <span class="s2">&quot;RIAGENDR&quot;</span><span class="p">:</span> <span class="s2">&quot;gender&quot;</span><span class="p">,</span>
        <span class="s2">&quot;DMDBORN4&quot;</span><span class="p">:</span> <span class="s2">&quot;usborn&quot;</span>
    <span class="p">},</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># rename columns</span>
    <span class="o">.</span><span class="n">dropna</span><span class="p">()</span>   <span class="c1"># remove missing values</span>
    <span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>We consider only the adult (at least 18 years old) participants,
whose country of birth (the US or not) is well-defined.
Letâs recode the <code class="docutils literal notranslate"><span class="pre">usborn</span></code> and <code class="docutils literal notranslate"><span class="pre">gender</span></code> variables (for readability)
and introduce the BMI categories:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;usborn&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>  <span class="c1"># NOT: nhanes.usborn = ..., it will not work</span>
    <span class="n">nhanes</span><span class="o">.</span><span class="n">usborn</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;category&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">cat</span><span class="o">.</span><span class="n">rename_categories</span><span class="p">([</span><span class="s2">&quot;yes&quot;</span><span class="p">,</span> <span class="s2">&quot;no&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;str&quot;</span><span class="p">)</span>       <span class="c1"># recode usborn</span>
<span class="p">)</span>
<span class="n">nhanes</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;gender&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">nhanes</span><span class="o">.</span><span class="n">gender</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;category&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">cat</span><span class="o">.</span><span class="n">rename_categories</span><span class="p">([</span><span class="s2">&quot;male&quot;</span><span class="p">,</span> <span class="s2">&quot;female&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;str&quot;</span><span class="p">)</span>  <span class="c1"># recode gender</span>
<span class="p">)</span>
<span class="n">nhanes</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;bmicat&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span>                             <span class="c1"># new column</span>
    <span class="n">nhanes</span><span class="o">.</span><span class="n">bmival</span><span class="p">,</span>
    <span class="n">bins</span><span class="o">=</span>  <span class="p">[</span> <span class="mi">0</span><span class="p">,</span>             <span class="mf">18.5</span><span class="p">,</span>        <span class="mi">25</span><span class="p">,</span>            <span class="mi">30</span><span class="p">,</span>       <span class="n">np</span><span class="o">.</span><span class="n">inf</span> <span class="p">],</span>
    <span class="n">labels</span><span class="o">=</span><span class="p">[</span>   <span class="s2">&quot;underweight&quot;</span><span class="p">,</span>    <span class="s2">&quot;normal&quot;</span><span class="p">,</span>  <span class="s2">&quot;overweight&quot;</span><span class="p">,</span>  <span class="s2">&quot;obese&quot;</span>       <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Here is a preview of this data frame:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<span class="c1">##    age  weight  height  bmival  gender usborn      bmicat</span>
<span class="c1">## 0   29    97.1   160.2    37.8  female     no       obese</span>
<span class="c1">## 1   49    98.8   182.3    29.7    male    yes  overweight</span>
<span class="c1">## 2   36    74.3   184.2    21.9    male    yes      normal</span>
<span class="c1">## 3   68   103.7   185.3    30.2    male    yes       obese</span>
<span class="c1">## 4   76    83.3   177.1    26.6    male    yes  overweight</span>
</pre></div>
</div>
<p>We have a mix of categorical (gender, US born-ness, BMI category)
and numerical (age, weight, height, BMI) variables.
Unless we had encoded qualitative variables as integers,
this would not be possible with plain matrices,
at least not with a single one.</p>
<p>In this section, we will treat the categorical columns
as grouping variables. This way, we will be able to e.g., summarise
or visualise the data <em>in each group</em> separately. Suffice it to say that
it is likely that data distributions vary across different factor levels.
It is much like having many data frames stored in one object,
e.g., the heights of women and men separately.</p>
<p><code class="docutils literal notranslate"><span class="pre">nhanes</span></code> is thus an example of heterogeneous data at their best.</p>
<section id="basic-methods">
<h2><span class="section-number">12.1. </span>Basic methods<a class="headerlink" href="#basic-methods" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">DataFrame</span></code> and <code class="docutils literal notranslate"><span class="pre">Series</span></code> objects are equipped with the <strong class="command">groupby</strong>
methods, which assist in performing a wide range
of operations in data groups defined by one
or more data frame columns (compare <span id="id2">[<a class="reference internal" href="999-bibliography.html#id38" title="Wickham, H. (2011).  The split-apply-combine strategy for data analysis. Journal of Statistical Software, 40(1):1â29. DOI: 10.18637/jss.v040.i01.">97</a>]</span>).
They return objects of the classes <code class="docutils literal notranslate"><span class="pre">DataFrameGroupBy</span></code>
and <code class="docutils literal notranslate"><span class="pre">SeriesGroupby</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">type</span><span class="p">(</span><span class="n">nhanes</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">))</span>
<span class="c1">## &lt;class &#39;pandas.core.groupby.generic.DataFrameGroupBy&#39;&gt;</span>
<span class="nb">type</span><span class="p">(</span><span class="n">nhanes</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">height</span><span class="p">)</span>  <span class="c1"># or (...)[&quot;height&quot;]</span>
<span class="c1">## &lt;class &#39;pandas.core.groupby.generic.SeriesGroupBy&#39;&gt;</span>
</pre></div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>When browsing the list of available attributes
in the <strong class="program">pandas</strong> manual, it is worth knowing that
<code class="docutils literal notranslate"><span class="pre">DataFrameGroupBy</span></code> and <code class="docutils literal notranslate"><span class="pre">SeriesGroupBy</span></code> are separate types.
Still, they have many methods and slots in common.</p>
</div>
<div class="proof proof-type-exercise" id="id27">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.1</span>
        
    </div><div class="proof-content">
<p>Skim through the
<a class="reference external" href="https://pandas.pydata.org/pandas-docs/stable/reference/groupby.html">documentation</a>
of the said classes.</p>
</div></div><p>For example, the <strong class="command">pandas.DataFrameGroupBy.size</strong>
method determines the number of observations in each group:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="c1">## gender</span>
<span class="c1">## female    4514</span>
<span class="c1">## male      4271</span>
<span class="c1">## dtype: int64</span>
</pre></div>
</div>
<p>It returns an object of the type <code class="docutils literal notranslate"><span class="pre">Series</span></code>.
We can also perform the grouping with respect
to a combination of levels in two qualitative columns:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="s2">&quot;bmicat&quot;</span><span class="p">],</span> <span class="n">observed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="c1">## gender  bmicat     </span>
<span class="c1">## female  underweight      93</span>
<span class="c1">##         normal         1161</span>
<span class="c1">##         overweight     1245</span>
<span class="c1">##         obese          2015</span>
<span class="c1">## male    underweight      65</span>
<span class="c1">##         normal         1074</span>
<span class="c1">##         overweight     1513</span>
<span class="c1">##         obese          1619</span>
<span class="c1">## dtype: int64</span>
</pre></div>
</div>
<p>This yields a <code class="docutils literal notranslate"><span class="pre">Series</span></code> with a hierarchical index (<a class="reference internal" href="410-data-frame.html#sec-df-index"><span class="std std-numref">SectionÂ 10.1.3</span></a>).
Nevertheless, we can always call
<strong class="command">reset_index</strong> to convert it to standalone columns:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span>
    <span class="p">[</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="s2">&quot;bmicat&quot;</span><span class="p">],</span> <span class="n">observed</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="s2">&quot;counts&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="c1">##    gender       bmicat  counts</span>
<span class="c1">## 0  female  underweight      93</span>
<span class="c1">## 1  female       normal    1161</span>
<span class="c1">## 2  female   overweight    1245</span>
<span class="c1">## 3  female        obese    2015</span>
<span class="c1">## 4    male  underweight      65</span>
<span class="c1">## 5    male       normal    1074</span>
<span class="c1">## 6    male   overweight    1513</span>
<span class="c1">## 7    male        obese    1619</span>
</pre></div>
</div>
<p>Take note of the <strong class="command">rename</strong> part. It gave us some readable
column names.</p>
<p>Furthermore, it is possible to group rows in a data frame
using a list of any <code class="docutils literal notranslate"><span class="pre">Series</span></code> objects, i.e., not just column names in
a given data frame; see <a class="reference internal" href="530-time-series.html#sec-df-datetime"><span class="std std-numref">SectionÂ 16.2.3</span></a>
for an example.</p>
<div class="proof proof-type-exercise" id="id28">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.2</span>
        
    </div><div class="proof-content">
<p>(*) Note the difference between
<strong class="command">pandas.GroupBy.count</strong> and <strong class="command">pandas.GroupBy.size</strong> methods
(by reading their documentation).</p>
</div></div><section id="aggregating-data-in-groups">
<h3><span class="section-number">12.1.1. </span>Aggregating data in groups<a class="headerlink" href="#aggregating-data-in-groups" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">DataFrameGroupBy</span></code> and <code class="docutils literal notranslate"><span class="pre">SeriesGroupBy</span></code> classes are equipped
with several well-known aggregation functions. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">numeric_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="c1">##    gender        age     weight      height     bmival</span>
<span class="c1">## 0  female  48.956580  78.351839  160.089189  30.489189</span>
<span class="c1">## 1    male  49.653477  88.589932  173.759541  29.243620</span>
</pre></div>
</div>
<p>The arithmetic mean was computed only on numeric columns<a class="footnote-reference brackets" href="#footoop" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>.
Alternatively, we can apply the aggregate only on specific columns:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">)[[</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="s2">&quot;height&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="c1">##    gender     weight      height</span>
<span class="c1">## 0  female  78.351839  160.089189</span>
<span class="c1">## 1    male  88.589932  173.759541</span>
</pre></div>
</div>
<p>Another example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="s2">&quot;bmicat&quot;</span><span class="p">],</span> <span class="n">observed</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="s2">&quot;height&quot;</span><span class="p">]</span><span class="o">.</span>\
    <span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="c1">##    gender       bmicat      height</span>
<span class="c1">## 0  female  underweight  161.976344</span>
<span class="c1">## 1  female       normal  160.149182</span>
<span class="c1">## 2  female   overweight  159.573012</span>
<span class="c1">## 3  female        obese  160.286452</span>
<span class="c1">## 4    male  underweight  174.073846</span>
<span class="c1">## 5    male       normal  173.443855</span>
<span class="c1">## 6    male   overweight  173.051685</span>
<span class="c1">## 7    male        obese  174.617851</span>
</pre></div>
</div>
<p>Further, the most common aggregates that we described in
<a class="reference internal" href="220-transform-vector.html#sec-aggregate"><span class="std std-numref">SectionÂ 5.1</span></a> can be generated by calling the
<strong class="command">describe</strong> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">height</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">T</span>
<span class="c1">## gender       female         male</span>
<span class="c1">## count   4514.000000  4271.000000</span>
<span class="c1">## mean     160.089189   173.759541</span>
<span class="c1">## std        7.035483     7.702224</span>
<span class="c1">## min      131.100000   144.600000</span>
<span class="c1">## 25%      155.300000   168.500000</span>
<span class="c1">## 50%      160.000000   173.800000</span>
<span class="c1">## 75%      164.800000   178.900000</span>
<span class="c1">## max      189.300000   199.600000</span>
</pre></div>
</div>
<p>We have applied the transpose (<strong class="command">T</strong>)
to get a more readable (âtallâ) result.</p>
<p>If different aggregates are needed, we can call <strong class="command">aggregate</strong>
to apply a custom list of functions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">nhanes</span><span class="o">.</span>
    <span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">)[[</span><span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">]]</span><span class="o">.</span>
    <span class="n">aggregate</span><span class="p">([</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="mi">2</span><span class="p">])</span><span class="o">.</span>
    <span class="n">reset_index</span><span class="p">()</span>
<span class="p">)</span>
<span class="c1">##    gender      height                      weight                 </span>
<span class="c1">##                  mean   len &lt;lambda_0&gt;       mean   len &lt;lambda_0&gt;</span>
<span class="c1">## 0  female  160.089189  4514      160.2  78.351839  4514     143.45</span>
<span class="c1">## 1    male  173.759541  4271      172.1  88.589932  4271     139.70</span>
</pre></div>
</div>
<p>Interestingly, the resultâs <code class="docutils literal notranslate"><span class="pre">columns</span></code> slot uses a hierarchical index.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The column names in the output object
are generated by reading the applied functionsâ <code class="docutils literal notranslate"><span class="pre">__name__</span></code> slots;
see, e.g., <strong class="command">print</strong><code class="code docutils literal notranslate"><span class="pre">(np.mean.__name__)</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mr</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">/</span><span class="mi">2</span>
<span class="n">mr</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="s2">&quot;midrange&quot;</span>
<span class="p">(</span><span class="n">nhanes</span><span class="o">.</span>
    <span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">]]</span><span class="o">.</span>
    <span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">)</span><span class="o">.</span>
    <span class="n">aggregate</span><span class="p">([</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">mr</span><span class="p">])</span><span class="o">.</span>
    <span class="n">reset_index</span><span class="p">()</span>
<span class="p">)</span>
<span class="c1">##    gender      height              weight         </span>
<span class="c1">##                  mean midrange       mean midrange</span>
<span class="c1">## 0  female  160.089189    160.2  78.351839   143.45</span>
<span class="c1">## 1    male  173.759541    172.1  88.589932   139.70</span>
</pre></div>
</div>
</div>
</section>
<section id="transforming-data-in-groups">
<h3><span class="section-number">12.1.2. </span>Transforming data in groups<a class="headerlink" href="#transforming-data-in-groups" title="Link to this heading">#</a></h3>
<p>We can easily transform individual columns relative to different
data groups by means of the <strong class="command">transform</strong> method for <code class="docutils literal notranslate"><span class="pre">GroupBy</span></code>
objects.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">std0</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">ddof</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">std0</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">=</span> <span class="s2">&quot;std0&quot;</span>

<span class="k">def</span> <span class="nf">standardise</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span><span class="o">/</span><span class="n">std0</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">nhanes</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s2">&quot;height_std&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">nhanes</span><span class="o">.</span>
    <span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="s2">&quot;gender&quot;</span><span class="p">]]</span><span class="o">.</span>
    <span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">)</span><span class="o">.</span>
    <span class="n">transform</span><span class="p">(</span><span class="n">standardise</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">nhanes</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<span class="c1">##    age  weight  height  bmival  gender usborn      bmicat  height_std</span>
<span class="c1">## 0   29    97.1   160.2    37.8  female     no       obese    0.015752</span>
<span class="c1">## 1   49    98.8   182.3    29.7    male    yes  overweight    1.108960</span>
<span class="c1">## 2   36    74.3   184.2    21.9    male    yes      normal    1.355671</span>
<span class="c1">## 3   68   103.7   185.3    30.2    male    yes       obese    1.498504</span>
<span class="c1">## 4   76    83.3   177.1    26.6    male    yes  overweight    0.433751</span>
</pre></div>
</div>
<p>The new column gives the <em>relative</em> z-scores:
a woman with a relative z-score of 0 has height of 160.1 cm, whereas
a man with the same z-score has height of 173.8 cm.</p>
<p>We can check that the means and standard deviations
in both groups are equal to 0 and 1:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">nhanes</span><span class="o">.</span>
    <span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="s2">&quot;height_std&quot;</span><span class="p">]]</span><span class="o">.</span>
    <span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">)</span><span class="o">.</span>
    <span class="n">aggregate</span><span class="p">([</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">std0</span><span class="p">])</span>
<span class="p">)</span>
<span class="c1">##             height              height_std     </span>
<span class="c1">##               mean      std0          mean std0</span>
<span class="c1">## gender                                         </span>
<span class="c1">## female  160.089189  7.034703 -1.351747e-15  1.0</span>
<span class="c1">## male    173.759541  7.701323  3.145329e-16  1.0</span>
</pre></div>
</div>
<p>Note that we needed to use a custom function for computing the standard
deviation with <code class="docutils literal notranslate"><span class="pre">ddof=0</span></code>. This is likely a bug in <strong class="program">pandas</strong> that
<strong class="command">nhanes.groupby</strong><code class="code docutils literal notranslate"><span class="pre">(&quot;gender&quot;).</span></code><strong class="command">aggregate</strong><code class="code docutils literal notranslate"><span class="pre">([np.std])</span></code> somewhat passes <code class="docutils literal notranslate"><span class="pre">ddof=1</span></code> to <strong class="command">numpy.std</strong>,</p>
<div class="proof proof-type-exercise" id="id29">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.3</span>
        
    </div><div class="proof-content">
<p>Create a data frame comprised of the five tallest men
and the five tallest women.</p>
</div></div></section>
<section id="manual-splitting-into-subgroups">
<h3><span class="section-number">12.1.3. </span>Manual splitting into subgroups (*)<a class="headerlink" href="#manual-splitting-into-subgroups" title="Link to this heading">#</a></h3>
<p>It turns out that <code class="docutils literal notranslate"><span class="pre">GroupBy</span></code> objects and their derivatives
are <em>iterable</em>; compare <a class="reference internal" href="130-sequential.html#sec-iterable"><span class="std std-numref">SectionÂ 3.4</span></a>. As a consequence,
the grouped data frames and series can be easily processed manually
in case where the built-in methods are insufficient (i.e.,
not so rarely).</p>
<p>Letâs consider a small sample of our data frame.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grouped</span> <span class="o">=</span> <span class="p">(</span><span class="n">nhanes</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
    <span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="s2">&quot;height&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">)</span>
<span class="p">)</span>
<span class="nb">list</span><span class="p">(</span><span class="n">grouped</span><span class="p">)</span>
<span class="c1">## [(&#39;female&#39;,    gender  weight  height</span>
<span class="c1">## 0  female    97.1   160.2), (&#39;male&#39;,   gender  weight  height</span>
<span class="c1">## 1   male    98.8   182.3</span>
<span class="c1">## 2   male    74.3   184.2</span>
<span class="c1">## 3   male   103.7   185.3</span>
<span class="c1">## 4   male    83.3   177.1)]</span>
</pre></div>
</div>
<p>The way the output is formatted is imperfect,
so we need to contemplate it for a tick.
We see that when iterating through a <code class="docutils literal notranslate"><span class="pre">GroupBy</span></code> object,
we get access to pairs giving all the levels of the grouping
variable and the subsets of the input data frame
corresponding to these categories.</p>
<p>Here is a simple example where we make use of the earlier fact:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">level</span><span class="p">,</span> <span class="n">df</span> <span class="ow">in</span> <span class="n">grouped</span><span class="p">:</span>
    <span class="c1"># level is a string label</span>
    <span class="c1"># df is a data frame - we can do whatever we want</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;There are </span><span class="si">{</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> subject(s) with gender=`</span><span class="si">{</span><span class="n">level</span><span class="si">}</span><span class="s2">`.&quot;</span><span class="p">)</span>
<span class="c1">## There are 1 subject(s) with gender=`female`.</span>
<span class="c1">## There are 4 subject(s) with gender=`male`.</span>
</pre></div>
</div>
<p>We see that splitting followed by manual processing of the chunks
in a loop is quite tedious in the case where we would merely like to compute
some basic aggregates.
These scenarios are extremely common. No wonder why the <strong class="program">pandas</strong>
developers introduced a convenient interface
in the form of the <strong class="command">pandas.DataFrame.groupby</strong>
and <strong class="command">pandas.Series.groupby</strong> methods and the <code class="docutils literal notranslate"><span class="pre">DataFrameGroupBy</span></code>
and <code class="docutils literal notranslate"><span class="pre">SeriesGroupby</span></code> classes.
Still, for more ambitious tasks, the low-level way to perform
the splitting will come in handy.</p>
<div class="proof proof-type-exercise" id="id30">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.4</span>
        
    </div><div class="proof-content">
<p>(**) Using the manual splitting and <strong class="command">matplotlib.pyplot.boxplot</strong>,
draw a box-and-whisker plot of heights grouped by BMI category
(four boxes side by side).</p>
</div></div><div class="proof proof-type-exercise" id="id31">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.5</span>
        
    </div><div class="proof-content">
<p>(**) Using the manual splitting,
compute the relative z-scores of the height
column separately for each BMI category.</p>
</div></div><div class="proof proof-type-example" id="id32">

    <div class="proof-title">
        <span class="proof-type">Example 12.6</span>
        
    </div><div class="proof-content">
<p>Letâs also demonstrate that the splitting can be done manually
without the use of <strong class="program">pandas</strong>.
Namely, calling <strong class="command">numpy.split</strong><code class="code docutils literal notranslate"><span class="pre">(a,</span> <span class="pre">ind)</span></code> returns a list
with <code class="docutils literal notranslate"><span class="pre">a</span></code> (being an array-like object, e.g., a vector, a matrix, or a data
frame) partitioned rowwisely into <strong class="command">len</strong><code class="code docutils literal notranslate"><span class="pre">(ind)+1</span></code> chunks
at indexes given by <code class="docutils literal notranslate"><span class="pre">ind</span></code>.
For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;one&quot;</span><span class="p">,</span> <span class="s2">&quot;two&quot;</span><span class="p">,</span> <span class="s2">&quot;three&quot;</span><span class="p">,</span> <span class="s2">&quot;four&quot;</span><span class="p">,</span> <span class="s2">&quot;five&quot;</span><span class="p">,</span> <span class="s2">&quot;six&quot;</span><span class="p">,</span> <span class="s2">&quot;seven&quot;</span><span class="p">,</span> <span class="s2">&quot;eight&quot;</span><span class="p">,</span> <span class="s2">&quot;nine&quot;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">]):</span>
    <span class="nb">print</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
<span class="c1">## array([&#39;one&#39;, &#39;two&#39;], dtype=&#39;&lt;U5&#39;)</span>
<span class="c1">## array([&#39;three&#39;, &#39;four&#39;, &#39;five&#39;, &#39;six&#39;], dtype=&#39;&lt;U5&#39;)</span>
<span class="c1">## array([&#39;seven&#39;, &#39;eight&#39;, &#39;nine&#39;], dtype=&#39;&lt;U5&#39;)</span>
</pre></div>
</div>
<p>To split a data frame into groups defined by a categorical column,
we can first sort it with respect to this criterion, for instance:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes_srt</span> <span class="o">=</span> <span class="n">nhanes</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s2">&quot;stable&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Then, we can use <strong class="command">numpy.unique</strong> to fetch the indexes of
the first occurrences of each series of identical labels:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">levels</span><span class="p">,</span> <span class="n">where</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">nhanes_srt</span><span class="o">.</span><span class="n">gender</span><span class="p">,</span> <span class="n">return_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">levels</span><span class="p">,</span> <span class="n">where</span>
<span class="c1">## (array([&#39;female&#39;, &#39;male&#39;], dtype=object), array([   0, 4514]))</span>
</pre></div>
</div>
<p>This can now be used for dividing the sorted data frame into chunks:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes_grp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">nhanes_srt</span><span class="p">,</span> <span class="n">where</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>  <span class="c1"># where[0] is not interesting</span>
<span class="c1">## /home/gagolews/.virtualenvs/python3-default/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: &#39;DataFrame.swapaxes&#39; is deprecated and will be removed in a future version. Please use &#39;DataFrame.transpose&#39; instead.</span>
<span class="c1">##   return bound(*args, **kwds)</span>
</pre></div>
</div>
<p>We obtained a list of data frames split at rows specified by <code class="docutils literal notranslate"><span class="pre">where[1:]</span></code>.
Here is a preview of the first and the last row in each chunk:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">levels</span><span class="p">)):</span>
    <span class="c1"># process (levels[i], nhanes_grp[i])</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;level=&#39;</span><span class="si">{</span><span class="n">levels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39;; preview:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">nhanes_grp</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">:</span> <span class="p">],</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="c1">## level=&#39;female&#39;; preview:</span>
<span class="c1">##       age  weight  height  bmival  gender usborn bmicat  height_std</span>
<span class="c1">## 0      29    97.1   160.2    37.8  female     no  obese    0.015752</span>
<span class="c1">## 8781   67    82.8   147.8    37.9  female     no  obese   -1.746938</span>
<span class="c1">## </span>
<span class="c1">## level=&#39;male&#39;; preview:</span>
<span class="c1">##       age  weight  height  bmival gender usborn      bmicat  height_std</span>
<span class="c1">## 1      49    98.8   182.3    29.7   male    yes  overweight    1.108960</span>
<span class="c1">## 8784   74    59.7   167.5    21.3   male     no      normal   -0.812788</span>
</pre></div>
</div>
<p>Within each subgroup, we can apply any operation we have learnt
so far: our imagination is the only major limiting factor.
For instance, we can aggregate some columns:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nhanes_agg</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nb">dict</span><span class="p">(</span>
        <span class="n">level</span><span class="o">=</span><span class="n">t</span><span class="o">.</span><span class="n">gender</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>  <span class="c1"># they are all the same here; take the first</span>
        <span class="n">height_mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">height</span><span class="p">),</span> <span class="mi">2</span><span class="p">),</span>
        <span class="n">weight_mean</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">weight</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">nhanes_grp</span>
<span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">nhanes_agg</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1">## {&#39;level&#39;: &#39;female&#39;, &#39;height_mean&#39;: 160.09, &#39;weight_mean&#39;: 78.35}</span>
<span class="nb">print</span><span class="p">(</span><span class="n">nhanes_agg</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="c1">## {&#39;level&#39;: &#39;male&#39;, &#39;height_mean&#39;: 173.76, &#39;weight_mean&#39;: 88.59}</span>
</pre></div>
</div>
<p>The resulting list of dictionaries can be combined to form a data frame:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">nhanes_agg</span><span class="p">)</span>
<span class="c1">##     level  height_mean  weight_mean</span>
<span class="c1">## 0  female       160.09        78.35</span>
<span class="c1">## 1    male       173.76        88.59</span>
</pre></div>
</div>
<p>Furthermore, there is a simple trick that allows grouping with respect
to more than one column. We can apply <strong class="command">numpy.unique</strong> on
a string vector that combines the levels of the grouping variables, e.g.,
by concatenating them like <code class="docutils literal notranslate"><span class="pre">nhanes_srt.gender</span> <span class="pre">+</span> <span class="pre">&quot;___&quot;</span> <span class="pre">+</span> <span class="pre">nhanes_srt.bmicat</span></code>
(assuming that <code class="docutils literal notranslate"><span class="pre">nhanes_srt</span></code> is ordered with respect to these
two criteria).</p>
</div></div></section>
</section>
<section id="plotting-data-in-groups">
<span id="sec-groupby-plot"></span><h2><span class="section-number">12.2. </span>Plotting data in groups<a class="headerlink" href="#plotting-data-in-groups" title="Link to this heading">#</a></h2>
<p>The <strong class="program">seaborn</strong> package is particularly convenient for
plotting grouped data â it is highly interoperable with <strong class="program">pandas</strong>.</p>
<section id="series-of-box-plots">
<h3><span class="section-number">12.2.1. </span>Series of box plots<a class="headerlink" href="#series-of-box-plots" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="#fig-gender-usborn-bmi"><span class="std std-numref">FigureÂ 12.1</span></a> depicts a box plot with four
boxes side by side:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;bmival&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;usborn&quot;</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">nhanes</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;Paired&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id33">
<span id="fig-gender-usborn-bmi"></span><img alt="../_images/gender-usborn-bmi-1.png" src="../_images/gender-usborn-bmi-1.png" />
<figcaption>
<p><span class="caption-number">FigureÂ 12.1 </span><span class="caption-text">The distribution of BMIs for different genders and countries of birth.</span><a class="headerlink" href="#id33" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Letâs contemplate for a while how easy it is now to compare
the BMI distributions in different groups.
Here, we have two grouping variables, as specified
by the <code class="docutils literal notranslate"><span class="pre">y</span></code> and <code class="docutils literal notranslate"><span class="pre">hue</span></code> arguments.</p>
<div class="proof proof-type-exercise" id="id34">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.7</span>
        
    </div><div class="proof-content">
<p>Create a similar series of violin plots.</p>
</div></div><div class="proof proof-type-exercise" id="id35">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.8</span>
        
    </div><div class="proof-content">
<p>(*) Add the average BMIs in each group to the
above box plot using <strong class="command">matplotlib.pyplot.plot</strong>.
Check <code class="docutils literal notranslate"><span class="pre">ylim</span></code> to determine the range on the y-axis.</p>
</div></div></section>
<section id="series-of-bar-plots">
<h3><span class="section-number">12.2.2. </span>Series of bar plots<a class="headerlink" href="#series-of-bar-plots" title="Link to this heading">#</a></h3>
<p>On the other hand, <a class="reference internal" href="#fig-gender-bmicat"><span class="std std-numref">FigureÂ 12.2</span></a>
shows a bar plot representing a contingency table.
It was obtained in a different way from that used
in <a class="reference internal" href="420-categorical.html#chap-categorical"><span class="std std-numref">ChapterÂ 11</span></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span>
    <span class="n">y</span><span class="o">=</span><span class="s2">&quot;counts&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;bmicat&quot;</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;Paired&quot;</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="p">(</span>
        <span class="n">nhanes</span><span class="o">.</span>
        <span class="n">groupby</span><span class="p">([</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="s2">&quot;bmicat&quot;</span><span class="p">],</span> <span class="n">observed</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span>
        <span class="n">size</span><span class="p">()</span><span class="o">.</span>
        <span class="n">rename</span><span class="p">(</span><span class="s2">&quot;counts&quot;</span><span class="p">)</span><span class="o">.</span>
        <span class="n">reset_index</span><span class="p">()</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id36">
<span id="fig-gender-bmicat"></span><img alt="../_images/gender-bmicat-3.png" src="../_images/gender-bmicat-3.png" />
<figcaption>
<p><span class="caption-number">FigureÂ 12.2 </span><span class="caption-text">Number of persons for each gender and BMI category.</span><a class="headerlink" href="#id36" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="proof proof-type-exercise" id="id37">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.9</span>
        
    </div><div class="proof-content">
<p>Draw a similar bar plot where the bar heights sum to 100%
for each gender.</p>
</div></div><div class="proof proof-type-exercise" id="id38">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.10</span>
        
    </div><div class="proof-content">
<p>Using the two-sample chi-squared test, verify
whether the BMI category distributions for men and women
differ significantly from each other.</p>
</div></div></section>
<section id="semitransparent-histograms">
<h3><span class="section-number">12.2.3. </span>Semitransparent histograms<a class="headerlink" href="#semitransparent-histograms" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="#fig-hist-transparent-weight-usborn"><span class="std std-numref">FigureÂ 12.3</span></a> illustrates
that playing with semitransparent objects
can make comparisons more intuitive (the <code class="docutils literal notranslate"><span class="pre">alpha</span></code> argument).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">nhanes</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;usborn&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span>
    <span class="n">element</span><span class="o">=</span><span class="s2">&quot;step&quot;</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">,</span> <span class="n">common_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id39">
<span id="fig-hist-transparent-weight-usborn"></span><img alt="../_images/hist-transparent-weight-usborn-5.png" src="../_images/hist-transparent-weight-usborn-5.png" />
<figcaption>
<p><span class="caption-number">FigureÂ 12.3 </span><span class="caption-text">The weight distribution of the US-born participants has a higher mean and variance.</span><a class="headerlink" href="#id39" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>By passing <code class="docutils literal notranslate"><span class="pre">common_norm=False</span></code>, we scaled each histogram separately,
so that it represents a density function (are under each curve is 1).
It is the behaviour we desire when the samples are of different lengths.</p>
</section>
<section id="scatter-plots-with-group-information">
<h3><span class="section-number">12.2.4. </span>Scatter plots with group information<a class="headerlink" href="#scatter-plots-with-group-information" title="Link to this heading">#</a></h3>
<p>Scatter plots for grouped data can display category information
using points of different colours and/or styles, compare
<a class="reference internal" href="#fig-height-weight-gender"><span class="std std-numref">FigureÂ 12.4</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;height&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span>
    <span class="n">data</span><span class="o">=</span><span class="n">nhanes</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">markers</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id40">
<span id="fig-height-weight-gender"></span><img alt="../_images/height-weight-gender-7.png" src="../_images/height-weight-gender-7.png" />
<figcaption>
<p><span class="caption-number">FigureÂ 12.4 </span><span class="caption-text">Weight vs height grouped by gender.</span><a class="headerlink" href="#id40" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="grid-trellis-plots">
<span id="sec-trellis"></span><h3><span class="section-number">12.2.5. </span>Grid (trellis) plots<a class="headerlink" href="#grid-trellis-plots" title="Link to this heading">#</a></h3>
<p>Grid plot (also known as trellis, panel, conditioning, or lattice plots)
are a way to visualise data separately for each factor level.
All the plots share the same coordinate ranges which makes them easily
comparable. For instance, <a class="reference internal" href="#fig-grid-weight"><span class="std std-numref">FigureÂ 12.5</span></a> depicts a series
of histograms of weights grouped by a combination of two
categorical variables.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">grid</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">FacetGrid</span><span class="p">(</span><span class="n">nhanes</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s2">&quot;gender&quot;</span><span class="p">,</span> <span class="n">row</span><span class="o">=</span><span class="s2">&quot;usborn&quot;</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">stat</span><span class="o">=</span><span class="s2">&quot;density&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id41">
<span id="fig-grid-weight"></span><img alt="../_images/grid-weight-9.png" src="../_images/grid-weight-9.png" />
<figcaption>
<p><span class="caption-number">FigureÂ 12.5 </span><span class="caption-text">Distribution of weights for different genders and countries of birth.</span><a class="headerlink" href="#id41" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="proof proof-type-exercise" id="id42">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.11</span>
        
    </div><div class="proof-content">
<p>Pass <code class="docutils literal notranslate"><span class="pre">hue=&quot;bmicat&quot;</span></code> additionally to <strong class="command">seaborn.FacetGrid</strong>.</p>
</div></div><div class="admonition important">
<p class="admonition-title">Important</p>
<p>Grid plots can feature any kind of data visualisation we have discussed
so far (e.g., histograms, bar plots, scatter plots).</p>
</div>
<div class="proof proof-type-exercise" id="id43">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.12</span>
        
    </div><div class="proof-content">
<p>Draw a trellis plot with scatter plots of weight vs height
grouped by BMI category and gender.</p>
</div></div></section>
<section id="kolmogorovsmirnov-test-for-comparing-ecdfs">
<span id="sec-ks-test2"></span><h3><span class="section-number">12.2.6. </span>KolmogorovâSmirnov test for comparing ECDFs (*)<a class="headerlink" href="#kolmogorovsmirnov-test-for-comparing-ecdfs" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="#fig-ecdf-weight-usborn"><span class="std std-numref">FigureÂ 12.6</span></a> compares the empirical
cumulative distribution functions of the weight distributions
for US and non-US born participants.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">usborn</span><span class="p">,</span> <span class="n">weight</span> <span class="ow">in</span> <span class="n">nhanes</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;usborn&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span><span class="p">:</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">ecdfplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">usborn</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;usborn&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id44">
<span id="fig-ecdf-weight-usborn"></span><img alt="../_images/ecdf-weight-usborn-11.png" src="../_images/ecdf-weight-usborn-11.png" />
<figcaption>
<p><span class="caption-number">FigureÂ 12.6 </span><span class="caption-text">Empirical cumulative distribution functions of weight distributions for different birthplaces.</span><a class="headerlink" href="#id44" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>We have used manual splitting of the <code class="docutils literal notranslate"><span class="pre">weight</span></code> column
into subgroups and then plotted the two ECDFs separately
because a call to
<strong class="command">seaborn.ecdfplot</strong><code class="code docutils literal notranslate"><span class="pre">(data=nhanes,</span> <span class="pre">x=&quot;weight&quot;,</span> <span class="pre">hue=&quot;usborn&quot;)</span></code>
does not honour our wish to use alternating lines styles
(most likely due to a bug).</p>
<div style="margin-top: 1em"></div><p>A two-sample KolmogorovâSmirnov test can be used
to check whether two ECDFs <span class="math">\(\hat{F}_n'\)</span>
(e.g., the weight of the US-born participants)
and <span class="math">\(\hat{F}_m''\)</span> (e.g., the weight of non-US-born persons)
are significantly different from each other:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
\left\{
\begin{array}{rll}
H_0: & \hat{F}_n' = \hat{F}_n'' & \text{(null hypothesis)}\\
H_1: & \hat{F}_n' \neq \hat{F}_n'' & \text{(two-sided alternative)} \\
\end{array}
\right.
\]</div>
</div>
<p>The test statistic will be a variation of the one-sample setting
discussed in <a class="reference internal" href="230-distribution.html#sec-ks-test"><span class="std std-numref">SectionÂ 6.2.3</span></a>. Namely, let:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[\hat{D}_{n,m} = \sup_{t\in\mathbb{R}} | \hat{F}_n'(t) - \hat{F}_m''(t) |.\]</div>
</div>
<p>Computing it is slightly trickier than
in the previous case<a class="footnote-reference brackets" href="#footintroalgs" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a>. Luckily,
an appropriate procedure is available in <strong class="program">scipy.stats</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x12</span> <span class="o">=</span> <span class="n">nhanes</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;usborn&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">weight</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">x12</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s2">&quot;yes&quot;</span><span class="p">]</span>  <span class="c1"># the first sample</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">x12</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s2">&quot;no&quot;</span><span class="p">]</span>   <span class="c1"># the second sample</span>
<span class="n">Dnm</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">ks_2samp</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">Dnm</span>
<span class="c1">## 0.22068075889911914</span>
</pre></div>
</div>
<p>Assuming significance level <span class="math">\(\alpha=0.001\)</span>,
the critical value is approximately (for larger <span class="math">\(n\)</span> and <span class="math">\(m\)</span>)
equal to:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
K_{n,m} = \sqrt{
-\frac{ \log(\alpha/2) (n+m) }{ 2nm }
}.
\]</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span><span class="o">+</span><span class="nb">len</span><span class="p">(</span><span class="n">x2</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">x2</span><span class="p">)))</span>
<span class="c1">## 0.04607410479813944</span>
</pre></div>
</div>
<p>As usual, we reject the null hypothesis
when <span class="math">\(\hat{D}_{n,m}\ge K_{n,m}\)</span>, which is exactly the case here
(at significance level <span class="math">\(0.1\%\)</span>).
In other words, weights of US- and non-US-born participants
differ significantly.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Frequentist hypothesis testing only takes into account the deviation
between distributions that is explainable due to sampling effects
(the assumed randomness of the data generation process).
For large sample sizes, even very small deviations<a class="footnote-reference brackets" href="#footerrdev" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a> will be
deemed <em>statistically significant</em>, but it does not mean that we
consider them as <em>practically significant</em>.</p>
<p>For instance, we might discover that a very costly, environmentally
unfriendly, and generally inconvenient for everyone upgrade leads
to a processâ improvement: we reject the null hypothesis stating that two
distributions are equal. Nevertheless, a careful inspection
told us that the gains are roughly 0.5%. In such a case, it is worthwhile
to apply good old common sense and refrain from implementing it.</p>
</div>
<div class="proof proof-type-exercise" id="id45">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.13</span>
        
    </div><div class="proof-content">
<p>Compare between the ECDFs of weights
of men and women who are between 18 and 25 years old.
Determine whether they are significantly different.</p>
</div></div><div class="admonition important">
<p class="admonition-title">Important</p>
<p>Some statistical textbooks and many research papers in the social
sciences (amongst many others) employ the significance level
of <span class="math">\(\alpha=5\%\)</span>, which is often criticised as too high<a class="footnote-reference brackets" href="#footpvalue" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>.
Many stakeholders aggressively push towards
constant improvements in terms of inventing bigger, better, faster,
more efficient things. In this context, larger <span class="math">\(\alpha\)</span> generates more
<em>sensational</em> discoveries: it considers
smaller differences as already significant.
This all adds to what we call the reproducibility crisis
in the empirical sciences.</p>
<p>We, on the other hand, claim that it is better to err on the side
of being cautious. This, in the long run, is more sustainable.</p>
</div>
</section>
<section id="comparing-quantiles">
<h3><span class="section-number">12.2.7. </span>Comparing quantiles<a class="headerlink" href="#comparing-quantiles" title="Link to this heading">#</a></h3>
<p>Plotting quantiles in two samples against each other
can also give us some further (informal) insight with regard to the
possible distributional differences. <a class="reference internal" href="#fig-qqplot2"><span class="std std-numref">FigureÂ 12.7</span></a> depicts an example
Q-Q plot (see also the one-sample version in <a class="reference internal" href="230-distribution.html#sec-qqplot"><span class="std std-numref">SectionÂ 6.2.2</span></a>),
where we see that the distributions have similar shapes (points
more or less lie on a straight line), but they are shifted and/or
scaled (if they were, they would be on the identity line).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">nhanes</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">nhanes</span><span class="o">.</span><span class="n">usborn</span> <span class="o">==</span> <span class="s2">&quot;yes&quot;</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">nhanes</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">nhanes</span><span class="o">.</span><span class="n">usborn</span> <span class="o">==</span> <span class="s2">&quot;no&quot;</span><span class="p">]</span>
<span class="n">xd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">yd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">xd</span><span class="p">)</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">yd</span><span class="p">):</span>  <span class="c1"># interpolate between quantiles in a longer sample</span>
    <span class="n">xd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">xd</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">yd</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">yd</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">yd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">yd</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">xd</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">xd</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xd</span><span class="p">,</span> <span class="n">yd</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axline</span><span class="p">((</span><span class="n">xd</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">xd</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span><span class="p">],</span> <span class="n">xd</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">xd</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span><span class="p">]),</span> <span class="n">slope</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;gray&quot;</span><span class="p">)</span>  <span class="c1"># identity line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample quantiles (weight; usborn=yes)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sample quantiles (weight; usborn=no)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id46">
<span id="fig-qqplot2"></span><img alt="../_images/qqplot2-13.png" src="../_images/qqplot2-13.png" />
<figcaption>
<p><span class="caption-number">FigureÂ 12.7 </span><span class="caption-text">A two-sample Q-Q plot.</span><a class="headerlink" href="#id46" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Notice that we interpolated between the quantiles in a larger sample
to match the length of the shorter vector.</p>
</section>
</section>
<section id="classification-tasks">
<h2><span class="section-number">12.3. </span>Classification tasks (*)<a class="headerlink" href="#classification-tasks" title="Link to this heading">#</a></h2>
<p>Consider a small sample of white, rather sweet wines
from a much larger <a class="reference external" href="http://archive.ics.uci.edu/ml/datasets/Wine+Quality">wine quality</a> dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">wine_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
        <span class="s2">&quot;teaching-data/master/other/sweetwhitewine_train2.csv&quot;</span><span class="p">,</span>
        <span class="n">comment</span><span class="o">=</span><span class="s2">&quot;#&quot;</span><span class="p">)</span>
<span class="n">wine_train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<span class="c1">##      alcohol      sugar  bad</span>
<span class="c1">## 0  10.625271  10.340159    0</span>
<span class="c1">## 1   9.066111  18.593274    1</span>
<span class="c1">## 2  10.806395   6.206685    0</span>
<span class="c1">## 3  13.432876   2.739529    0</span>
<span class="c1">## 4   9.578162   3.053025    0</span>
</pre></div>
</div>
<p>We are given each wineâs alcohol and residual sugar content,
as well as a binary categorical variable stating whether a group of
sommeliers deem a given beverage quite bad (1) or not (0).
<a class="reference internal" href="#fig-wines"><span class="std std-numref">FigureÂ 12.8</span></a> reveals
that subpar wines are rather low inâ¦ alcohol and, to some extent, sugar.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;alcohol&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;sugar&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">wine_train</span><span class="p">,</span>
    <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;bad&quot;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;bad&quot;</span><span class="p">,</span> <span class="n">markers</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="s2">&quot;v&quot;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;alcohol&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;sugar&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;bad&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id47">
<span id="fig-wines"></span><img alt="../_images/wines-15.png" src="../_images/wines-15.png" />
<figcaption>
<p><span class="caption-number">FigureÂ 12.8 </span><span class="caption-text">Scatter plot for sugar vs alcohol content for white, rather sweet wines, and whether they are considered bad (1) or drinkable (0) by some experts.</span><a class="headerlink" href="#id47" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Someone answer the door! We have a delivery of
a few new wine bottles. Interestingly, their alcohol and sugar contents have
been given on their respective labels.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">wine_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
        <span class="s2">&quot;teaching-data/master/other/sweetwhitewine_test2.csv&quot;</span><span class="p">,</span>
        <span class="n">comment</span><span class="o">=</span><span class="s2">&quot;#&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">wine_test</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<span class="c1">##      alcohol      sugar</span>
<span class="c1">## 0   9.315523  10.041971</span>
<span class="c1">## 1  12.909232   6.814249</span>
<span class="c1">## 2   9.051020  12.818683</span>
<span class="c1">## 3   9.567601  11.091827</span>
<span class="c1">## 4   9.494031  12.053790</span>
</pre></div>
</div>
<p>We would like to determine which of the wines from the test
set might be not-bad without asking an expert for their opinion.
In other words, we would like to exercise a <em>classification</em> task
(see, e.g., <span id="id7">[<a class="reference internal" href="999-bibliography.html#id56" title="Bishop, C. (2006).  Pattern Recognition and Machine Learning. Springer-Verlag. URL: https://www.microsoft.com/en-us/research/people/cmbishop.">9</a>, <a class="reference internal" href="999-bibliography.html#id55" title="Hastie, T., Tibshirani, R., and Friedman, J. (2017).  The Elements of Statistical Learning. Springer-Verlag. URL: https://hastie.su.domains/ElemStatLearn.">48</a>]</span>). More formally:</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Assume we are given a set of training points
<span class="math">\(\mathbf{X}\in\mathbb{R}^{n\times m}\)</span>
and the corresponding reference outputs
<span class="math">\(\boldsymbol{y}\in\{L_1,L_2,\dots,L_l\}^n\)</span> in the form of a categorical
variable with <span class="math">\(l\)</span> distinct levels.
The aim of a <em>classification</em> algorithm is to predict
what the outputs for each point from a possibly different
dataset <span class="math">\(\mathbf{X}'\in\mathbb{R}^{n'\times m}\)</span>, i.e.,
<span class="math">\(\hat{\boldsymbol{y}}'\in\{L_1,L_2,\dots,L_l\}^{n'}\)</span>, might be.</p>
</div>
<p>In other words, we are asked to fill the gaps in a categorical
variable.
Recall that in a regression problem (<a class="reference internal" href="330-relationship.html#sec-regression"><span class="std std-numref">SectionÂ 9.2</span></a>),
the reference outputs were numerical.</p>
<div class="proof proof-type-exercise" id="id48">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.14</span>
        
    </div><div class="proof-content">
<p>Which of the following are instances of classification problems
and which are regression tasks?</p>
<ul class="simple">
<li><p>Detect email spam.</p></li>
<li><p>Predict a market stock price (good luck with that).</p></li>
<li><p>Assess credit risk.</p></li>
<li><p>Detect tumour tissues in medical images.</p></li>
<li><p>Predict the time-to-recovery of cancer patients.</p></li>
<li><p>Recognise smiling faces on photographs (kind of creepy).</p></li>
<li><p>Detect unattended luggage in airport security camera footage.</p></li>
</ul>
<p>What kind of data should you gather to tackle them?</p>
</div></div><section id="k-nearest-neighbour-classification">
<span id="sec-knn-classification"></span><h3><span class="section-number">12.3.1. </span><em>K</em>-nearest neighbour classification (*)<a class="headerlink" href="#k-nearest-neighbour-classification" title="Link to this heading">#</a></h3>
<p>One of the simplest approaches to classification
is based on the information about a test pointâs nearest neighbours
living in the training sample; compare <a class="reference internal" href="320-transform-matrix.html#sec-knn-vs-radius"><span class="std std-numref">SectionÂ 8.4.4</span></a>.</p>
<p>Fix <span class="math">\(k\ge 1\)</span>.
Namely, to classify some <span class="math">\(\boldsymbol{x}'\in\mathbb{R}^m\)</span>:</p>
<ol class="arabic">
<li><p>Find the indexes <span class="math">\(N_k(\boldsymbol{x}')=\{i_1,\dots,i_k\}\)</span>
of the <span class="math">\(k\)</span> points from
<span class="math">\(\mathbf{X}\)</span> closest to <span class="math">\(\boldsymbol{x}'\)</span>, i.e., ones that fulfil
for all <span class="math">\(j\not\in\{i_1,\dots,i_k\}\)</span>:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[ \|\mathbf{x}_{i_1,\cdot}-\boldsymbol{x}'\|
    \le\dots\le
    \| \mathbf{x}_{i_k,\cdot} -\boldsymbol{x}' \|
    \le
    \| \mathbf{x}_{j,\cdot} -\boldsymbol{x}' \|.
    \]</div>
</div>
</li>
<li><p>Classify <span class="math">\(\boldsymbol{x}'\)</span>
as <span class="math">\(\hat{y}'=\mathrm{mode}(y_{i_1},\dots,y_{i_k})\)</span>,
i.e., assign it the label that most frequently occurs
amongst its <span class="math">\(k\)</span> nearest neighbours.
If a mode is nonunique, resolve the ties at random.</p></li>
</ol>
<p>It is thus a similar algorithm to
<span class="math">\(k\)</span>-nearest neighbour regression (<a class="reference internal" href="330-relationship.html#sec-knn-regression"><span class="std std-numref">SectionÂ 9.2.1</span></a>).
We only replaced the <em>quantitative</em> mean with the <em>qualitative</em> mode.</p>
<p>This is a variation on the theme: if you donât know what to do
in a given situation, try to mimic what the majority of people around you
are doing or saying. For instance, if you donât know what to think
about a particular wine, discover that amongst the five most
similar ones (in terms of alcohol and sugar content)
three are said to be awful. Now you can claim that you donât like
it because itâs not sweet enough. Thanks to this, others will
take you for a very refined wine taster.</p>
<p>Letâs apply a 5-nearest neighbour classifier on the standardised version
of the dataset. As we are about to use a technique based on pairwise
distances, it would be best if the variables were on the same scale.
Thus, we first compute the z-scores for the training set:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">wine_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;alcohol&quot;</span><span class="p">,</span> <span class="s2">&quot;sugar&quot;</span><span class="p">]])</span>
<span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">sds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">Z_train</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_train</span><span class="o">-</span><span class="n">means</span><span class="p">)</span><span class="o">/</span><span class="n">sds</span>
</pre></div>
</div>
<p>Then, we determine the z-scores for the test set:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Z_test</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">wine_test</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;alcohol&quot;</span><span class="p">,</span> <span class="s2">&quot;sugar&quot;</span><span class="p">]])</span><span class="o">-</span><span class="n">means</span><span class="p">)</span><span class="o">/</span><span class="n">sds</span>
</pre></div>
</div>
<p>Letâs stress that we referred to the aggregates computed
for the training set. This is a representative example of a situation where
we cannot simply use a built-in method from <strong class="command">pandas</strong>.
Instead, we apply what we have learnt about <strong class="command">numpy</strong>.</p>
<p>To make the predictions, we will use the following function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">knn_class</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="n">nnis</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">spatial</span><span class="o">.</span><span class="n">KDTree</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">k</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">nnls</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[</span><span class="n">nnis</span><span class="p">]</span>  <span class="c1"># same as: y_train[nnis.reshape(-1)].reshape(-1, k)</span>
    <span class="k">return</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="n">nnls</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>First, we fetched the indexes of each test pointâs nearest neighbours
(amongst the points in the training set).
Then, we read their corresponding labels; they are stored
in a matrix with <span class="math">\(k\)</span> columns.
Finally, we computed the modes in each row.
As a consequence, we have each point in the test set
classified.</p>
<p>And now:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">wine_train</span><span class="o">.</span><span class="n">bad</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">knn_class</span><span class="p">(</span><span class="n">Z_test</span><span class="p">,</span> <span class="n">Z_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
<span class="n">y_pred</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>  <span class="c1"># preview</span>
<span class="c1">## array([1, 0, 0, 1, 1, 0, 1, 0, 0, 1])</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unfortunately, <strong class="command">scipy.stats.mode</strong> does not resolve possible
ties at random: e.g., the mode of <span class="math">\((1, 1, 1, 2, 2, 2)\)</span> is always 1.
Nevertheless, in our case, <span class="math">\(k\)</span> is odd and the number
of possible classes is <span class="math">\(l=2\)</span>, so the mode is always unique.</p>
</div>
<p><a class="reference internal" href="#fig-knn-class"><span class="std std-numref">FigureÂ 12.9</span></a> shows how nearest neighbour classification
categorises different regions of a section of the two-dimensional
plane. The greater the <span class="math">\(k\)</span>, the smoother the decision boundaries.
Naturally, in regions corresponding to few training points, we do not
expect the classification accuracy to be acceptable<a class="footnote-reference brackets" href="#football" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Z_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">Z_train</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">Z_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">Z_train</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">xg1</span><span class="p">,</span> <span class="n">xg2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">)</span>
<span class="n">Xg12</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">xg1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">xg2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
<span class="n">ks</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">25</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ks</span><span class="p">)):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">ks</span><span class="p">),</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">yg12</span> <span class="o">=</span> <span class="n">knn_class</span><span class="p">(</span><span class="n">Xg12</span><span class="p">,</span> <span class="n">Z_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">ks</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Z_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Z_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">c</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Z_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Z_train</span><span class="p">[</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">c</span><span class="o">=</span><span class="s2">&quot;#DF536B&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;v&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">yg12</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x2</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)),</span>
        <span class="n">cmap</span><span class="o">=</span><span class="s2">&quot;gist_heat&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;$k=</span><span class="si">{</span><span class="n">ks</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2">$&quot;</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;alcohol&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;sugar&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id49">
<span id="fig-knn-class"></span><img alt="../_images/knn-class-17.png" src="../_images/knn-class-17.png" />
<figcaption>
<p><span class="caption-number">FigureÂ 12.9 </span><span class="caption-text"><span class="math">\(k\)</span>-nearest neighbour classification of a whole, dense, two-dimensional grid of points for different <span class="math">\(k\)</span>.</span><a class="headerlink" href="#id49" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="proof proof-type-example" id="id50">

    <div class="proof-title">
        <span class="proof-type">Example 12.15</span>
        
    </div><div class="proof-content">
<p>(*) The same with the <strong class="program">scikit-learn</strong> package:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sklearn.neighbors</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">neighbors</span><span class="o">.</span><span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
<span class="n">knn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Z_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred2</span> <span class="o">=</span> <span class="n">knn</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Z_test</span><span class="p">)</span>
</pre></div>
</div>
<p>We can verify that the results match by calling:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">y_pred2</span> <span class="o">==</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="c1">## True</span>
</pre></div>
</div>
</div></div></section>
<section id="assessing-prediction-quality">
<span id="sec-classifier-metrics"></span><h3><span class="section-number">12.3.2. </span>Assessing prediction quality (*)<a class="headerlink" href="#assessing-prediction-quality" title="Link to this heading">#</a></h3>
<p>It is time to reveal the truth: our test wines, it turns out,
have already been assessed by some experts.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
        <span class="s2">&quot;teaching-data/master/other/sweetwhitewine_test2.csv&quot;</span><span class="p">,</span>
        <span class="n">comment</span><span class="o">=</span><span class="s2">&quot;#&quot;</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">bad</span><span class="p">)</span>
<span class="n">y_test</span><span class="p">[:</span><span class="mi">10</span><span class="p">]</span>  <span class="c1"># preview</span>
<span class="c1">## array([1, 0, 0, 1, 0, 0, 1, 0, 1, 1])</span>
</pre></div>
</div>
<div style="margin-top: 1em"></div><p>The <em>accuracy</em> score is the most straightforward measure of the similarity
between these true labels (denoted <span class="math">\(\boldsymbol{y}'=(y_1',\dots,y_{n'}')\)</span>)
and the ones predicted by the classifier
(denoted <span class="math">\(\hat{\boldsymbol{y}}'=(\hat{y}_1',\dots,\hat{y}_{n'}')\)</span>).
It is defined as a ratio between the correctly classified instances
and all the instances:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
\text{Accuracy}(\boldsymbol{y}', \hat{\boldsymbol{y}}') =
\frac{\sum_{i=1}^{n'} \mathbf{1}(y_i' = \hat{y}_i')}{n'},
\]</div>
</div>
<p>where the <em>indicator function</em>
<span class="math">\(\mathbf{1}(y_i' = \hat{y}_i')=1\)</span> if and only if
<span class="math">\(y_i' = \hat{y}_i'\)</span> and <span class="math">\(0\)</span> otherwise.
Computing it for our test sample gives:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_test</span> <span class="o">==</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="c1">## 0.706</span>
</pre></div>
</div>
<p>Thus, 71% of the wines were correctly classified with regard to their
true quality. Before we get too enthusiastic,
letâs note that our dataset is slightly <em>imbalanced</em>
in terms of the distribution of label counts:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>  <span class="c1"># contingency table</span>
<span class="c1">## 0    330</span>
<span class="c1">## 1    170</span>
<span class="c1">## Name: count, dtype: int64</span>
</pre></div>
</div>
<p>It turns out that the majority of the wines (330 out of 500)
in our sample are <em>truly</em> delicious.
Notice that a dummy classifier which labels <em>all</em> the wines as great
would have accuracy of 66%. Our <span class="math">\(k\)</span>-nearest neighbour approach
to wine quality assessment is not that usable after all.</p>
<div style="margin-top: 1em"></div><p>It is therefore always beneficial to analyse the corresponding
<em>confusion matrix</em>, which is a two-way contingency table
summarising the correct decisions and errors we make.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="nb">dict</span><span class="p">(</span><span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="o">=</span><span class="n">y_test</span><span class="p">)</span>
<span class="p">)</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">fill_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">C</span>
<span class="c1">## y_test    0   1</span>
<span class="c1">## y_pred         </span>
<span class="c1">## 0       272  89</span>
<span class="c1">## 1        58  81</span>
</pre></div>
</div>
<div style="margin-top: 1em"></div><p>In the binary classification case (<span class="math">\(l=2\)</span>) such as this one,
its entries are usually referred to as (see also the table below):</p>
<ul class="simple">
<li><p>TN â the number of cases where the true <span class="math">\(y_i'=0\)</span> and
the predicted <span class="math">\(\hat{y}_i'=0\)</span> (true negative),</p></li>
<li><p>TP â the number of instances such that the true <span class="math">\(y_i'=1\)</span> and
the predicted <span class="math">\(\hat{y}_i'=1\)</span> (true positive),</p></li>
<li><p>FN â how many times the true <span class="math">\(y_i'=1\)</span> but
the predicted <span class="math">\(\hat{y}_i'=0\)</span> (false negative),</p></li>
<li><p>FN â how many times the true <span class="math">\(y_i'=0\)</span> but the predicted <span class="math">\(\hat{y}_i'=1\)</span>
(false positive).</p></li>
</ul>
<p>The terms <em>positive</em> and <em>negative</em> refer to
the output predicted by a classifier, i.e., they indicate whether some
<span class="math">\(\hat{y}_i'\)</span> is equal to 1 and 0, respectively.</p>
<div class="table-wrapper colwidths-auto docutils container" id="id51">
<table class="docutils align-default" id="id51">
<caption><span class="caption-number">TableÂ 12.1 </span><span class="caption-text">The different cases of true vs predicted labels in a binary classification task <span class="math">\((l=2)\)</span></span><a class="headerlink" href="#id51" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p><span class="math">\(y_i'=0\)</span></p></th>
<th class="head"><p><span class="math">\(y_i'=1\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math">\(\hat{y}_i'=0\)</span></p></td>
<td><p><strong>True Negative</strong></p></td>
<td><p>False Negative (Type II error)</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math">\(\hat{y}_i'=1\)</span></p></td>
<td><p>False Positive (Type I error)</p></td>
<td><p><strong>True Positive</strong></p></td>
</tr>
</tbody>
</table>
</div>
<p>Ideally, the number of false positives and false negatives
should be as low as possible. The accuracy score only takes the raw
number of true negatives (TN) and true positives (TP)
into account:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
\text{Accuracy}(\boldsymbol{y}', \hat{\boldsymbol{y}}') =
\frac{\text{TN}+\text{TP}}{\text{TN}+\text{TP}+\text{FN}+\text{FP}}.
\]</div>
</div>
<p>Consequently, it might not be a valid metric in imbalanced
classification problems.</p>
<p>There are, fortunately, some more meaningful measures in the case where
class 1 is less prevalent and where mispredicting it is considered
more hazardous than making an inaccurate prediction with respect to class 0.
After all, most will agree that it is better to be surprised
by a vino mislabelled as bad, than be disappointed with
a highly recommended product where we have already
built some expectations around it.
Further, getting a virus infection not recognised where we are genuinely
sick can be more dangerous for the people around us than
being asked to stay at home with nothing but a headache.</p>
<div style="margin-top: 1em"></div><p><em>Precision</em> answers the question: If the classifier outputs 1,
what is the probability that this is indeed true?</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
\text{Precision}(\boldsymbol{y}', \hat{\boldsymbol{y}}') =
\frac{\text{TP}}{\text{TP}+\text{FP}}
=
\frac{\sum_{i=1}^{n'} y_i' \hat{y}_i'}{\sum_{i=1}^{n'} \hat{y}_i'}
.
\]</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>  <span class="c1"># convert to matrix</span>
<span class="n">C</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">C</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># precision</span>
<span class="c1">## 0.5827338129496403</span>
<span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_test</span><span class="o">*</span><span class="n">y_pred</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>  <span class="c1"># equivalently</span>
<span class="c1">## 0.5827338129496403</span>
</pre></div>
</div>
<p>When a classifier labels a vino as bad, in 58% of cases
it is veritably undrinkable.</p>
<div style="margin-top: 1em"></div><p><em>Recall</em> (sensitivity, hit rate, or true positive rate)
addresses the question:
If the true class is 1, what is the probability that the classifier
will detect it?</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
\text{Recall}(\boldsymbol{y}', \hat{\boldsymbol{y}}') =
\frac{\text{TP}}{\text{TP}+\text{FN}}
=
\frac{\sum_{i=1}^{n'} y_i' \hat{y}_i'}{\sum_{i=1}^{n'} {y}_i'}.
\]</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">C</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="n">C</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># recall</span>
<span class="c1">## 0.4764705882352941</span>
<span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_test</span><span class="o">*</span><span class="n">y_pred</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>  <span class="c1"># equivalently</span>
<span class="c1">## 0.4764705882352941</span>
</pre></div>
</div>
<p>Only 48% of the really bad wines will be filtered out
by the classifier.</p>
<div style="margin-top: 1em"></div><p>The <em>F measure</em> (or <span class="math">\(F_1\)</span> measure), is the harmonic<a class="footnote-reference brackets" href="#foothar" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a> mean of
precision and recall in the case where we would rather have them
aggregated into a single number:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
\text{F}(\boldsymbol{y}', \hat{\boldsymbol{y}}') = \frac{1}{
    \frac{
        \frac{1}{\text{Precision}}+\frac{1}{\text{Recall}}
    }{2}
}
=
\left(
\frac{1}{2}
\left(
\text{Precision}^{-1}+\text{Recall}^{-1}
\right)
\right)^{-1}
=
\frac{\text{TP}}{\text{TP} + \frac{\text{FP} + \text{FN}}{2}}.
\]</div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">C</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="p">(</span><span class="n">C</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mf">0.5</span><span class="o">*</span><span class="n">C</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">+</span><span class="mf">0.5</span><span class="o">*</span><span class="n">C</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># F</span>
<span class="c1">## 0.5242718446601942</span>
</pre></div>
</div>
<p>Overall, we can conclude that our classifier is rather weak.</p>
<div class="proof proof-type-exercise" id="id52">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.16</span>
        
    </div><div class="proof-content">
<p>Would you use precision or recall in the following settings:</p>
<ul class="simple">
<li><p>medical diagnosis,</p></li>
<li><p>medical screening,</p></li>
<li><p>suggestions of potential matches in a dating app,</p></li>
<li><p>plagiarism detection,</p></li>
<li><p>wine recommendation?</p></li>
</ul>
</div></div></section>
<section id="splitting-into-training-and-test-sets">
<span id="sec-train-test-split"></span><h3><span class="section-number">12.3.3. </span>Splitting into training and test sets (*)<a class="headerlink" href="#splitting-into-training-and-test-sets" title="Link to this heading">#</a></h3>
<p>The training set was used as a source of knowledge about our
problem domain. The <span class="math">\(k\)</span>-nearest neighbour classifier is technically
<em>model-free</em>. As a consequence, to generate a new prediction, we need
to be able to query all the points in the database every time.</p>
<p>Nonetheless, most statistical/machine learning algorithms, by construction,
generalise the patterns discovered in the dataset in the form
of mathematical functions (oftentimes, very complicated ones),
that are fitted by minimising some error metric.
Linear regression analysis by means of the least squares approximation
uses exactly this kind of approach.
Logistic regression for a binary response variable
would be a conceptually similar classifier, but it is beyond
our introductory course.</p>
<p>Either way, we used a separate <em>test set</em> to verify the quality
of our classifier on so-far <em>unobserved</em> data, i.e., its <em>predictive</em>
capabilities. We do not want our model to fit to the
training data too closely. This could lead to its being completely useless
when filling the gaps between the points it was exposed to.
This is like being a student who
can only repeat what the teacher says, and when faced with a slightly
different real-world problem, they panic and say complete gibberish.</p>
<p>In the preceding example, the training and test sets were created by yours
truly. Normally, however, the data scientists split a single data frame
into two parts themselves; see <a class="reference internal" href="410-data-frame.html#sec-random-sampling"><span class="std std-numref">SectionÂ 10.5.4</span></a>.
This way, they can <em>mimic</em> the situation
where some <em>test</em> observations become available after the learning
phase is complete.</p>
</section>
<section id="validating-many-models-parameter-selection">
<span id="sec-model-validation"></span><h3><span class="section-number">12.3.4. </span>Validating many models (parameter selection) (**)<a class="headerlink" href="#validating-many-models-parameter-selection" title="Link to this heading">#</a></h3>
<p>In statistical modelling, there often are many
<em>hyperparameters</em> that need to be tweaked. For example:</p>
<ul class="simple">
<li><p>which independent variables should be used for model building,</p></li>
<li><p>what is the best way to preprocess them; e.g., which of them should be
standardised,</p></li>
<li><p>if an algorithm has some tunable parameters, what is their
best combination; for instance, which <span class="math">\(k\)</span> should we use in the
<span class="math">\(k\)</span>-nearest neighbours search.</p></li>
</ul>
<p>At initial stages of data analysis, we usually tune them up by trial and
error. Later, but this is already beyond the scope of this introductory
course, we are used to exploring all the possible combinations thereof
(exhaustive grid search) or making use of some local search-based
heuristics (e.g., greedy optimisers such as hill climbing).</p>
<p>These always involve verifying the performance of <em>many</em> different classifiers,
for example, 1-, 3-, 9, and 15-nearest neighbours-based ones.
For each of them, we need to compute separate quality metrics,
e.g., the F measures. Then, we promote the classifier which enjoys
the highest score. Unfortunately, if we do it recklessly,
this can lead to <em>overfitting</em>, this time with respect to the test set.
The obtained metrics might be too optimistic and can poorly reflect
the real performance of the solution on future data.</p>
<p>Assuming that our dataset carries a decent number of observations,
to overcome this problem, we can perform a random
<em>training/validation/test split</em>:</p>
<ul class="simple">
<li><p><em>training sample</em>  (e.g., 60% of randomly chosen rows) â
for model construction,</p></li>
<li><p><em>validation sample</em> (e.g., 20%) â
used to tune the hyperparameters of many classifiers and to choose
the best one,</p></li>
<li><p><em>test (hold-out) sample</em> (e.g., the remaining 20%) â used to assess
the goodness of fit of the best classifier.</p></li>
</ul>
<p>This common sense-based approach is not limited to classification.
We can validate different regression
models in the same way.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>We would like to obtain a valid estimate of a classifierâs performance on
previously unobserved data. For this reason, the test (hold-out) sample
must neither be used in the training nor the validation phase.</p>
</div>
<div class="proof proof-type-exercise" id="id53">
<span id="exercise-knnfmeasurewine"></span>
    <div class="proof-title">
        <span class="proof-type">Exercise 12.17</span>
        
    </div><div class="proof-content">
<p>Determine the <em>best</em> parameter setting
for the <span class="math">\(k\)</span>-nearest neighbour classification of the <code class="docutils literal notranslate"><span class="pre">color</span></code> variable
based on standardised versions of
some physicochemical features (chosen columns) of wines
in the <a class="reference external" href="https://github.com/gagolews/teaching-data/raw/master/other/wine_quality_all.csv"><code class="docutils literal notranslate"><span class="pre">wine_quality_all</span></code></a> dataset.
Create a 60/20/20% dataset split.
For each <span class="math">\(k=1, 3, 5, 7, 9\)</span>, compute the corresponding
F measure on the validation test.
Evaluate the quality of the best classifier on the test set.</p>
</div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>(*)
Instead of a training/validation/test split,
we can use various <em>cross-validation</em> techniques, especially
on smaller datasets.
For instance, in a <em>5-fold cross-validation</em>, we split the original
training set randomly into five disjoint parts: <span class="math">\(A, B, C, D, E\)</span>
(more or less of the same size). We use each
combination of four chunks as training sets and the remaining part
as the validation set, for which we generate the predictions and then
compute, say, the F measure:</p>
<div class="table-wrapper colwidths-auto docutils container">
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>training set</p></th>
<th class="head"><p>validation set</p></th>
<th class="head"><p>F measure</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math">\(B\cup C\cup D\cup E\)</span></p></td>
<td><p><span class="math">\(A\)</span></p></td>
<td><p><span class="math">\(F_A\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math">\(A\cup C\cup D\cup E\)</span></p></td>
<td><p><span class="math">\(B\)</span></p></td>
<td><p><span class="math">\(F_B\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math">\(A\cup B\cup D\cup E\)</span></p></td>
<td><p><span class="math">\(C\)</span></p></td>
<td><p><span class="math">\(F_C\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><span class="math">\(A\cup B\cup C\cup E\)</span></p></td>
<td><p><span class="math">\(D\)</span></p></td>
<td><p><span class="math">\(F_D\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math">\(A\cup B\cup C\cup D\)</span></p></td>
<td><p><span class="math">\(E\)</span></p></td>
<td><p><span class="math">\(F_E\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<p>In the end, we can determine the average F measure,
<span class="math">\((F_A+F_B+F_C+F_D+F_E)/5\)</span>, as a basis for assessing different
classifiersâ quality.</p>
<p>Once the best classifier is chosen, we can use the whole
training sample to fit the final model and then consider the separate
test sample to assess its quality.</p>
<p>Furthermore, for highly imbalanced labels,
some form of <em>stratified sampling</em> might be necessary.
Such problems are typically explored in more advanced courses in
statistical learning.</p>
</div>
<div class="proof proof-type-exercise" id="id54">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.18</span>
        
    </div><div class="proof-content">
<p>(**)
Redo <a class="reference internal" href="#exercise-knnfmeasurewine"><span class="std std-numref">ExerciseÂ 12.17</span></a>,
but this time maximise the F measure
obtained by a 5-fold cross-validation.</p>
</div></div></section>
</section>
<section id="clustering-tasks">
<span id="sec-kmeans"></span><h2><span class="section-number">12.4. </span>Clustering tasks (*)<a class="headerlink" href="#clustering-tasks" title="Link to this heading">#</a></h2>
<p>So far, we have been implicitly assuming that either
each dataset comes from a single homogeneous distribution,
or we have a categorical variable that naturally defines the
groups that we can split the dataset into.
Nevertheless, it might be the case that we are given a sample coming from
a distribution mixture, where some subsets behave differently, but a
grouping variable has not been provided at all (e.g., we have height
and weight data but no information about the subjectsâ sexes).</p>
<p><em>Clustering methods</em> (also known as segmentation or quantisation;
see, e.g., <span id="id10">[<a class="reference internal" href="999-bibliography.html#id156" title="Aggarwal, C.C. (2015).  Data Mining: The Textbook. Springer.">2</a>, <a class="reference internal" href="999-bibliography.html#id108" title="WierzchoÅ, S.T. and KÅopotek, M.A. (2018).  Modern Algorithms for Cluster Analysis. Springer. DOI: 10.1007/978-3-319-69308-8.">100</a>]</span>)
partition a dataset into groups based only on the spatial structure
of the pointsâ relative densities. In the undermentioned <span class="math">\(k\)</span>-means method,
the cluster structure is determined based on
the pointsâ proximity to <span class="math">\(k\)</span> carefully chosen group centroids;
compare <a class="reference internal" href="320-transform-matrix.html#sec-centroid"><span class="std std-numref">SectionÂ 8.4.2</span></a>.</p>
<section id="k-means-method">
<h3><span class="section-number">12.4.1. </span><em>K</em>-means method (*)<a class="headerlink" href="#k-means-method" title="Link to this heading">#</a></h3>
<p>Fix <span class="math">\(k \ge 2\)</span>.
In the <span class="math">\(k\)</span>-means method<a class="footnote-reference brackets" href="#footkmeans" id="id11" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a>, we seek <span class="math">\(k\)</span> pivot points,
<span class="math">\(\boldsymbol{c}_1, \boldsymbol{c}_2,\dots,\boldsymbol{c}_k\in\mathbb{R}^m\)</span>,
such that the sum of squared distances between the input points
in <span class="math">\(\mathbf{X}\in\mathbb{R}^{n\times m}\)</span>
and their closest pivots is minimised:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
\text{minimise}\ \sum_{i=1}^n
\min\left\{
\| \mathbf{x}_{i,\cdot} - \boldsymbol{c}_{1} \|^2,
\| \mathbf{x}_{i,\cdot} - \boldsymbol{c}_{2} \|^2,
\dots,
\| \mathbf{x}_{i,\cdot} - \boldsymbol{c}_{k} \|^2
\right\}
\qquad\text{w.r.t. }{\boldsymbol{c}_1, \boldsymbol{c}_2,\dots,\boldsymbol{c}_k}.
\]</div>
</div>
<p>Letâs introduce the <em>label vector</em> <span class="math">\(\boldsymbol{l}\)</span> such that:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
l_i = \mathrm{arg}\min_{j} \| \mathbf{x}_{i,\cdot} - \boldsymbol{c}_{j} \|^2,
\]</div>
</div>
<p>i.e., it is the index of the pivot closest to <span class="math">\(\mathbf{x}_{i,\cdot}\)</span>.</p>
<p>We will consider all the points <span class="math">\(\mathbf{x}_{i,\cdot}\)</span>
with <span class="math">\(i\)</span> such that <span class="math">\(l_i=j\)</span> as belonging to the same, <span class="math">\(j\)</span>-th,
<em>cluster</em> (point group). This way <span class="math">\(\boldsymbol{l}\)</span> defines
a <em>partition</em> of the original dataset
into <span class="math">\(k\)</span> nonempty, mutually disjoint subsets.</p>
<p>Now, the aforementioned optimisation task can be equivalently rewritten as:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
\text{minimise}\ \sum_{i=1}^n \| \mathbf{x}_{i,\cdot} - \boldsymbol{c}_{l_i} \|^2
\qquad\text{w.r.t. }{\boldsymbol{c}_1, \boldsymbol{c}_2,\dots,\boldsymbol{c}_k}.
\]</div>
</div>
<p>We refer to the objective function as the (total) <em>within-cluster sum of
squares</em> (WCSS). This version looks easier, but it is only some false
impression: <span class="math">\(l_i\)</span>s depend on <span class="math">\(\boldsymbol{c}_j\)</span>s. They vary <em>together</em>.
We have just made them less explicit.</p>
<p>It can be shown that given a fixed label vector
<span class="math">\(\boldsymbol{l}\)</span> representing a partition,
<span class="math">\(\boldsymbol{c}_j\)</span> must be the centroid (<a class="reference internal" href="320-transform-matrix.html#sec-centroid"><span class="std std-numref">SectionÂ 8.4.2</span></a>)
of the points assigned thereto:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
\boldsymbol{c}_j = \frac{1}{n_j} \sum_{i: l_i=j} \mathbf{x}_{i,\cdot},
\]</div>
</div>
<p>where <span class="math">\(n_j=|\{i: l_i=j\}|\)</span> gives the number of
<span class="math">\(i\)</span>s such that <span class="math">\(l_i=j\)</span>, i.e., the size of the <span class="math">\(j\)</span>-th cluster.</p>
<div style="margin-top: 1em"></div><p>Here is an example dataset (see below for a scatter plot):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching-data/master/marek/blobs1.txt&quot;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s2">&quot;,&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>We can call <strong class="command">scipy.cluster.vq.kmeans2</strong> to find <span class="math">\(k=2\)</span>
clusters:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.cluster.vq</span>
<span class="n">C</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">vq</span><span class="o">.</span><span class="n">kmeans2</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>The discovered cluster centres are stored in a matrix
with <span class="math">\(k\)</span> rows and <span class="math">\(m\)</span> columns, i.e., the <span class="math">\(j\)</span>-th row
gives <span class="math">\(\mathbf{c}_j\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">C</span>
<span class="c1">## array([[ 0.99622971,  1.052801  ],</span>
<span class="c1">##        [-0.90041365, -1.08411794]])</span>
</pre></div>
</div>
<p>The label vector is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">l</span>
<span class="c1">## array([1, 1, 1, ..., 0, 0, 0], dtype=int32)</span>
</pre></div>
</div>
<p>As usual in Python, indexing starts at 0. So for <span class="math">\(k=2\)</span>
we only obtain the labels 0 and 1.</p>
<p><a class="reference internal" href="#fig-two-blobs-clusters"><span class="std std-numref">FigureÂ 12.10</span></a>
depicts the two clusters together with the cluster centroids.
We use <code class="docutils literal notranslate"><span class="pre">l</span></code> as a colour selector
in <code class="docutils literal notranslate"><span class="pre">my_colours[l]</span></code> (this is a clever instance of the integer vector-based
indexing). It seems that we correctly discovered the
very natural partitioning of this dataset into two clusters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="s2">&quot;#DF536B&quot;</span><span class="p">])[</span><span class="n">l</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">C</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">C</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;yX&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id55">
<span id="fig-two-blobs-clusters"></span><img alt="../_images/two-blobs-clusters-19.png" src="../_images/two-blobs-clusters-19.png" />
<figcaption>
<p><span class="caption-number">FigureÂ 12.10 </span><span class="caption-text">Two clusters discovered by the <span class="math">\(k\)</span>-means method. Cluster centroids are marked separately.</span><a class="headerlink" href="#id55" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Here are the cluster sizes:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>  <span class="c1"># or, e.g., pd.Series(l).value_counts()</span>
<span class="c1">## array([1017, 1039])</span>
</pre></div>
</div>
<p>The label vector <code class="docutils literal notranslate"><span class="pre">l</span></code> can be added as a new column
in the dataset. Here is a preview:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Xl</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">X1</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X2</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">l</span><span class="o">=</span><span class="n">l</span><span class="p">))</span>
<span class="n">Xl</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>  <span class="c1"># some randomly chosen rows</span>
<span class="c1">##             X1        X2  l</span>
<span class="c1">## 184  -0.973736 -0.417269  1</span>
<span class="c1">## 1724  1.432034  1.392533  0</span>
<span class="c1">## 251  -2.407422 -0.302862  1</span>
<span class="c1">## 1121  2.158669 -0.000564  0</span>
<span class="c1">## 1486  2.060772  2.672565  0</span>
</pre></div>
</div>
<p>We can now enjoy all the techniques for processing
data in groups that we have discussed so far.
In particular, computing the columnwise means
gives nothing else than the above cluster centroids:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Xl</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;l&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="c1">##          X1        X2</span>
<span class="c1">## l                    </span>
<span class="c1">## 0  0.996230  1.052801</span>
<span class="c1">## 1 -0.900414 -1.084118</span>
</pre></div>
</div>
<p>The label vector <code class="docutils literal notranslate"><span class="pre">l</span></code> can be recreated by computing
the distances between all the points and the centroids
and then picking the indexes of the closest pivots:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">l_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">scipy</span><span class="o">.</span><span class="n">spatial</span><span class="o">.</span><span class="n">distance</span><span class="o">.</span><span class="n">cdist</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">l_test</span> <span class="o">==</span> <span class="n">l</span><span class="p">)</span>  <span class="c1"># verify they are identical</span>
<span class="c1">## True</span>
</pre></div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>By construction<a class="footnote-reference brackets" href="#footvoronoi" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a>,
the <span class="math">\(k\)</span>-means method can only detect clusters of convex shapes
(such as Gaussian blobs).</p>
</div>
<div class="proof proof-type-exercise" id="id56">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.19</span>
        
    </div><div class="proof-content">
<p>Perform the clustering of the
<a class="reference external" href="https://github.com/gagolews/teaching-data/raw/master/clustering/wut_isolation.csv"><code class="docutils literal notranslate"><span class="pre">wut_isolation</span></code></a>
dataset and notice how nonsensical, geometrically speaking,
the returned clusters are.</p>
</div></div><div class="proof proof-type-exercise" id="id57">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.20</span>
        
    </div><div class="proof-content">
<p>Determine a clustering of the
<a class="reference external" href="https://github.com/gagolews/teaching-data/raw/master/clustering/wut_twosplashes.csv"><code class="docutils literal notranslate"><span class="pre">wut_twosplashes</span></code></a>
dataset and display the results on a scatter plot.
Compare them with those obtained on the standardised
version of the dataset. Recall what we said about the Euclidean distance
and its perception being disturbed when a plotâs aspect ratio is not 1:1.</p>
</div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>(*)
An even simpler classifier than the <span class="math">\(k\)</span>-nearest neighbours one
builds upon the concept of the nearest centroids.
Namely, it first determines the centroids (componentwise
arithmetic means) of the points in each class. Then, a new point
(from the test set) is assigned to the class whose centroid
is the closest thereto.
The implementation of such a classifier is left as a rather
straightforward exercise for the reader.
As an application, we recommend using it
to extrapolate the results generated by the <span class="math">\(k\)</span>-means method
(for different <span class="math">\(k\)</span>s) to previously unobserved data,
e.g., all points on a dense equidistant grid.</p>
</div>
</section>
<section id="solving-k-means-is-hard">
<h3><span class="section-number">12.4.2. </span>Solving <em>k</em>-means is hard (*)<a class="headerlink" href="#solving-k-means-is-hard" title="Link to this heading">#</a></h3>
<p>Unfortunately, the <span class="math">\(k\)</span>-means method â the identification
of label vectors/cluster centres that minimise the total within-cluster
sum of squares â relies on solving a computationally hard combinatorial
optimisation problem (e.g., <span id="id13">[<a class="reference internal" href="999-bibliography.html#id134" title="Lee, J. (2011).  A First Course in Combinatorial Optimisation. Cambridge University Press.">59</a>]</span>). In other words, the search
for the <em>truly</em> (i.e., globally) optimal solution takes,
for larger <span class="math">\(n\)</span> and <span class="math">\(k\)</span>, an impractically long time.</p>
<p>As a consequence, we must rely on some approximate algorithms
which all have one drawback in common. Namely, whatever they return
can be <em>suboptimal</em>. Hence, they can constitute a
possibly meaningless solution.</p>
<p>The documentation of <strong class="command">scipy.cluster.vq.kmeans2</strong> is, of course,
honest about it. It states that the method <em>attempts to minimise the
Euclidean distance between observations and centroids</em>.
Further, <strong class="command">sklearn.cluster.KMeans</strong>, which implements a similar
algorithm, mentions that the procedure <em>is very fast [â¦],
but it falls in local minima. That is why it can be useful to restart it
several times.</em></p>
<p>To understand what it all means, it will be very educational
to study this issue in more detail.
This is because the discussed approach
to clustering is not the only hard problem in data science
(selecting an optimal set of independent variables with respect
to AIC or BIC in linear regression is another example).</p>
</section>
<section id="lloyd-algorithm">
<h3><span class="section-number">12.4.3. </span>Lloyd algorithm (*)<a class="headerlink" href="#lloyd-algorithm" title="Link to this heading">#</a></h3>
<p>Technically, there is no such thing as <em>the</em> <span class="math">\(k\)</span>-means <em>algorithm</em>.
There are many procedures, based on numerous different heuristics,
that attempt to solve the <span class="math">\(k\)</span>-means <em>problem</em>. Unfortunately,
neither of them is perfect. This is not possible.</p>
<p>Perhaps the most widely known and easiest to understand
method is traditionally attributed to Lloyd <span id="id14">[<a class="reference internal" href="999-bibliography.html#id133" title="Lloyd, S.P. (1957 (1982)).  Least squares quantization in PCM. IEEE Transactions on Information Theory, 28:128â137. Originally a 1957 Bell Telephone Laboratories Research Report; republished in 1982. DOI: 10.1109/TIT.1982.1056489.">62</a>]</span>.
It is based on the fixed-point iteration and.
For a given <span class="math">\(\mathbf{X}\in\mathbb{R}^{n\times m}\)</span> and <span class="math">\(k \ge 2\)</span>:</p>
<ol class="arabic">
<li><p>Pick initial cluster centres
<span class="math">\(\boldsymbol{c}_{1}, \dots, \boldsymbol{c}_{k}\)</span> randomly.</p></li>
<li><p>For each point in the dataset, <span class="math">\(\mathbf{x}_{i,\cdot}\)</span>,
determine the index of its closest centre <span class="math">\(l_i\)</span>:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
    l_i = \mathrm{arg}\min_{j} \| \mathbf{x}_{i,\cdot} - \boldsymbol{c}_{j} \|^2.
    \]</div>
</div>
</li>
<li><p>Compute the centroids of the clusters defined by the label vector
<span class="math">\(\boldsymbol{l}\)</span>, i.e., for every <span class="math">\(j=1,2,\dots,k\)</span>:</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
    \boldsymbol{c}_j = \frac{1}{n_j} \sum_{i: l_i=j} \mathbf{x}_{i,\cdot},
    \]</div>
</div>
<p>where <span class="math">\(n_j=|\{i: l_i=j\}|\)</span> gives the size of the <span class="math">\(j\)</span>-th cluster.</p>
</li>
<li><p>If the objective function (total within-cluster sum of squares)
has not changed significantly since the last
iteration (say, the absolute value of the difference
between the last and the current loss is less than <span class="math">\(10^{-9}\)</span>),
then stop and return the current
<span class="math">\(\boldsymbol{c}_1,\dots,\boldsymbol{c}_k\)</span> as the result.
Otherwise, go to Step 2.</p></li>
</ol>
<div class="proof proof-type-exercise" id="id58">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.21</span>
        
    </div><div class="proof-content">
<p>(*) Implement the Lloyd algorithm in the form of a function
<strong class="command">kmeans</strong><code class="code docutils literal notranslate"><span class="pre">(X,</span> <span class="pre">C)</span></code>, where <code class="docutils literal notranslate"><span class="pre">X</span></code> is the data matrix (<span class="math">\(n\times m\)</span>)
and where the rows in <code class="docutils literal notranslate"><span class="pre">C</span></code>, being a <span class="math">\(k\times m\)</span> matrix,
give the initial cluster centres.</p>
</div></div></section>
<section id="local-minima">
<span id="sec-local-minima"></span><h3><span class="section-number">12.4.4. </span>Local minima (*)<a class="headerlink" href="#local-minima" title="Link to this heading">#</a></h3>
<p>The way the foregoing algorithm is constructed implies what follows.</p>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Lloydâs method guarantees that the centres
<span class="math">\(\boldsymbol{c}_1,\dots,\boldsymbol{c}_k\)</span> it returns cannot be
significantly improved any further by repeating Steps 2 and 3
of the algorithm.
Still, it does not necessarily
mean that they yield the <em>globally</em> optimal (the best possible) WCSS.
We might as well get stuck in a <em>local</em> minimum, where there is no
better positioning thereof in the <em>neighbourhoods</em> of the current
cluster centres; compare <a class="reference internal" href="#fig-many-local-minima"><span class="std std-numref">FigureÂ 12.11</span></a>. Yet, had we
looked beyond them, we could have found a superior solution.</p>
</div>
<figure class="align-default" id="id59">
<span id="fig-many-local-minima"></span><img alt="../_images/many-local-minima-21.png" src="../_images/many-local-minima-21.png" />
<figcaption>
<p><span class="caption-number">FigureÂ 12.11 </span><span class="caption-text">An example function (of only one variable; our problem is much higher-dimensional) with many local minima. How can we be sure there is no better minimum outside of the depicted interval?</span><a class="headerlink" href="#id59" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>A variant of the Lloyd method is given
in <strong class="command">scipy.cluster.vq.kmeans2</strong>, where the initial cluster
centres are picked at random. Letâs test its behaviour
by analysing three chosen categories
from the 2016 <a class="reference external" href="https://ssi.wi.th-koeln.de/">Sustainable Society Indices</a>
dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ssi</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;https://raw.githubusercontent.com/gagolews/&quot;</span> <span class="o">+</span>
    <span class="s2">&quot;teaching-data/master/marek/ssi_2016_categories.csv&quot;</span><span class="p">,</span>
    <span class="n">comment</span><span class="o">=</span><span class="s2">&quot;#&quot;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">ssi</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s2">&quot;Country&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span>
    <span class="p">[</span><span class="s2">&quot;PersonalDevelopmentAndHealth&quot;</span><span class="p">,</span> <span class="s2">&quot;WellBalancedSociety&quot;</span><span class="p">,</span> <span class="s2">&quot;Economy&quot;</span><span class="p">]</span>
<span class="p">]</span><span class="o">.</span><span class="n">rename</span><span class="p">({</span>
        <span class="s2">&quot;PersonalDevelopmentAndHealth&quot;</span><span class="p">:</span> <span class="s2">&quot;Health&quot;</span><span class="p">,</span>
        <span class="s2">&quot;WellBalancedSociety&quot;</span><span class="p">:</span> <span class="s2">&quot;Balance&quot;</span><span class="p">,</span>
        <span class="s2">&quot;Economy&quot;</span><span class="p">:</span> <span class="s2">&quot;Economy&quot;</span>
    <span class="p">},</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># rename columns</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[[</span><span class="s2">&quot;Australia&quot;</span><span class="p">,</span> <span class="s2">&quot;Germany&quot;</span><span class="p">,</span> <span class="s2">&quot;Poland&quot;</span><span class="p">,</span> <span class="s2">&quot;United States&quot;</span><span class="p">],</span> <span class="p">:]</span>  <span class="c1"># preview</span>
<span class="c1">##                  Health   Balance   Economy</span>
<span class="c1">## Country                                    </span>
<span class="c1">## Australia      8.590927  6.105539  7.593052</span>
<span class="c1">## Germany        8.629024  8.036620  5.575906</span>
<span class="c1">## Poland         8.265950  7.331700  5.989513</span>
<span class="c1">## United States  8.357395  5.069076  3.756943</span>
</pre></div>
</div>
<p>It is a three-dimensional dataset, where each point (row) corresponds
to a different country. Letâs find a partition into <span class="math">\(k=3\)</span> clusters.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">k</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>  <span class="c1"># reproducibility matters</span>
<span class="n">C1</span><span class="p">,</span> <span class="n">l1</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">vq</span><span class="o">.</span><span class="n">kmeans2</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
<span class="n">C1</span>
<span class="c1">## array([[7.99945084, 6.50033648, 4.36537659],</span>
<span class="c1">##        [7.6370645 , 4.54396676, 6.89893746],</span>
<span class="c1">##        [6.24317074, 3.17968018, 3.60779268]])</span>
</pre></div>
</div>
<p>The objective function (total within-cluster sum of squares)
at the returned cluster centres is equal to:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.spatial.distance</span>
<span class="k">def</span> <span class="nf">get_wcss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">C</span><span class="p">):</span>
    <span class="n">D</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">spatial</span><span class="o">.</span><span class="n">distance</span><span class="o">.</span><span class="n">cdist</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

<span class="n">get_wcss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">C1</span><span class="p">)</span>
<span class="c1">## 446.5221283436733</span>
</pre></div>
</div>
<p>Is it acceptable or not necessarily? We are unable to tell.
What we can do, however, is to run the algorithm again,
this time from a different starting point:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1234</span><span class="p">)</span>  <span class="c1"># different seed - different initial centres</span>
<span class="n">C2</span><span class="p">,</span> <span class="n">l2</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">vq</span><span class="o">.</span><span class="n">kmeans2</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
<span class="n">C2</span>
<span class="c1">## array([[7.80779013, 5.19409177, 6.97790733],</span>
<span class="c1">##        [6.31794579, 3.12048584, 3.84519706],</span>
<span class="c1">##        [7.92606993, 6.35691349, 3.91202972]])</span>
<span class="n">get_wcss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">C2</span><span class="p">)</span>
<span class="c1">## 437.51120966832775</span>
</pre></div>
</div>
<p>It is a better solution (we are lucky;
it might as well have been worse). But is it the best possible?
Again, we cannot tell, alone in the dark.</p>
<p>Does a potential suboptimality affect the way the data points are grouped?
It is indeed the case here. Letâs look at the contingency
table for the two label vectors:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">l1</span><span class="o">=</span><span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="n">l2</span><span class="p">))</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">fill_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1">## l2   0   1   2</span>
<span class="c1">## l1            </span>
<span class="c1">## 0    8   0  43</span>
<span class="c1">## 1   39   6   0</span>
<span class="c1">## 2    0  57   1</span>
</pre></div>
</div>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p>Clusters are essentially unordered.
The label vector <span class="math">\((1, 1, 2, 2, 1, 3)\)</span> represents
the same clustering as the label vectors <span class="math">\((3, 3, 2, 2, 3, 1)\)</span>
and <span class="math">\((2, 2, 3, 3, 2, 1)\)</span>.</p>
</div>
<p>By looking at the contingency table, we see
that clusters 0, 1, and 2 in <code class="docutils literal notranslate"><span class="pre">l1</span></code> correspond, respectively,
to clusters 2, 0, and 1 in <code class="docutils literal notranslate"><span class="pre">l2</span></code> (via a kind of majority voting).
We can relabel the elements in <code class="docutils literal notranslate"><span class="pre">l1</span></code> to get a more readable
result:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">l1p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])[</span><span class="n">l1</span><span class="p">]</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">l1p</span><span class="o">=</span><span class="n">l1p</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="n">l2</span><span class="p">))</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">fill_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1">## l2    0   1   2</span>
<span class="c1">## l1p            </span>
<span class="c1">## 0    39   6   0</span>
<span class="c1">## 1     0  57   1</span>
<span class="c1">## 2     8   0  43</span>
</pre></div>
</div>
<p>It is an improvement. It turns out that 8+6+1 countries
are categorised differently.
We definitely want to avoid a diplomatic crisis stemming from
our not knowing that the algorithm might return
suboptimal solutions.</p>
<div class="proof proof-type-exercise" id="id60">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.22</span>
        
    </div><div class="proof-content">
<p>(*) Determine which countries are affected.</p>
</div></div></section>
<section id="random-restarts">
<h3><span class="section-number">12.4.5. </span>Random restarts (*)<a class="headerlink" href="#random-restarts" title="Link to this heading">#</a></h3>
<p>There will never be any guarantees, but we can increase
the probability of generating a satisfactory solution by simply
restarting the method multiple times from many randomly chosen points
and picking the best<a class="footnote-reference brackets" href="#footkmeansdoesntmatter" id="id15" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a> solution
(the one with the smallest WCSS)
identified as the result.</p>
<p>Letâs make 1000 such <em>restarts</em>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">wcss</span><span class="p">,</span> <span class="n">Cs</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">C</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">vq</span><span class="o">.</span><span class="n">kmeans2</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
    <span class="n">Cs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
    <span class="n">wcss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">get_wcss</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span>
</pre></div>
</div>
<p>The best of the local minima (no guarantee that it is the global one, again)
is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">wcss</span><span class="p">)</span>
<span class="c1">## 437.51120966832775</span>
</pre></div>
</div>
<p>It corresponds to the cluster centres:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Cs</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">wcss</span><span class="p">)]</span>
<span class="c1">## array([[7.80779013, 5.19409177, 6.97790733],</span>
<span class="c1">##        [7.92606993, 6.35691349, 3.91202972],</span>
<span class="c1">##        [6.31794579, 3.12048584, 3.84519706]])</span>
</pre></div>
</div>
<p>They are the same as <code class="docutils literal notranslate"><span class="pre">C2</span></code> above (up to a permutation of
labels). We were lucky<a class="footnote-reference brackets" href="#footlucky" id="id16" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a>, after all.</p>
<p>It is very educational to look at the distribution of the
objective function at the identified local minima to see
that, proportionally, in the case of this dataset
it is not rare to end up in a quite bad solution;
see <a class="reference internal" href="#fig-wcss-local-minima"><span class="std std-numref">FigureÂ 12.12</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">wcss</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default" id="id61">
<span id="fig-wcss-local-minima"></span><img alt="../_images/wcss-local-minima-23.png" src="../_images/wcss-local-minima-23.png" />
<figcaption>
<p><span class="caption-number">FigureÂ 12.12 </span><span class="caption-text">Within-cluster sum of squares at the results returned by different runs of the <span class="math">\(k\)</span>-means algorithm. Sometimes we might be very unlucky.</span><a class="headerlink" href="#id61" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Also, <a class="reference internal" href="#fig-kmeans-suboptimal-centres"><span class="std std-numref">FigureÂ 12.13</span></a> depicts
all the cluster centres to which the algorithm converged.
We see that we should not be trusting the results
generated by a single run of a heuristic solver to the <span class="math">\(k\)</span>-means problem.</p>
<figure class="align-default" id="id62">
<span id="fig-kmeans-suboptimal-centres"></span><img alt="../_images/kmeans-suboptimal-centres-25.png" src="../_images/kmeans-suboptimal-centres-25.png" />
<figcaption>
<p><span class="caption-number">FigureÂ 12.13 </span><span class="caption-text">Traces of different cluster centres our k-means algorithm converged to. Some are definitely not optimal, and therefore the method must be restarted a few times to increase the likelihood of pinpointing the true solution.</span><a class="headerlink" href="#id62" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="proof proof-type-example" id="id63">

    <div class="proof-title">
        <span class="proof-type">Example 12.23</span>
        
    </div><div class="proof-content">
<p>(*)
The <strong class="program">scikit-learn</strong> package has an algorithm that is similar
to the Lloydâs one. The method is equipped with
the <code class="docutils literal notranslate"><span class="pre">n_init</span></code> parameter (which defaults to 10) which automatically
applies the aforementioned restarting.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sklearn.cluster</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">km</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">cluster</span><span class="o">.</span><span class="n">KMeans</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n_init</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">km</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="c1">## KMeans(n_clusters=3, n_init=10)</span>
<span class="n">km</span><span class="o">.</span><span class="n">inertia_</span>  <span class="c1"># WCSS â not optimal!</span>
<span class="c1">## 437.5467188958928</span>
</pre></div>
</div>
<p>Still, there are no guarantees: the solution is suboptimal too.
As an exercise, pass <code class="docutils literal notranslate"><span class="pre">n_init=100</span></code>, <code class="docutils literal notranslate"><span class="pre">n_init=1000</span></code>,
and <code class="docutils literal notranslate"><span class="pre">n_init=10000</span></code> and determine the returned WCSS.</p>
</div></div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is theoretically possible that a developer from the
<strong class="program">scikit-learn</strong> team, when they see the preceding result,
will make a tweak in the algorithm so that after an update to the package,
the returned minimum will be better.
This cannot be deemed a bug fix, though, as there are no bugs here.
Improving the behaviour of the method in this example
will lead to its degradation in others.
There is no free lunch in optimisation.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some datasets are more well-behaving than others.
The <span class="math">\(k\)</span>-means method is <em>overall</em> quite usable,
but we must always be cautious.</p>
<p>We recommend performing at least 100 random restarts.
Also, if a report from data analysis does not say anything about
the number of tries performed, we are advised to assume that the results
are gibberish<a class="footnote-reference brackets" href="#footr" id="id17" role="doc-noteref"><span class="fn-bracket">[</span>11<span class="fn-bracket">]</span></a>. People will complain about our being
a pain, but we know better; compare Rule#9.</p>
</div>
<div class="proof proof-type-exercise" id="id64">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.24</span>
        
    </div><div class="proof-content">
<p>Run the <span class="math">\(k\)</span>-means method, <span class="math">\(k=8\)</span>, on the
<a class="reference external" href="https://github.com/gagolews/teaching-data/raw/master/clustering/sipu_unbalance.csv"><code class="docutils literal notranslate"><span class="pre">sipu_unbalance</span></code></a>
dataset from many random sets of cluster centres.
Note the value of the total within-cluster sum
of squares. Also, plot the cluster centres discovered. Do they make sense?
Compare these to the case where you start the method from the
following cluster centres which are close to the global minimum.</p>
<div class="math-wrapper docutils container">
<div class="math">
\[
\mathbf{C} = \left[
\begin{array}{cc}
   -15  & 5    \\
   -12  & 10   \\
   -10  & 5    \\
    15  & 0    \\
    15  & 10   \\
    20  & 5    \\
    25  & 0    \\
    25  & 10   \\
\end{array}
\right].
\]</div>
</div>
</div></div></section>
</section>
<section id="further-reading">
<h2><span class="section-number">12.5. </span>Further reading<a class="headerlink" href="#further-reading" title="Link to this heading">#</a></h2>
<p>An overall noteworthy introduction to classification is <span id="id18">[<a class="reference internal" href="999-bibliography.html#id55" title="Hastie, T., Tibshirani, R., and Friedman, J. (2017).  The Elements of Statistical Learning. Springer-Verlag. URL: https://hastie.su.domains/ElemStatLearn.">48</a>]</span>
and <span id="id19">[<a class="reference internal" href="999-bibliography.html#id56" title="Bishop, C. (2006).  Pattern Recognition and Machine Learning. Springer-Verlag. URL: https://www.microsoft.com/en-us/research/people/cmbishop.">9</a>]</span>.
Nevertheless, as we said earlier, we recommend going through
a solid course in matrix algebra and mathematical statistics
first, e.g., <span id="id20">[<a class="reference internal" href="999-bibliography.html#id76" title="Deisenroth, M.P., Faisal, A.A., and Ong, C.S. (2020).  Mathematics for Machine Learning. Cambridge University Press. URL: https://mml-book.github.io/.">21</a>, <a class="reference internal" href="999-bibliography.html#id135" title="Gentle, J.E. (2017).  Matrix Algebra: Theory, Computations and Applications in Statistics. Springer.">40</a>]</span>
and <span id="id21">[<a class="reference internal" href="999-bibliography.html#id47" title="Dekking, F.M., Kraaikamp, C., LopuhaÃ¤, H.P., and Meester, L.E. (2005).  A Modern Introduction to Probability and Statistics: Understanding Why and How. Springer.">22</a>, <a class="reference internal" href="999-bibliography.html#id50" title="Gentle, J.E. (2009).  Computational Statistics. Springer-Verlag.">39</a>, <a class="reference internal" href="999-bibliography.html#id45" title="Gentle, J.E. (2020).  Theory of Statistics. book draft. URL: https://mason.gmu.edu/~jgentle/books/MathStat.pdf.">41</a>]</span>.
For advanced theoretical (probabilistic, information-theoretic) results,
see, e.g., <span id="id22">[<a class="reference internal" href="999-bibliography.html#id120" title="Blum, A., Hopcroft, J., and Kannan, R. (2020).  Foundations of Data Science. Cambridge University Press. URL: https://www.cs.cornell.edu/jeh/book.pdf.">10</a>, <a class="reference internal" href="999-bibliography.html#id80" title="Devroye, L., GyÃ¶rfi, L., and Lugosi, G. (1996).  A Probabilistic Theory of Pattern Recognition. Springer. DOI: 10.1007/978-1-4612-0711-5.">23</a>]</span>.</p>
<p>Hierarchical clustering algorithms
(see, e.g., <span id="id23">[<a class="reference internal" href="999-bibliography.html#id86" title="Gagolewski, M. (2021).  genieclust: Fast and robust hierarchical clustering. SoftwareX, 15:100722. URL: https://genieclust.gagolewski.com/, DOI: 10.1016/j.softx.2021.100722.">33</a>, <a class="reference internal" href="999-bibliography.html#id114" title="MÃ¼llner, D. (2011).  Modern hierarchical, agglomerative clustering algorithms. arXiv:1109.2378 [stat.ML]. URL: https://arxiv.org/abs/1109.2378v1.">67</a>]</span>)
are also worthwhile as they do not require
asking for a fixed number of clusters.
Furthermore, density-based algorithms (DBSCAN and its variants)
<span id="id24">[<a class="reference internal" href="999-bibliography.html#id88" title="Campello, R.J.G.B., Moulavi, D., Zimek, A., and Sander, J. (2015).  Hierarchical density estimates for data clustering, visualization, and outlier detection. ACM Transactions on Knowledge Discovery from Data, 10(1):5:1â5:51. DOI: 10.1145/2733381.">13</a>, <a class="reference internal" href="999-bibliography.html#id89" title="Ester, M., Kriegel, H.P., Sander, J., and Xu, X. (1996).  A density-based algorithm for discovering clusters in large spatial databases with noise. In: Proc. KDD'96, pp. 226â231.">26</a>, <a class="reference internal" href="999-bibliography.html#id90" title="Ling, R.F. (1973).  A probability theory of cluster analysis. Journal of the American Statistical Association, 68(341):159â164. DOI: 10.1080/01621459.1973.10481356.">60</a>]</span>
utilise the notion of fixed-radius search that we introduced
in <a class="reference internal" href="320-transform-matrix.html#sec-knn-vs-radius"><span class="std std-numref">SectionÂ 8.4.4</span></a>.</p>
<p>There are quite a few ways that aim to assess the quality of clustering
results, but their meaningfulness is somewhat limited;
see <span id="id25">[<a class="reference internal" href="999-bibliography.html#id10" title="Gagolewski, M., Bartoszuk, M., and Cena, A. (2021).  Are cluster validity measures (in)valid? Information Sciences, 581:620â636. DOI: 10.1016/j.ins.2021.10.004.">37</a>]</span> for discussion.</p>
</section>
<section id="exercises">
<h2><span class="section-number">12.6. </span>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<div class="proof proof-type-exercise" id="id65">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.25</span>
        
    </div><div class="proof-content">
<p>Name the data type of the objects that the <strong class="command">DataFrame.groupby</strong>
method returns.</p>
</div></div><div class="proof proof-type-exercise" id="id66">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.26</span>
        
    </div><div class="proof-content">
<p>What is the relationship between the <code class="docutils literal notranslate"><span class="pre">GroupBy</span></code>, <code class="docutils literal notranslate"><span class="pre">DataFrameGroupBy</span></code>,
and <code class="docutils literal notranslate"><span class="pre">SeriesGroupBy</span></code> classes?</p>
</div></div><div class="proof proof-type-exercise" id="id67">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.27</span>
        
    </div><div class="proof-content">
<p>What are relative z-scores and how can we compute them?</p>
</div></div><div class="proof proof-type-exercise" id="id68">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.28</span>
        
    </div><div class="proof-content">
<p>Why and when the accuracy score might not be the best way to quantify a
classifierâs performance?</p>
</div></div><div class="proof proof-type-exercise" id="id69">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.29</span>
        
    </div><div class="proof-content">
<p>What is the difference between recall and precision, both in terms
of how they are defined and where they are the most useful?</p>
</div></div><div class="proof proof-type-exercise" id="id70">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.30</span>
        
    </div><div class="proof-content">
<p>Explain how the <span class="math">\(k\)</span>-nearest neighbour classification and regression
algorithms work. Why do we say that they are model-free?</p>
</div></div><div class="proof proof-type-exercise" id="id71">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.31</span>
        
    </div><div class="proof-content">
<p>In the context of <span class="math">\(k\)</span>-nearest neighbour classification,
why it might be important to resolve the
potential ties at random when computing the mode of the neighboursâ labels?</p>
</div></div><div class="proof proof-type-exercise" id="id72">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.32</span>
        
    </div><div class="proof-content">
<p>What is the purpose of a training/test and a training/validation/test set
split?</p>
</div></div><div class="proof proof-type-exercise" id="id73">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.33</span>
        
    </div><div class="proof-content">
<p>Give the formula for the total within-cluster sum of squares.</p>
</div></div><div class="proof proof-type-exercise" id="id74">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.34</span>
        
    </div><div class="proof-content">
<p>Are there any cluster shapes that cannot be detected by the <span class="math">\(k\)</span>-means
method?</p>
</div></div><div class="proof proof-type-exercise" id="id75">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.35</span>
        
    </div><div class="proof-content">
<p>Why do we say that solving the <span class="math">\(k\)</span>-means problem is hard?</p>
</div></div><div class="proof proof-type-exercise" id="id76">

    <div class="proof-title">
        <span class="proof-type">Exercise 12.36</span>
        
    </div><div class="proof-content">
<p>Why restarting Lloydâs algorithm many times is necessary?
Why are reports from data analysis that do not mention the number
of restarts not trustworthy?</p>
</div></div><hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="footoop" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">1</a><span class="fn-bracket">]</span></span>
<p>(*) In this example, we called <strong class="command">pandas.GroupBy.mean</strong>.
Note that it has slightly different functionality
from <strong class="command">pandas.DataFrame.mean</strong> and <strong class="command">pandas.Series.mean</strong>,
which all needed to be implemented separately so that we can
use them in complex operation chains.
Still, they all call the underlying <strong class="command">numpy.mean</strong> function.
Object-orientated programming has its pros (more expressive syntax)
and cons (sometimes more redundancy in the API design).</p>
</aside>
<aside class="footnote brackets" id="footintroalgs" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">2</a><span class="fn-bracket">]</span></span>
<p>Remember that this is an introductory course,
and we are still being very generous here.
We encourage the readers to upskill themselves (later, of course)
not only in mathematics, but also in programming
(e.g., algorithms and data structures).</p>
</aside>
<aside class="footnote brackets" id="footerrdev" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">3</a><span class="fn-bracket">]</span></span>
<p>Including those that are merely due to round-off errors.</p>
</aside>
<aside class="footnote brackets" id="footpvalue" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">4</a><span class="fn-bracket">]</span></span>
<p>For similar reasons, we do not introduce the notion
of p-values. Most practitioners tend to misunderstand them anyway.</p>
</aside>
<aside class="footnote brackets" id="football" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">5</a><span class="fn-bracket">]</span></span>
<p>(*) As an exercise, we could author a fixed-radius
classifier; compare <a class="reference internal" href="320-transform-matrix.html#sec-knn-vs-radius"><span class="std std-numref">SectionÂ 8.4.4</span></a>.
In sparsely populated regions, the decision might be
âunknownâ.</p>
</aside>
<aside class="footnote brackets" id="foothar" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">6</a><span class="fn-bracket">]</span></span>
<p>(*) For any vector of nonnegative values,
its minimum <span class="math">\(\le\)</span> its harmonic mean <span class="math">\(\le\)</span> its arithmetic mean.</p>
</aside>
<aside class="footnote brackets" id="footkmeans" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">7</a><span class="fn-bracket">]</span></span>
<p>We do not have to denote the number of clusters
by <span class="math">\(k\)</span>. We could be speaking about the <span class="math">\(2\)</span>-means, <span class="math">\(3\)</span>-means,
<span class="math">\(l\)</span>-means, or <em>Ã¼</em>-means method too. Nevertheless,
some mainstream practitioners consider <span class="math">\(k\)</span>-means as a kind of a
brand name, letâs thus refrain from adding to their confusion.
Another widely known algorithm
is called fuzzy (weighted) <span class="math">\(c\)</span>-means <span id="id26">[<a class="reference internal" href="999-bibliography.html#id92" title="Bezdek, J.C., Ehrlich, R., and Full, W. (1984).  FCM: The fuzzy c-means clustering algorithm. Computer and Geosciences, 10(2â3):191â203. DOI: 10.1016/0098-3004(84)90020-7.">7</a>]</span>.</p>
</aside>
<aside class="footnote brackets" id="footvoronoi" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">8</a><span class="fn-bracket">]</span></span>
<p>(*) And its relation to Voronoi diagrams.</p>
</aside>
<aside class="footnote brackets" id="footkmeansdoesntmatter" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">9</a><span class="fn-bracket">]</span></span>
<p>If we have many different heuristics,
each aiming to approximate
a solution to the <span class="math">\(k\)</span>-means problem, from the practical
point of view it does not really matter which one returns
the best solution â they are merely our tools to achieve
a higher goal. Ideally, we could run all of them
many times and get the result that corresponds to the smallest
WCSS. It is crucial to <em>do our best</em> to
find the optimal set of cluster centres â the
more approaches we test, the better the chance of success.</p>
</aside>
<aside class="footnote brackets" id="footlucky" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">10</a><span class="fn-bracket">]</span></span>
<p>Mind who is the benevolent dictator
of the pseudorandom number generatorâs seed.</p>
</aside>
<aside class="footnote brackets" id="footr" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id17">11</a><span class="fn-bracket">]</span></span>
<p>For instance, Râs <strong class="command">stats::kmeans</strong>
automatically uses <code class="docutils literal notranslate"><span class="pre">nstart=1</span></code>. It is not rare, unfortunately,
that data analysts only stick with the default arguments.</p>
</aside>
</aside>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="440-sql.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title"><span class="section-number">13. </span>Accessing databases</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="420-categorical.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title"><span class="section-number">11. </span>Handling categorical data</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
              
              
              Copyright &#169; 2022â2024 by <a href="https://www.gagolewski.com/">Marek Gagolewski</a>.
              Some rights reserved. Licensed under <a href='https://creativecommons.org/licenses/by-nc-nd/4.0'>CC BY-NC-ND 4.0</a>.
              Built with <a href="https://sphinx-doc.org/">Sphinx</a>
              and a customised <a href="https://github.com/pradyunsg/furo">Furo</a> theme.
              Last updated on 2024-01-25T13:12:31+1100.
              This site will never display any ads: it is a non-profit project.
              It does not collect any data.
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            In this chapter
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">12. Processing data in groups</a><ul>
<li><a class="reference internal" href="#basic-methods">12.1. Basic methods</a><ul>
<li><a class="reference internal" href="#aggregating-data-in-groups">12.1.1. Aggregating data in groups</a></li>
<li><a class="reference internal" href="#transforming-data-in-groups">12.1.2. Transforming data in groups</a></li>
<li><a class="reference internal" href="#manual-splitting-into-subgroups">12.1.3. Manual splitting into subgroups (*)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#plotting-data-in-groups">12.2. Plotting data in groups</a><ul>
<li><a class="reference internal" href="#series-of-box-plots">12.2.1. Series of box plots</a></li>
<li><a class="reference internal" href="#series-of-bar-plots">12.2.2. Series of bar plots</a></li>
<li><a class="reference internal" href="#semitransparent-histograms">12.2.3. Semitransparent histograms</a></li>
<li><a class="reference internal" href="#scatter-plots-with-group-information">12.2.4. Scatter plots with group information</a></li>
<li><a class="reference internal" href="#grid-trellis-plots">12.2.5. Grid (trellis) plots</a></li>
<li><a class="reference internal" href="#kolmogorovsmirnov-test-for-comparing-ecdfs">12.2.6. KolmogorovâSmirnov test for comparing ECDFs (*)</a></li>
<li><a class="reference internal" href="#comparing-quantiles">12.2.7. Comparing quantiles</a></li>
</ul>
</li>
<li><a class="reference internal" href="#classification-tasks">12.3. Classification tasks (*)</a><ul>
<li><a class="reference internal" href="#k-nearest-neighbour-classification">12.3.1. <em>K</em>-nearest neighbour classification (*)</a></li>
<li><a class="reference internal" href="#assessing-prediction-quality">12.3.2. Assessing prediction quality (*)</a></li>
<li><a class="reference internal" href="#splitting-into-training-and-test-sets">12.3.3. Splitting into training and test sets (*)</a></li>
<li><a class="reference internal" href="#validating-many-models-parameter-selection">12.3.4. Validating many models (parameter selection) (**)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#clustering-tasks">12.4. Clustering tasks (*)</a><ul>
<li><a class="reference internal" href="#k-means-method">12.4.1. <em>K</em>-means method (*)</a></li>
<li><a class="reference internal" href="#solving-k-means-is-hard">12.4.2. Solving <em>k</em>-means is hard (*)</a></li>
<li><a class="reference internal" href="#lloyd-algorithm">12.4.3. Lloyd algorithm (*)</a></li>
<li><a class="reference internal" href="#local-minima">12.4.4. Local minima (*)</a></li>
<li><a class="reference internal" href="#random-restarts">12.4.5. Random restarts (*)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#further-reading">12.5. Further reading</a></li>
<li><a class="reference internal" href="#exercises">12.6. Exercises</a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=d0e60088"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=32e29ea5"></script>
    <script src="../_static/proof.js"></script>
    <script src="../_static/katex.min.js?v=deb2f140"></script>
    <script src="../_static/auto-render.min.js?v=8b9f325c"></script>
    <script src="../_static/katex_autorenderer.js?v=bebc588a"></script>
    </body>
</html>